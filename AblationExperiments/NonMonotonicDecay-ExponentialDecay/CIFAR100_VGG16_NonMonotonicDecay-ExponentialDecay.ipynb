{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72144a-bf13-4faa-8516-aea25a2b841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Current working directory: C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\n",
      "âœ… sys.path updated:\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\python310.zip\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\DLLs\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\n",
      "   ğŸ“‚ \n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\Pythonwin\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\models\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\activation\n",
      "âœ… FFTGate imported successfully!\n",
      "âœ… FFTGate instance created successfully!\n",
      "âœ… FFTGate_VGG imported successfully!\n",
      "CIFAR100 Training Script Initialized...\n",
      "Using device: cuda\n",
      "Parsed learning rate: 0.001 (type: <class 'float'>)\n",
      "Formatted learning rate for filenames: 0_001\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Length of training dataset: 50000\n",
      "Length of testing dataset: 10000\n",
      "Number of classes in CIFAR-100: 100\n",
      "==> Building model..\n",
      "âœ… Found 13 FFTGate layers.\n",
      "âœ… Collected 13 trainable activation parameters.\n",
      "   ğŸ”¹ Layer 0: FFTGate()\n",
      "   ğŸ”¹ Layer 1: FFTGate()\n",
      "   ğŸ”¹ Layer 2: FFTGate()\n",
      "   ğŸ”¹ Layer 3: FFTGate()\n",
      "   ğŸ”¹ Layer 4: FFTGate()\n",
      "   ğŸ”¹ Layer 5: FFTGate()\n",
      "   ğŸ”¹ Layer 6: FFTGate()\n",
      "   ğŸ”¹ Layer 7: FFTGate()\n",
      "   ğŸ”¹ Layer 8: FFTGate()\n",
      "   ğŸ”¹ Layer 9: FFTGate()\n",
      "   ğŸ”¹ Layer 10: FFTGate()\n",
      "   ğŸ”¹ Layer 11: FFTGate()\n",
      "   ğŸ”¹ Layer 12: FFTGate()\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 1.A. IMPORTS LIBRARIES | XXX -----------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\"\"\"Train CIFAR100 with PyTorch.\"\"\"\n",
    "\n",
    "# Python 2/3 compatibility\n",
    "# from __future__ import print_function\n",
    "\n",
    "\n",
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# PyTorch and related modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# torchvision for datasets and transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch_optimizer as torch_opt  # Use 'torch_opt' for torch_optimizer\n",
    "from timm.scheduler import CosineLRScheduler \n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Define currect working directory to ensure on right directory\n",
    "VGG16_PATH = r\"C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\"\n",
    "if os.getcwd() != VGG16_PATH:\n",
    "    os.chdir(VGG16_PATH)\n",
    "print(f\"âœ… Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# âœ… Define absolute paths\n",
    "PROJECT_PATH = VGG16_PATH\n",
    "MODELS_PATH = os.path.join(VGG16_PATH, \"models\")\n",
    "ACTIVATION_PATH = os.path.join(VGG16_PATH, \"activation\")\n",
    "# PAU_PATH = os.path.join(VGG16_PATH, \"pau\")\n",
    "\n",
    "# âœ… Ensure necessary paths are in sys.path\n",
    "for path in [PROJECT_PATH, MODELS_PATH, ACTIVATION_PATH]:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "# âœ… Print updated sys.path for debugging\n",
    "print(\"âœ… sys.path updated:\")\n",
    "for path in sys.path:\n",
    "    print(\"   ğŸ“‚\", path)\n",
    "\n",
    "# âœ… Import FFTGate (Check if the module exists)\n",
    "try:\n",
    "    from activation.FFTGate import FFTGate  # type: ignore\n",
    "    print(\"âœ… FFTGate imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ Import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'Activation4.py' exists inside: {ACTIVATION_PATH}\")\n",
    "\n",
    "# âœ… Test if FFTGate is callable\n",
    "try:\n",
    "    activation_test = FFTGate()\n",
    "    print(\"âœ… FFTGate instance created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error while initializing FFTGate: {e}\")\n",
    "\n",
    "# âœ… Now import FFTGate_VGG (Ensure module exists inside models/)\n",
    "try:\n",
    "    from models.FFTGate_VGG import FFTGate_VGG  # type: ignore\n",
    "    print(\"âœ… FFTGate_VGG imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ FFTGate_VGG import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'FFTGate_VGG.py' exists inside: {MODELS_PATH}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 1.B. SEEDING FOR REPRODUCIBILITY | XXX -------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def set_seed_torch(seed):\n",
    "    torch.manual_seed(seed)                          \n",
    "\n",
    "\n",
    "\n",
    "def set_seed_main(seed):\n",
    "    random.seed(seed)                                ## Python's random module\n",
    "    np.random.seed(seed)                             ## NumPy's random module\n",
    "    torch.cuda.manual_seed(seed)                     ## PyTorch's random module for CUDA\n",
    "    torch.cuda.manual_seed_all(seed)                 ## Seed for all CUDA devices\n",
    "    torch.backends.cudnn.deterministic = True        ## Ensure deterministic behavior for CuDNN\n",
    "    torch.backends.cudnn.benchmark = False           ## Disable CuDNN's autotuning for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)   \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (Optional) Import Optimizers - Uncomment as needed\n",
    "# from Opt import opt\n",
    "# from diffGrad import diffGrad\n",
    "# from diffRGrad import diffRGrad, SdiffRGrad, BetaDiffRGrad, Beta12DiffRGrad, BetaDFCDiffRGrad\n",
    "# from RADAM import Radam, BetaRadam\n",
    "# from BetaAdam import BetaAdam, BetaAdam1, BetaAdam2, BetaAdam3, BetaAdam4, BetaAdam5, BetaAdam6, BetaAdam7, BetaAdam4A\n",
    "# from AdamRM import AdamRM, AdamRM1, AdamRM2, AdamRM3, AdamRM4, AdamRM5\n",
    "# from sadam import sadam\n",
    "# from SdiffGrad import SdiffGrad\n",
    "# from SRADAM import SRADAM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 2. DEFINE MODEL Lr | XXX ---------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Main Execution (Placeholder)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"CIFAR100 Training Script Initialized...\")\n",
    "    # Add your training pipeline here\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Argument parser to get user inputs\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR100 Training')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "\n",
    "args, unknown = parser.parse_known_args()  # Avoids Jupyter argument issues\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Ensure lr is correctly parsed\n",
    "lr = args.lr  # Get learning rate from argparse\n",
    "lr_str = str(lr).replace('.', '_')  # Convert to string and replace '.' for filenames\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Parsed learning rate: {lr} (type: {type(lr)})\")\n",
    "print(f\"Formatted learning rate for filenames: {lr_str}\")\n",
    "\n",
    "# Initialize training variables\n",
    "best_acc = 0  # Best test accuracy\n",
    "start_epoch = 0  # Start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 3. LOAD DATASET | XXX ------------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "bs = 64 #set batch size\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Length of train and test datasets\n",
    "len_train = len(trainset)\n",
    "len_test = len(testset)\n",
    "print(f\"Length of training dataset: {len_train}\")\n",
    "print(f\"Length of testing dataset: {len_test}\")\n",
    "\n",
    "# âœ… Print number of classes\n",
    "num_classes_Print = len(trainset.classes)\n",
    "print(f\"Number of classes in CIFAR-100: {num_classes_Print}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 4. DYNAMIC REGULARIZATION| XXX ---------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def apply_dynamic_regularization(inputs, feature_activations, epoch,\n",
    "                                  prev_params, layer_index_map, batch_idx):\n",
    "\n",
    "\n",
    "    global activation_layers  # âœ… Reference already-collected layers\n",
    "\n",
    "    # âœ… Print gamma1 stats early in training for monitoring\n",
    "    if batch_idx == 0 and epoch <= 4:\n",
    "        print(f\"\\nğŸš¨ ENTERED apply_dynamic_regularization | Epoch={epoch} | Batch={batch_idx}\", flush=True)\n",
    "\n",
    "        # ğŸ§  Print gamma1 details\n",
    "        all_layer_info = []\n",
    "        for idx, layer in enumerate(activation_layers):\n",
    "            param = getattr(layer, \"gamma1\")\n",
    "            all_layer_info.append(f\"Layer {idx}: ID={id(param)} | Mean={param.mean().item():.5f}\")\n",
    "        print(\"ğŸ§  GAMMA1 INFO:\", \" | \".join(all_layer_info), flush=True)\n",
    "\n",
    "    # âœ… Initialize gamma1 regularization accumulator\n",
    "    gamma1_reg = 0.0\n",
    "\n",
    "    # âœ… Compute batch std and define regularization strength\n",
    "    batch_std = torch.std(inputs) + 1e-6\n",
    "    regularization_strength = 0.05 if epoch < 40 else (0.01 if epoch < 60 else 0.005)\n",
    "\n",
    "    # âœ… Track layers where noise is injected (informative)\n",
    "    noisy_layers = []\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        if idx not in layer_index_map:\n",
    "            continue\n",
    "\n",
    "        prev_layer_params = prev_params[layer_index_map[idx]]\n",
    "        param_name = \"gamma1\"\n",
    "        param = getattr(layer, param_name)\n",
    "        prev_param = prev_layer_params[param_name]\n",
    "\n",
    "        # âœ… Target based on input stats\n",
    "        target = compute_target(param_name, batch_std)\n",
    "\n",
    "        # âœ… Adaptive Target Regularization\n",
    "        gamma1_reg += regularization_strength * (param - target).pow(2).mean() * 1.2\n",
    "\n",
    "        # âœ… Adaptive Cohesion Regularization\n",
    "        cohesion = (param - prev_param).pow(2)\n",
    "        gamma1_reg += 0.005 * cohesion.mean()\n",
    "\n",
    "        # âœ… Adaptive Noise Regularization\n",
    "        epoch_AddNoise = 50\n",
    "        if epoch > epoch_AddNoise:\n",
    "            param_variation = torch.abs(param - prev_param).mean()\n",
    "            if param_variation < 0.015:\n",
    "                noise = (0.001 + 0.0004 * batch_std.item()) * torch.randn_like(param)\n",
    "                penalty = (param - (prev_param + noise)).pow(2).sum()\n",
    "                gamma1_reg += 0.00015 * penalty\n",
    "                noisy_layers.append(f\"{idx} (Î”={param_variation.item():.5f})\") # Collect index and variation\n",
    "\n",
    "    # âœ… Print noise injection summary\n",
    "    if batch_idx == 0 and epoch <= (epoch_AddNoise + 4) and noisy_layers:\n",
    "        print(f\"ğŸ”¥ Stable Noise Injected | Epoch {epoch} | Batch {batch_idx} | Layers: \" + \", \".join(noisy_layers), flush=True)\n",
    "    mags = feature_activations.abs().mean(dim=(0, 2, 3))\n",
    "    m = mags / mags.sum()\n",
    "    gamma1_reg += 0.005 * (-(m * torch.log(m + 1e-6)).sum())\n",
    "\n",
    "    return gamma1_reg\n",
    "\n",
    "\n",
    "def compute_target(param_name, batch_std):\n",
    "    if param_name == \"gamma1\":\n",
    "        return 2.0 + 0.2 * batch_std.item()  \n",
    "\n",
    "    raise ValueError(f\"Unknown param {param_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 5. INITIALIZE MODEL | XXX --------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "#net = Elliott_VGG('VGG16'); net1 = 'Elliott_VGG16'\n",
    "#net = GELU_MobileNet(); net1 = 'GELU_MobileNet'\n",
    "#net = GELU_SENet18(); net1 = 'GELU_SENet18'\n",
    "#net = PDELU_ResNet50(); net1 = 'PDELU_ResNet50'\n",
    "# net = Sigmoid_GoogLeNet(); net1 = 'Sigmoid_GoogLeNet'\n",
    "#net = GELU_DenseNet121(); net1 = 'GELU_DenseNet121'\n",
    "# net = ReLU_VGG('VGG16'); net1 = 'ReLU_VGG16'\n",
    "net = FFTGate_VGG('VGG16'); net1 = 'FFTGate_VGG16'\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9); optimizer1 = 'SGDM5'\n",
    "#optimizer = optim.Adagrad(net.parameters()); optimizer1 = 'AdaGrad'\n",
    "#optimizer = optim.Adadelta(net.parameters()); optimizer1 = 'AdaDelta'\n",
    "#optimizer = optim.RMSprop(net.parameters()); optimizer1 = 'RMSprop'\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr); optimizer1 = 'Adam'\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr, amsgrad=True); optimizer1 = 'amsgrad'\n",
    "#optimizer = diffGrad(net.parameters(), lr=args.lr); optimizer1 = 'diffGrad'\n",
    "#optimizer = Radam(net.parameters(), lr=args.lr); optimizer1 = 'Radam'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 6. INITIALIZE ACTIVATION PARAMETERS, OPTIMIZERS & SCHEDULERS | XXX ---------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# âœ… Step 1: Collect Activation Parameters from ALL Layers (Ensure Compatibility with DataParallel)\n",
    "if isinstance(net, torch.nn.DataParallel):\n",
    "    features = net.module.features\n",
    "else:\n",
    "    features = net.features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 2: Recursively search for FFTGate layers\n",
    "activation_params = []\n",
    "activation_layers = []\n",
    "\n",
    "for layer in features:\n",
    "    if isinstance(layer, FFTGate):  \n",
    "        activation_layers.append(layer)\n",
    "        activation_params.append(layer.gamma1)  # âœ… Only gamma1 is trainable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 3: Define Unfreeze Epoch\n",
    "unfreeze_activation_epoch = 1  # âœ… Change this value if needed\n",
    "# unfreeze_activation_epoch = 10  # âœ… Delay unfreezing until epoch 10\n",
    "\n",
    "\n",
    "# âœ… Define the warm-up epoch value\n",
    "# WARMUP_ACTIVATION_EPOCHS = 5  # The number of epochs for warm-up\n",
    "WARMUP_ACTIVATION_EPOCHS = 0  # The number of epochs for warm-up\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initially Freeze Activation Parameters\n",
    "for param in activation_params:\n",
    "    param.requires_grad = False  # ğŸš« Keep frozen before the unfreeze epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initialize Activation Optimizers (Using AdamW for Better Weight Decay)\n",
    "activation_optimizers = {\n",
    "    \"gamma1\": torch.optim.AdamW(activation_params, lr=0.0015, weight_decay=1e-6)  # ğŸ”º Reduce LR from 0.005 â†’ 0.0025\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 5: Initialize Activation Schedulers with Warm Restarts (Per Parameter Type)\n",
    "activation_schedulers = {\n",
    "    \"gamma1\": CosineAnnealingWarmRestarts(\n",
    "        activation_optimizers[\"gamma1\"],\n",
    "        T_0=10,      # Shorter cycle to explore aggressively\n",
    "        T_mult=2,    # Increase cycle length gradually\n",
    "        eta_min=5e-5  # âœ… recommended safer modification\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 6: Print collected activation layers and parameters\n",
    "if activation_layers and activation_params:\n",
    "    print(f\"âœ… Found {len(activation_layers)} FFTGate layers.\")\n",
    "    print(f\"âœ… Collected {len(activation_params)} trainable activation parameters.\")\n",
    "    \n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        print(f\"   ğŸ”¹ Layer {idx}: {layer}\")\n",
    "\n",
    "elif activation_layers and not activation_params:\n",
    "    print(f\"âš  Warning: Found {len(activation_layers)} FFTGate layers, but no trainable parameters were collected.\")\n",
    "\n",
    "elif activation_params and not activation_layers:\n",
    "    print(f\"âš  Warning: Collected {len(activation_params)} activation parameters, but no FFTGate layers were recorded.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš  Warning: No FFTGate layers or activation parameters found! Skipping activation optimizer.\")\n",
    "    activation_optimizers = None\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 7. INITIALIZE MAIN OPTIMIZER SCHEDULER | XXX -------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Step 6: Define MultiStepLR for Main Optimizer\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "main_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 8. MODEL CHECK POINT | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists('checkpoint'):\n",
    "    os.makedirs('checkpoint')\n",
    "\n",
    "if not os.path.exists('Results'):\n",
    "    os.makedirs('Results')\n",
    "\n",
    "# Construct checkpoint path\n",
    "checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr}_{net1}_{optimizer1}.t7'\n",
    "\n",
    "# Resume checkpoint only if file exists\n",
    "if args.resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 9. DEFINE TRAIN LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Used for naming files \n",
    "decay_mode = \"Exponential\"  # Options: \"exp\" or \"linear\"\n",
    "\n",
    "# Training\n",
    "\n",
    "def train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler , WARMUP_ACTIVATION_EPOCHS):\n",
    "    global train_loss_history, best_train_acc, prev_params, recent_test_acc, gamma1_history, activation_layers, test_acc_history, train_acc_history, decay_mode  # ğŸŸ¢ğŸŸ¢ğŸŸ¢\n",
    "\n",
    "    if epoch == 0:\n",
    "        train_loss_history = []\n",
    "        train_acc_history = []\n",
    "        best_train_acc = 0.0\n",
    "        recent_test_acc = 0.0\n",
    "        gamma1_history = {}         # âœ… Initialize history\n",
    "        test_acc_history = []       # âœ… test accuracy history\n",
    "\n",
    "\n",
    "\n",
    "    prev_params = {}\n",
    "    layer_index_map = {idx: idx for idx in range(len(activation_layers))}  \n",
    "\n",
    "    # âœ… Cache previous gamma1 values from activation layers\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_accuracy = 0.0\n",
    "\n",
    "    # âœ… Initialize log history\n",
    "    log_history = []\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Define path to store Training log\n",
    "    save_paths = {\n",
    "       \n",
    "        \"log_history\": f\"C:\\\\Users\\\\emeka\\\\Research\\\\ModelCUDA\\\\Big_Data_Journal\\\\Comparison\\\\Code\\\\Paper\\\\github2\\\\AblationExperiments\\\\NonMonotonicDecay-ExponentialDecay\\\\Results\\\\FFTGate\\\\FFTGate_training_logs.txt\"  # âœ… Training log_history \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 1: Unfreeze Activation Parameters (Only Once Per Epoch)\n",
    "    if epoch == unfreeze_activation_epoch:\n",
    "        print(\"\\nğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\")\n",
    "        for layer in net.module.features if isinstance(net, torch.nn.DataParallel) else net.features:\n",
    "            if isinstance(layer, FFTGate):   \n",
    "                layer.gamma1.requires_grad = True  # âœ… Only gamma1 is trainable\n",
    "        print(\"âœ… Activation Parameters Unfrozen! ğŸš€\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 2: Gradual Warm-up for Activation Learning Rates (AFTER Unfreezing)\n",
    "    warmup_start = unfreeze_activation_epoch  # ğŸ”¹ Start warm-up when unfreezing happens\n",
    "    warmup_end = unfreeze_activation_epoch + WARMUP_ACTIVATION_EPOCHS  # ğŸ”¹ End warm-up period\n",
    "\n",
    "    # âœ… Adjust learning rates **only** during the warm-up phase\n",
    "    if warmup_start <= epoch < warmup_end:\n",
    "        warmup_factor = (epoch - warmup_start + 1) / WARMUP_ACTIVATION_EPOCHS  \n",
    "\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            for param_group in act_scheduler.optimizer.param_groups:\n",
    "                if \"initial_lr\" not in param_group:\n",
    "                    param_group[\"initial_lr\"] = param_group[\"lr\"]  # ğŸ”¹ Store initial LR\n",
    "                param_group[\"lr\"] = param_group[\"initial_lr\"] * warmup_factor  # ğŸ”¹ Scale LR\n",
    "\n",
    "        # âœ… Debugging output to track warm-up process\n",
    "        print(f\"ğŸ”¥ Warm-up Epoch {epoch}: Scaling LR by {warmup_factor:.3f}\")\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            print(f\"  ğŸ”¹ {name} LR: {act_scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    activation_history = []  # ğŸ”´ Initialize empty history at start of epoch (outside batch loop)\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Training Loop\n",
    "    with tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch}\") as progress:\n",
    "        for batch_idx, (inputs, targets) in progress:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # zero_grad activation parameter\n",
    "            for opt in activation_optimizers.values():\n",
    "                opt.zero_grad()\n",
    "\n",
    "\n",
    "            # âœ… Forward Pass\n",
    "            outputs = net(inputs, epoch=epoch, train_accuracy=train_accuracy, targets=targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            \n",
    "            feature_activations = features(inputs)  # Feature activations\n",
    "\n",
    "\n",
    "            # âœ… Collect Activation History | âœ… Per-layer mean activations\n",
    "            batch_means = [layer.saved_output.mean().item() for layer in activation_layers]\n",
    "            activation_history.extend(batch_means)\n",
    "\n",
    "            # âœ… Apply Decay strategy to history for each activation layer\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    if isinstance(layer, FFTGate):\n",
    "                        layer.decay_spectral_history(epoch, num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Compute Training Accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            train_accuracy = 100. * correct / total if total > 0 else 0.0  # Compute training accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Call Regularization Function for the Activation Parameter\n",
    "            if epoch > 0:\n",
    "                gamma1_reg = apply_dynamic_regularization(\n",
    "                    inputs, feature_activations, epoch,\n",
    "                    prev_params, layer_index_map, batch_idx\n",
    "                )\n",
    "                loss += gamma1_reg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… ğŸ¯ Adaptive Gradient Clipping of gamma1  \n",
    "            for layer in features:\n",
    "                if isinstance(layer, FFTGate):  # âœ… Ensure layer has gamma1 before clipping\n",
    "                    torch.nn.utils.clip_grad_norm_([layer.gamma1], max_norm=0.7)\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Apply Optimizer Step for Model Parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # âœ… Apply Optimizer Steps for Activation Parameters (Only if Unfrozen)\n",
    "            if epoch >= unfreeze_activation_epoch:\n",
    "                for opt in activation_optimizers.values():\n",
    "                    opt.step()\n",
    "\n",
    "\n",
    "            # âœ… Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Clamping of gamma1 (Applied AFTER Optimizer Step)\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    layer.gamma1.clamp_(0.1, 6.0)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Update progress bar\n",
    "            progress.set_postfix(Train_loss=round(train_loss / (batch_idx + 1), 3),\n",
    "                                 Train_acc=train_accuracy)  \n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step the main optimizer scheduler (ONLY for model parameters)\n",
    "    main_scheduler.step()\n",
    "\n",
    "    # âœ… Step the activation parameter schedulers (ONLY for activation parameters) | Epoch-wise stepping\n",
    "    if epoch >= unfreeze_activation_epoch:\n",
    "        for name, act_scheduler in activation_schedulers.items():  \n",
    "            act_scheduler.step()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… ONLY update prev_params here AFTER all updates | âœ… Update prev_params AFTER training epoch\n",
    "    for idx, layer in enumerate(activation_layers):      \n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Logging Activation Parameters & Gradients\n",
    "    last_batch_grads = {\"Gamma1 Grad\": []}\n",
    "    current_params = {\"Gamma1\": []}\n",
    "\n",
    "    for layer in features:\n",
    "        if isinstance(layer, FFTGate):  \n",
    "            # âœ… Convert gradients to scalar floats and format to 5 decimal places (removes device='cuda:0' and tensor(...))\n",
    "            last_batch_grads[\"Gamma1 Grad\"].append(f\"{layer.gamma1.grad.item():.5f}\" if layer.gamma1.grad is not None else \"None\")\n",
    "\n",
    "            # âœ… Collect current parameter values (already scalar), formatted to 5 decimal places\n",
    "            current_params[\"Gamma1\"].append(f\"{layer.gamma1.item():.5f}\")\n",
    "\n",
    "    # âœ… Build log message (showing params and gradients for ALL layers)\n",
    "    log_msg = (\n",
    "        f\"Epoch {epoch}: M_Optimizer LR => {optimizer.param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1 LR => {activation_optimizers['gamma1'].param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1: {current_params['Gamma1']} | \"\n",
    "        f\"Gamma1 Grad: {last_batch_grads['Gamma1 Grad']}\"\n",
    "    )\n",
    "\n",
    "    log_history.append(log_msg)\n",
    "    print(log_msg)  # âœ… Prints only once per epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (Clear old logs)\n",
    "    if epoch == 0:  # âœ… Only clear at the start of training\n",
    "        with open(save_paths[\"log_history\"], \"w\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Save logs once per epoch (Append new logs)\n",
    "    if log_history:\n",
    "        with open(save_paths[\"log_history\"], \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_history) + \"\\n\")         # âœ… Ensure each entry is on a new line\n",
    "        print(f\"ğŸ“œ Logs saved to {save_paths['log_history']}!\")  # âœ… Only prints once per epoch\n",
    "    else:\n",
    "        print(\"âš  No logs to save!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final training accuracy for the epoch\n",
    "    final_train_loss = train_loss / len(trainloader)\n",
    "    final_train_acc = 100. * correct / total\n",
    "\n",
    "    # âœ… Append to history\n",
    "    train_loss_history.append(final_train_loss)\n",
    "\n",
    "    # Append per-epoch training accuracy\n",
    "    train_acc_history.append(final_train_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save training results (without affecting best accuracy tracking)\n",
    "    train_results_path = f'./Results/CIFAR100_Train_{decay_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "    # âœ… Clear the log file at the start of training (Epoch 0)\n",
    "    if epoch == 0 and os.path.exists(train_results_path):\n",
    "        with open(train_results_path, 'w') as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs only once\n",
    "\n",
    "    # âœ… Append new training results for each epoch\n",
    "    with open(train_results_path, 'a') as f:\n",
    "        f.write(f\"Epoch {epoch} | Train Loss: {final_train_loss:.3f} | Train Acc: {final_train_acc:.3f}%\\n\")\n",
    "\n",
    "    if final_train_acc > best_train_acc:\n",
    "        best_train_acc = final_train_acc  # âœ… Update best training accuracy\n",
    "        print(f\"ğŸ† New Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "    # âœ… Append the best training accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):  # Only log once at the final epoch\n",
    "        with open(train_results_path, 'a') as f:\n",
    "            f.write(f\"\\nğŸ† Best Training Accuracy: {best_train_acc:.3f}%\\n\")  \n",
    "\n",
    "    # âœ… Print both Final and Best Training Accuracy\n",
    "    print(f\"ğŸ“Š Train Accuracy: {final_train_acc:.3f}% | ğŸ† Best Train Accuracy: {best_train_acc:.3f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"ğŸ“œ Training logs saved to {train_results_path}!\")\n",
    "    print(f\"ğŸ† Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"ğŸ“ Sizes â†’ ActivationHist: {len(activation_history)} | TestAccHist: {len(test_acc_history)} | TrainLossHist: {len(train_loss_history)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # return final_train_loss, final_train_acc, feature_activations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 10. DEFINE TEST LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(epoch, save_results=True):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and optionally saves the results.\n",
    "    \n",
    "    Args:\n",
    "    - epoch (int): The current epoch number.\n",
    "    - save_results (bool): Whether to save results to a file.\n",
    "\n",
    "    Returns:\n",
    "    - acc (float): Test accuracy percentage.\n",
    "    \"\"\"\n",
    "    global best_acc, val_accuracy, decay_mode  \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # âœ… Ensure activation function parameters are clamped before evaluation\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(testloader), total=len(testloader), desc=f\"Testing Epoch {epoch}\") as progress:\n",
    "            for batch_idx, (inputs, targets) in progress:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                # âœ… Pass validation accuracy to activation function\n",
    "                val_accuracy = 100. * correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "                # âœ… Update progress bar with loss & accuracy\n",
    "                progress.set_postfix(Test_loss=round(test_loss / (batch_idx + 1), 3),\n",
    "                                     Test_acc=round(val_accuracy, 3))\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final test accuracy\n",
    "    final_test_loss = test_loss / len(testloader)\n",
    "    final_test_acc = 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Ensure \"Results\" folder exists (just like training logs)\n",
    "    results_dir = os.path.join(PROJECT_PATH, \"Results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # âœ… Define log file path for test results\n",
    "    test_results_path = os.path.join(results_dir, f'CIFAR100_Test_{decay_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt')\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (clear old logs)\n",
    "    if epoch == 0:\n",
    "        with open(test_results_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Append new test results for each epoch (same style as training)\n",
    "    with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Epoch {epoch} | Test Loss: {final_test_loss:.3f} | Test Acc: {final_test_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save checkpoint if accuracy improves (does NOT interfere with logging)\n",
    "    if final_test_acc > best_acc:\n",
    "        print('ğŸ† Saving best model...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': final_test_acc,  # âœ… Ensures the best test accuracy is saved in checkpoint\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Ensure checkpoint directory exists\n",
    "        checkpoint_dir = \"checkpoint\"\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "        # âœ… Format learning rate properly before saving filename\n",
    "        lr_str = str(lr).replace('.', '_')\n",
    "        checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr_str}_{net1}_{optimizer1}.t7'\n",
    "        torch.save(state, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "        best_acc = final_test_acc  # âœ… Update best accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Append the best test accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):\n",
    "        with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\nğŸ† Best Test Accuracy: {best_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Print both Final and Best Test Accuracy (always executed)\n",
    "    print(f\"ğŸ“Š Test Accuracy: {final_test_acc:.3f}% | ğŸ† Best Test Accuracy: {best_acc:.3f}%\")\n",
    "    print(f\"ğŸ“œ Test logs saved to {test_results_path}!\")\n",
    "\n",
    "\n",
    "    global recent_test_acc\n",
    "    recent_test_acc = final_test_acc  # Capture latest test accuracy for next train() call | Store latest test accuracy\n",
    "\n",
    "    test_acc_history.append(final_test_acc)\n",
    "\n",
    "    return final_test_acc  # âœ… Return the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190e50a-187a-4137-b21a-e4e220c84cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:25<00:00, 30.56it/s, Train_acc=3.35, Train_loss=4.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000'] | Gamma1 Grad: ['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 3.350% (Updated)\n",
      "ğŸ“Š Train Accuracy: 3.350% | ğŸ† Best Train Accuracy: 3.350%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 3.350% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.20it/s, Test_acc=5.41, Test_loss=4.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 5.410% | ğŸ† Best Test Accuracy: 5.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "\n",
      "ğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\n",
      "âœ… Activation Parameters Unfrozen! ğŸš€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=1 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=3221649784512 | Mean=1.50000 | Layer 1: ID=3221649931808 | Mean=1.50000 | Layer 2: ID=3221618873808 | Mean=1.50000 | Layer 3: ID=3221618873008 | Mean=1.50000 | Layer 4: ID=3221618872048 | Mean=1.50000 | Layer 5: ID=3221618870928 | Mean=1.50000 | Layer 6: ID=3221618870448 | Mean=1.50000 | Layer 7: ID=3222318891712 | Mean=1.50000 | Layer 8: ID=3222318892592 | Mean=1.50000 | Layer 9: ID=3222318893472 | Mean=1.50000 | Layer 10: ID=3222318894272 | Mean=1.50000 | Layer 11: ID=3222318895152 | Mean=1.50000 | Layer 12: ID=3222318896032 | Mean=1.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.97it/s, Train_acc=6.75, Train_loss=4.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.22781', '2.23194', '2.23034', '2.22686', '2.22855', '2.22689', '2.22713', '2.22528', '2.22704', '2.22714', '2.22265', '2.21825', '2.21159'] | Gamma1 Grad: ['0.00214', '-0.00183', '-0.00286', '-0.00159', '-0.00979', '-0.01015', '-0.00498', '-0.00686', '-0.01170', '-0.01135', '-0.00357', '-0.02437', '-0.00659']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 6.746% (Updated)\n",
      "ğŸ“Š Train Accuracy: 6.746% | ğŸ† Best Train Accuracy: 6.746%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 6.746% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.68it/s, Test_acc=7.64, Test_loss=3.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 7.640% | ğŸ† Best Test Accuracy: 7.640%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=2 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=3221649784512 | Mean=2.22781 | Layer 1: ID=3221649931808 | Mean=2.23194 | Layer 2: ID=3221618873808 | Mean=2.23034 | Layer 3: ID=3221618873008 | Mean=2.22686 | Layer 4: ID=3221618872048 | Mean=2.22855 | Layer 5: ID=3221618870928 | Mean=2.22689 | Layer 6: ID=3221618870448 | Mean=2.22713 | Layer 7: ID=3222318891712 | Mean=2.22528 | Layer 8: ID=3222318892592 | Mean=2.22704 | Layer 9: ID=3222318893472 | Mean=2.22714 | Layer 10: ID=3222318894272 | Mean=2.22265 | Layer 11: ID=3222318895152 | Mean=2.21825 | Layer 12: ID=3222318896032 | Mean=2.21159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.45it/s, Train_acc=10.3, Train_loss=3.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.28495', '2.29992', '2.28982', '2.28479', '2.28348', '2.28275', '2.28155', '2.28322', '2.28172', '2.28178', '2.27952', '2.27889', '2.26284'] | Gamma1 Grad: ['-0.02534', '0.00153', '0.00537', '-0.00349', '-0.00198', '0.00287', '0.00000', '-0.00125', '0.00623', '-0.00278', '-0.00325', '0.02547', '-0.00187']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 10.310% (Updated)\n",
      "ğŸ“Š Train Accuracy: 10.310% | ğŸ† Best Train Accuracy: 10.310%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 10.310% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.15it/s, Test_acc=12.4, Test_loss=3.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 12.390% | ğŸ† Best Test Accuracy: 12.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=3 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=3221649784512 | Mean=2.28495 | Layer 1: ID=3221649931808 | Mean=2.29992 | Layer 2: ID=3221618873808 | Mean=2.28982 | Layer 3: ID=3221618873008 | Mean=2.28479 | Layer 4: ID=3221618872048 | Mean=2.28348 | Layer 5: ID=3221618870928 | Mean=2.28275 | Layer 6: ID=3221618870448 | Mean=2.28155 | Layer 7: ID=3222318891712 | Mean=2.28322 | Layer 8: ID=3222318892592 | Mean=2.28172 | Layer 9: ID=3222318893472 | Mean=2.28178 | Layer 10: ID=3222318894272 | Mean=2.27952 | Layer 11: ID=3222318895152 | Mean=2.27889 | Layer 12: ID=3222318896032 | Mean=2.26284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.40it/s, Train_acc=14.9, Train_loss=3.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.30631', '2.30927', '2.30376', '2.29201', '2.29277', '2.29178', '2.29116', '2.28787', '2.29035', '2.28992', '2.28731', '2.28535', '2.27190'] | Gamma1 Grad: ['0.00481', '0.01261', '0.00507', '0.00230', '0.00955', '-0.00182', '-0.00476', '0.00619', '-0.00535', '-0.01281', '-0.01213', '-0.04024', '-0.02490']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 14.864% (Updated)\n",
      "ğŸ“Š Train Accuracy: 14.864% | ğŸ† Best Train Accuracy: 14.864%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 14.864% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.73it/s, Test_acc=17.4, Test_loss=3.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 17.360% | ğŸ† Best Test Accuracy: 17.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=4 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=3221649784512 | Mean=2.30631 | Layer 1: ID=3221649931808 | Mean=2.30927 | Layer 2: ID=3221618873808 | Mean=2.30376 | Layer 3: ID=3221618873008 | Mean=2.29201 | Layer 4: ID=3221618872048 | Mean=2.29277 | Layer 5: ID=3221618870928 | Mean=2.29178 | Layer 6: ID=3221618870448 | Mean=2.29116 | Layer 7: ID=3222318891712 | Mean=2.28787 | Layer 8: ID=3222318892592 | Mean=2.29035 | Layer 9: ID=3222318893472 | Mean=2.28992 | Layer 10: ID=3222318894272 | Mean=2.28731 | Layer 11: ID=3222318895152 | Mean=2.28535 | Layer 12: ID=3222318896032 | Mean=2.27190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.65it/s, Train_acc=19.4, Train_loss=3.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.30224', '2.31555', '2.30293', '2.29850', '2.29402', '2.29978', '2.29413', '2.29269', '2.29427', '2.29638', '2.29254', '2.28212', '2.28239'] | Gamma1 Grad: ['0.00181', '0.00036', '0.00540', '0.00272', '0.00113', '0.00029', '-0.00641', '-0.00201', '-0.00353', '-0.00813', '-0.00905', '0.01753', '0.00542']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 19.378% (Updated)\n",
      "ğŸ“Š Train Accuracy: 19.378% | ğŸ† Best Train Accuracy: 19.378%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 19.378% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.23it/s, Test_acc=21.4, Test_loss=3.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 21.450% | ğŸ† Best Test Accuracy: 21.450%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.85it/s, Train_acc=23.8, Train_loss=2.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.31415', '2.32279', '2.31610', '2.30188', '2.30212', '2.30470', '2.29551', '2.29570', '2.29787', '2.29307', '2.29076', '2.28335', '2.27907'] | Gamma1 Grad: ['-0.00248', '0.01535', '-0.00592', '-0.01383', '-0.00546', '0.00441', '0.00662', '-0.00304', '-0.00236', '0.00978', '0.00652', '0.01437', '-0.02213']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 23.818% (Updated)\n",
      "ğŸ“Š Train Accuracy: 23.818% | ğŸ† Best Train Accuracy: 23.818%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 23.818% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.27it/s, Test_acc=27.2, Test_loss=2.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 27.250% | ğŸ† Best Test Accuracy: 27.250%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.29it/s, Train_acc=28.1, Train_loss=2.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.32142', '2.31311', '2.30795', '2.28848', '2.30355', '2.30045', '2.29032', '2.29158', '2.28979', '2.28994', '2.29244', '2.28824', '2.29176'] | Gamma1 Grad: ['-0.02193', '-0.01981', '-0.01363', '-0.00726', '-0.00556', '0.00666', '0.00100', '-0.00321', '0.00125', '0.00313', '0.00006', '0.00055', '0.00679']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 28.132% (Updated)\n",
      "ğŸ“Š Train Accuracy: 28.132% | ğŸ† Best Train Accuracy: 28.132%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 28.132% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 78.20it/s, Test_acc=28.9, Test_loss=2.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 28.930% | ğŸ† Best Test Accuracy: 28.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.09it/s, Train_acc=32.1, Train_loss=2.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.31621', '2.31274', '2.30009', '2.29203', '2.29652', '2.29598', '2.29170', '2.29203', '2.29050', '2.29308', '2.29298', '2.28522', '2.29021'] | Gamma1 Grad: ['0.00524', '0.00827', '0.01702', '-0.00313', '-0.00184', '-0.00375', '0.00343', '-0.00218', '-0.00649', '-0.00805', '0.01119', '-0.03516', '0.11920']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 32.114% (Updated)\n",
      "ğŸ“Š Train Accuracy: 32.114% | ğŸ† Best Train Accuracy: 32.114%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 32.114% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.68it/s, Test_acc=33.1, Test_loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 33.130% | ğŸ† Best Test Accuracy: 33.130%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.84it/s, Train_acc=36, Train_loss=2.36]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.32046', '2.31969', '2.30985', '2.29372', '2.29612', '2.29671', '2.28956', '2.29852', '2.29951', '2.30040', '2.30000', '2.27924', '2.29307'] | Gamma1 Grad: ['0.01051', '0.00013', '-0.00165', '0.02623', '0.00533', '0.00015', '0.00153', '-0.00222', '0.00916', '0.00219', '0.00728', '0.00428', '0.05219']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 36.042% (Updated)\n",
      "ğŸ“Š Train Accuracy: 36.042% | ğŸ† Best Train Accuracy: 36.042%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 36.042% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.36it/s, Test_acc=38.6, Test_loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 38.600% | ğŸ† Best Test Accuracy: 38.600%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.57it/s, Train_acc=39.5, Train_loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.32142', '2.31966', '2.31572', '2.29480', '2.30418', '2.29750', '2.28799', '2.30048', '2.29739', '2.29852', '2.29850', '2.27756', '2.29923'] | Gamma1 Grad: ['-0.00198', '-0.02950', '0.00399', '0.00633', '-0.00367', '0.00147', '0.00400', '0.00035', '0.00794', '0.00250', '-0.00556', '0.00774', '0.02601']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 39.496% (Updated)\n",
      "ğŸ“Š Train Accuracy: 39.496% | ğŸ† Best Train Accuracy: 39.496%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 39.496% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.10it/s, Test_acc=42.5, Test_loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 42.480% | ğŸ† Best Test Accuracy: 42.480%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.77it/s, Train_acc=43, Train_loss=2.07]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.31928', '2.32180', '2.31086', '2.29954', '2.29881', '2.30259', '2.28996', '2.29500', '2.29570', '2.29574', '2.29473', '2.28736', '2.29742'] | Gamma1 Grad: ['-0.00746', '0.01075', '-0.01073', '-0.00966', '-0.00376', '0.00669', '0.01324', '0.00472', '0.00775', '-0.00177', '0.00503', '-0.01044', '0.02581']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 43.028% (Updated)\n",
      "ğŸ“Š Train Accuracy: 43.028% | ğŸ† Best Train Accuracy: 43.028%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 43.028% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 10 | TrainLossHist: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.29it/s, Test_acc=43.1, Test_loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 43.140% | ğŸ† Best Test Accuracy: 43.140%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.57it/s, Train_acc=45.5, Train_loss=1.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.31966', '2.31492', '2.31832', '2.29369', '2.32402', '2.29231', '2.28501', '2.29872', '2.30446', '2.29604', '2.29875', '2.27838', '2.30080'] | Gamma1 Grad: ['-0.02763', '0.01788', '-0.00846', '0.01088', '-0.01332', '-0.01867', '-0.00595', '0.00926', '0.00849', '0.00189', '0.00144', '-0.01179', '0.01114']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 45.530% (Updated)\n",
      "ğŸ“Š Train Accuracy: 45.530% | ğŸ† Best Train Accuracy: 45.530%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 45.530% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.11it/s, Test_acc=45.7, Test_loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 45.700% | ğŸ† Best Test Accuracy: 45.700%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.98it/s, Train_acc=48, Train_loss=1.87]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.31196', '2.31069', '2.31805', '2.28527', '2.29437', '2.30077', '2.28592', '2.29637', '2.28881', '2.27865', '2.27965', '2.28884', '2.30791'] | Gamma1 Grad: ['-0.00530', '0.02613', '-0.00464', '-0.00337', '-0.00816', '-0.02771', '-0.00243', '-0.01235', '-0.00783', '-0.00635', '-0.00084', '-0.01586', '0.08617']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 47.998% (Updated)\n",
      "ğŸ“Š Train Accuracy: 47.998% | ğŸ† Best Train Accuracy: 47.998%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 47.998% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.91it/s, Test_acc=48.7, Test_loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 48.680% | ğŸ† Best Test Accuracy: 48.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.84it/s, Train_acc=50.1, Train_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.31734', '2.31753', '2.31352', '2.31238', '2.30023', '2.30412', '2.29641', '2.31442', '2.30533', '2.29892', '2.29191', '2.28383', '2.30074'] | Gamma1 Grad: ['0.02165', '0.01001', '-0.00676', '-0.00343', '-0.01895', '-0.03043', '0.00889', '0.00102', '0.00181', '0.00008', '-0.00672', '0.01165', '0.07934']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 50.116% (Updated)\n",
      "ğŸ“Š Train Accuracy: 50.116% | ğŸ† Best Train Accuracy: 50.116%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 50.116% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.02it/s, Test_acc=50.4, Test_loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 50.350% | ğŸ† Best Test Accuracy: 50.350%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.78it/s, Train_acc=52.8, Train_loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.29726', '2.32906', '2.32360', '2.29728', '2.30397', '2.30010', '2.28855', '2.30594', '2.29265', '2.29400', '2.29575', '2.27924', '2.30591'] | Gamma1 Grad: ['0.00013', '0.00418', '0.00646', '-0.00422', '-0.01305', '-0.01072', '-0.00814', '-0.00334', '0.00721', '0.00544', '0.00330', '0.00752', '-0.04276']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 52.774% (Updated)\n",
      "ğŸ“Š Train Accuracy: 52.774% | ğŸ† Best Train Accuracy: 52.774%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 52.774% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.65it/s, Test_acc=51.5, Test_loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 51.490% | ğŸ† Best Test Accuracy: 51.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.97it/s, Train_acc=54.3, Train_loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['2.30380', '2.31925', '2.30620', '2.30685', '2.31286', '2.30606', '2.29835', '2.29733', '2.30664', '2.29651', '2.30844', '2.28967', '2.29497'] | Gamma1 Grad: ['0.03183', '0.04129', '-0.00731', '0.03877', '-0.03744', '0.01009', '-0.00972', '0.02304', '0.01850', '0.03711', '0.03630', '-0.00540', '0.05090']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 54.280% (Updated)\n",
      "ğŸ“Š Train Accuracy: 54.280% | ğŸ† Best Train Accuracy: 54.280%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 54.280% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.67it/s, Test_acc=51.5, Test_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 51.520% | ğŸ† Best Test Accuracy: 51.520%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.57it/s, Train_acc=55.9, Train_loss=1.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.29722', '2.30483', '2.31162', '2.29936', '2.30591', '2.31441', '2.28593', '2.30892', '2.30312', '2.29698', '2.29717', '2.28984', '2.30177'] | Gamma1 Grad: ['0.03081', '-0.00621', '0.05532', '0.01109', '-0.01888', '-0.00093', '0.00612', '-0.00126', '-0.00235', '0.00362', '0.00397', '0.02038', '-0.00609']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 55.922% (Updated)\n",
      "ğŸ“Š Train Accuracy: 55.922% | ğŸ† Best Train Accuracy: 55.922%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 55.922% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.21it/s, Test_acc=52.1, Test_loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 52.130% | ğŸ† Best Test Accuracy: 52.130%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.28it/s, Train_acc=57.9, Train_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['2.32187', '2.32379', '2.30870', '2.30216', '2.29751', '2.30796', '2.28942', '2.30735', '2.29680', '2.28853', '2.28486', '2.29031', '2.32087'] | Gamma1 Grad: ['-0.04454', '0.01188', '0.02123', '0.00308', '-0.01219', '-0.02880', '-0.00980', '0.00096', '0.00555', '0.00322', '-0.00585', '0.00792', '0.01792']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 57.942% (Updated)\n",
      "ğŸ“Š Train Accuracy: 57.942% | ğŸ† Best Train Accuracy: 57.942%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 57.942% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.72it/s, Test_acc=54.5, Test_loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 54.510% | ğŸ† Best Test Accuracy: 54.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.92it/s, Train_acc=59.1, Train_loss=1.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.31113', '2.31919', '2.29941', '2.30979', '2.31325', '2.31665', '2.30034', '2.30369', '2.30731', '2.30514', '2.29644', '2.28492', '2.31837'] | Gamma1 Grad: ['0.01504', '0.00180', '0.02244', '0.02720', '0.00618', '0.02109', '0.00124', '-0.00162', '-0.01528', '-0.00076', '0.01075', '0.00812', '0.06281']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 59.066% (Updated)\n",
      "ğŸ“Š Train Accuracy: 59.066% | ğŸ† Best Train Accuracy: 59.066%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 59.066% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.65it/s, Test_acc=55.2, Test_loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 55.220% | ğŸ† Best Test Accuracy: 55.220%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.82it/s, Train_acc=60.7, Train_loss=1.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['2.29846', '2.32449', '2.29952', '2.28616', '2.30113', '2.29709', '2.28581', '2.30473', '2.30122', '2.29096', '2.29493', '2.28777', '2.30565'] | Gamma1 Grad: ['0.00452', '0.02699', '-0.01499', '0.00952', '0.01832', '0.00749', '-0.00690', '-0.00787', '-0.00816', '0.00459', '-0.00179', '-0.01902', '-0.02310']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 60.668% (Updated)\n",
      "ğŸ“Š Train Accuracy: 60.668% | ğŸ† Best Train Accuracy: 60.668%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 60.668% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 78.95it/s, Test_acc=56, Test_loss=1.57]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 56.020% | ğŸ† Best Test Accuracy: 56.020%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.68it/s, Train_acc=62.3, Train_loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.29910', '2.30983', '2.30670', '2.29628', '2.30387', '2.30368', '2.29536', '2.30197', '2.29873', '2.29339', '2.28830', '2.28017', '2.30086'] | Gamma1 Grad: ['0.03703', '-0.00483', '-0.03777', '0.03336', '-0.01157', '-0.02143', '-0.01755', '0.00067', '-0.01410', '0.00264', '-0.00421', '-0.00930', '-0.05142']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 62.322% (Updated)\n",
      "ğŸ“Š Train Accuracy: 62.322% | ğŸ† Best Train Accuracy: 62.322%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 62.322% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 20 | TrainLossHist: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.57it/s, Test_acc=56.7, Test_loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 56.720% | ğŸ† Best Test Accuracy: 56.720%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.69it/s, Train_acc=63.8, Train_loss=1.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['2.29781', '2.30470', '2.30654', '2.30157', '2.30049', '2.31149', '2.30472', '2.30247', '2.29921', '2.29333', '2.30207', '2.29546', '2.30180'] | Gamma1 Grad: ['0.05176', '0.02713', '0.03274', '0.02215', '0.00412', '0.00089', '-0.02754', '-0.00137', '0.02344', '0.01192', '0.02159', '0.00542', '0.03473']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 63.832% (Updated)\n",
      "ğŸ“Š Train Accuracy: 63.832% | ğŸ† Best Train Accuracy: 63.832%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 63.832% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.45it/s, Test_acc=57.5, Test_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 57.510% | ğŸ† Best Test Accuracy: 57.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.05it/s, Train_acc=64.9, Train_loss=1.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.30790', '2.30775', '2.29159', '2.29652', '2.29851', '2.30437', '2.30584', '2.30163', '2.29851', '2.29555', '2.29373', '2.29214', '2.30746'] | Gamma1 Grad: ['0.00015', '-0.02006', '0.00101', '-0.00477', '-0.02726', '-0.01711', '-0.01066', '0.00185', '-0.01524', '-0.00688', '-0.00889', '-0.01000', '0.01651']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 64.900% (Updated)\n",
      "ğŸ“Š Train Accuracy: 64.900% | ğŸ† Best Train Accuracy: 64.900%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 64.900% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.49it/s, Test_acc=58.1, Test_loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 58.080% | ğŸ† Best Test Accuracy: 58.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.43it/s, Train_acc=66, Train_loss=1.17]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.29845', '2.30768', '2.30157', '2.29934', '2.29644', '2.31590', '2.29214', '2.30766', '2.29995', '2.29329', '2.29362', '2.28193', '2.30705'] | Gamma1 Grad: ['-0.03564', '-0.00499', '-0.00340', '-0.01269', '0.00549', '0.01679', '-0.01239', '0.00174', '-0.00093', '-0.00611', '-0.00506', '0.02948', '0.01708']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 65.972% (Updated)\n",
      "ğŸ“Š Train Accuracy: 65.972% | ğŸ† Best Train Accuracy: 65.972%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 65.972% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.38it/s, Test_acc=58.2, Test_loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 58.230% | ğŸ† Best Test Accuracy: 58.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.61it/s, Train_acc=67, Train_loss=1.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.31022', '2.31449', '2.30324', '2.30607', '2.29971', '2.31038', '2.29110', '2.30284', '2.30382', '2.29931', '2.30365', '2.28902', '2.30252'] | Gamma1 Grad: ['-0.03481', '0.00177', '0.06209', '0.07780', '0.04093', '-0.05740', '0.01015', '-0.00982', '0.00734', '0.00028', '-0.00768', '0.01966', '-0.01109']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 67.018% (Updated)\n",
      "ğŸ“Š Train Accuracy: 67.018% | ğŸ† Best Train Accuracy: 67.018%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 67.018% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.06it/s, Test_acc=59.2, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 59.250% | ğŸ† Best Test Accuracy: 59.250%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.96it/s, Train_acc=68.3, Train_loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['2.31770', '2.30086', '2.30377', '2.30963', '2.30241', '2.30343', '2.29435', '2.29874', '2.28458', '2.28644', '2.28971', '2.28714', '2.31118'] | Gamma1 Grad: ['-0.00452', '0.02779', '0.02619', '0.01405', '0.01710', '-0.02609', '0.00080', '0.00706', '-0.00162', '-0.00098', '0.00674', '0.01356', '-0.04145']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 68.258% (Updated)\n",
      "ğŸ“Š Train Accuracy: 68.258% | ğŸ† Best Train Accuracy: 68.258%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 68.258% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.38it/s, Test_acc=58.9, Test_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 58.860% | ğŸ† Best Test Accuracy: 59.250%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.67it/s, Train_acc=69.7, Train_loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.30601', '2.31367', '2.30129', '2.29443', '2.30458', '2.30858', '2.30101', '2.31003', '2.29887', '2.29083', '2.28934', '2.28555', '2.29873'] | Gamma1 Grad: ['0.00421', '0.02098', '-0.01823', '-0.05779', '-0.04655', '-0.01248', '0.02474', '-0.00686', '-0.01909', '-0.00686', '-0.02682', '0.01112', '-0.02654']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 69.674% (Updated)\n",
      "ğŸ“Š Train Accuracy: 69.674% | ğŸ† Best Train Accuracy: 69.674%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 69.674% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.85it/s, Test_acc=60.2, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.250% | ğŸ† Best Test Accuracy: 60.250%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.91it/s, Train_acc=71.1, Train_loss=0.989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['2.29767', '2.30704', '2.29488', '2.29474', '2.29882', '2.29773', '2.30306', '2.30961', '2.30155', '2.29771', '2.29456', '2.29709', '2.29927'] | Gamma1 Grad: ['-0.00722', '-0.02544', '-0.04767', '0.00775', '0.05176', '-0.02527', '-0.00471', '0.02316', '-0.00410', '0.00147', '0.00958', '0.00570', '0.08519']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 71.122% (Updated)\n",
      "ğŸ“Š Train Accuracy: 71.122% | ğŸ† Best Train Accuracy: 71.122%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 71.122% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.35it/s, Test_acc=60.4, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.350% | ğŸ† Best Test Accuracy: 60.350%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.06it/s, Train_acc=71.9, Train_loss=0.957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.29976', '2.31313', '2.30900', '2.29777', '2.30406', '2.30931', '2.30961', '2.30382', '2.29660', '2.29107', '2.30057', '2.28320', '2.30699'] | Gamma1 Grad: ['0.01363', '0.01483', '0.08429', '-0.03017', '-0.02034', '-0.00287', '0.01847', '0.00262', '0.01497', '0.00798', '-0.00528', '0.01161', '0.00951']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 71.928% (Updated)\n",
      "ğŸ“Š Train Accuracy: 71.928% | ğŸ† Best Train Accuracy: 71.928%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 71.928% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.13it/s, Test_acc=60.5, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.500% | ğŸ† Best Test Accuracy: 60.500%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.73it/s, Train_acc=72.8, Train_loss=0.92] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['2.29369', '2.31238', '2.30001', '2.28996', '2.29793', '2.29958', '2.30356', '2.30937', '2.30111', '2.29370', '2.29497', '2.28345', '2.29472'] | Gamma1 Grad: ['0.03228', '-0.03301', '0.03223', '0.00272', '-0.02546', '-0.00248', '0.02307', '-0.00969', '0.00689', '0.00076', '0.00573', '-0.04979', '0.06946']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 72.794% (Updated)\n",
      "ğŸ“Š Train Accuracy: 72.794% | ğŸ† Best Train Accuracy: 72.794%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 72.794% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.47it/s, Test_acc=61.1, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.110% | ğŸ† Best Test Accuracy: 61.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.24it/s, Train_acc=74, Train_loss=0.888]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.31063', '2.30595', '2.30239', '2.29111', '2.29203', '2.30859', '2.29932', '2.30925', '2.29856', '2.29266', '2.29706', '2.28979', '2.30701'] | Gamma1 Grad: ['-0.00204', '0.00710', '0.03415', '-0.01476', '0.00927', '0.01283', '-0.00278', '0.00646', '-0.00325', '-0.01156', '-0.00354', '0.01063', '-0.01097']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 73.970% (Updated)\n",
      "ğŸ“Š Train Accuracy: 73.970% | ğŸ† Best Train Accuracy: 73.970%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 73.970% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 30 | TrainLossHist: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.84it/s, Test_acc=60.9, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 60.940% | ğŸ† Best Test Accuracy: 61.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.62it/s, Train_acc=75.1, Train_loss=0.843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.30952', '2.30844', '2.30820', '2.30246', '2.30653', '2.31927', '2.30634', '2.31106', '2.28875', '2.29092', '2.28954', '2.29612', '2.30004'] | Gamma1 Grad: ['0.03521', '-0.00844', '0.02507', '-0.04943', '0.03890', '0.00519', '0.00061', '0.02114', '0.00043', '0.00063', '-0.00370', '-0.00162', '-0.04301']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 75.136% (Updated)\n",
      "ğŸ“Š Train Accuracy: 75.136% | ğŸ† Best Train Accuracy: 75.136%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 75.136% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.27it/s, Test_acc=61.7, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.700% | ğŸ† Best Test Accuracy: 61.700%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.66it/s, Train_acc=76.1, Train_loss=0.813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.30435', '2.31132', '2.30785', '2.28698', '2.28739', '2.31443', '2.29624', '2.31184', '2.29533', '2.28966', '2.28812', '2.28449', '2.29900'] | Gamma1 Grad: ['0.00014', '0.02445', '-0.01852', '-0.03790', '-0.04485', '-0.02342', '0.01051', '0.00452', '-0.00787', '-0.01614', '-0.01631', '0.01136', '-0.01902']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 76.108% (Updated)\n",
      "ğŸ“Š Train Accuracy: 76.108% | ğŸ† Best Train Accuracy: 76.108%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 76.108% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.72it/s, Test_acc=61.7, Test_loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.720% | ğŸ† Best Test Accuracy: 61.720%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.56it/s, Train_acc=76.7, Train_loss=0.788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.30476', '2.31741', '2.29070', '2.28799', '2.27523', '2.30156', '2.30045', '2.31186', '2.30518', '2.28983', '2.29220', '2.30107', '2.30563'] | Gamma1 Grad: ['0.04424', '0.02099', '-0.03376', '-0.01186', '0.02568', '-0.02680', '0.01675', '-0.01025', '0.00205', '0.00661', '0.00179', '-0.01862', '0.04220']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 76.668% (Updated)\n",
      "ğŸ“Š Train Accuracy: 76.668% | ğŸ† Best Train Accuracy: 76.668%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 76.668% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.06it/s, Test_acc=62, Test_loss=1.47]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.020% | ğŸ† Best Test Accuracy: 62.020%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.84it/s, Train_acc=77.6, Train_loss=0.756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.30758', '2.32514', '2.27961', '2.29399', '2.28906', '2.32018', '2.30257', '2.30757', '2.30910', '2.30992', '2.29582', '2.29625', '2.30473'] | Gamma1 Grad: ['-0.02655', '-0.05279', '-0.05109', '0.05355', '-0.02002', '0.03114', '-0.01207', '0.00907', '-0.00247', '0.00751', '0.00294', '-0.00565', '-0.00107']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 77.636% (Updated)\n",
      "ğŸ“Š Train Accuracy: 77.636% | ğŸ† Best Train Accuracy: 77.636%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 77.636% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.51it/s, Test_acc=62.4, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.370% | ğŸ† Best Test Accuracy: 62.370%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.04it/s, Train_acc=78.1, Train_loss=0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00144 | Gamma1: ['2.29194', '2.30536', '2.29141', '2.28532', '2.29969', '2.31554', '2.30761', '2.30635', '2.29515', '2.28279', '2.30201', '2.28377', '2.29522'] | Gamma1 Grad: ['-0.00472', '-0.03480', '0.02726', '-0.01174', '-0.03089', '0.01874', '-0.00016', '0.00726', '-0.00583', '-0.00162', '-0.00514', '0.01075', '-0.00021']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 78.128% (Updated)\n",
      "ğŸ“Š Train Accuracy: 78.128% | ğŸ† Best Train Accuracy: 78.128%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 78.128% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.97it/s, Test_acc=62, Test_loss=1.49]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.030% | ğŸ† Best Test Accuracy: 62.370%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.88it/s, Train_acc=79.2, Train_loss=0.698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.31239', '2.29953', '2.30658', '2.29501', '2.29534', '2.31505', '2.30099', '2.30881', '2.30488', '2.30327', '2.31855', '2.28952', '2.30512'] | Gamma1 Grad: ['0.03113', '0.03239', '-0.04167', '0.00264', '0.07634', '-0.00265', '0.02421', '0.01064', '-0.02298', '-0.00884', '-0.00146', '-0.01386', '0.05819']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 79.204% (Updated)\n",
      "ğŸ“Š Train Accuracy: 79.204% | ğŸ† Best Train Accuracy: 79.204%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 79.204% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.06it/s, Test_acc=62.5, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.550% | ğŸ† Best Test Accuracy: 62.550%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.18it/s, Train_acc=79.9, Train_loss=0.675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00139 | Gamma1: ['2.31432', '2.30594', '2.29784', '2.27776', '2.28794', '2.31513', '2.31010', '2.31644', '2.30195', '2.28475', '2.28678', '2.30030', '2.30897'] | Gamma1 Grad: ['0.04678', '0.02253', '0.07674', '0.04595', '0.02970', '-0.03860', '0.00567', '0.00462', '0.00536', '-0.02011', '-0.01402', '0.02066', '0.00373']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 79.858% (Updated)\n",
      "ğŸ“Š Train Accuracy: 79.858% | ğŸ† Best Train Accuracy: 79.858%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 79.858% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.08it/s, Test_acc=63.2, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.160% | ğŸ† Best Test Accuracy: 63.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.51it/s, Train_acc=80.6, Train_loss=0.647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.28681', '2.32175', '2.30087', '2.30486', '2.29024', '2.31553', '2.30955', '2.31457', '2.29899', '2.29428', '2.29766', '2.29175', '2.30110'] | Gamma1 Grad: ['-0.00060', '-0.01459', '0.01893', '-0.00068', '-0.02701', '0.03421', '-0.00691', '-0.00424', '-0.00844', '-0.01204', '0.00087', '-0.00487', '0.05425']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 80.580% (Updated)\n",
      "ğŸ“Š Train Accuracy: 80.580% | ğŸ† Best Train Accuracy: 80.580%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 80.580% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.74it/s, Test_acc=62.8, Test_loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.820% | ğŸ† Best Test Accuracy: 63.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.69it/s, Train_acc=81.4, Train_loss=0.624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00133 | Gamma1: ['2.30686', '2.30055', '2.30360', '2.28614', '2.29703', '2.30138', '2.30079', '2.30171', '2.29068', '2.29874', '2.28797', '2.29005', '2.28554'] | Gamma1 Grad: ['-0.03932', '-0.02419', '0.05869', '-0.04950', '0.00183', '0.09320', '-0.01250', '-0.00676', '-0.00691', '0.00412', '-0.02452', '0.00006', '-0.03017']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 81.370% (Updated)\n",
      "ğŸ“Š Train Accuracy: 81.370% | ğŸ† Best Train Accuracy: 81.370%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 81.370% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.56it/s, Test_acc=63.1, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.090% | ğŸ† Best Test Accuracy: 63.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.74it/s, Train_acc=82.1, Train_loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['2.33136', '2.34334', '2.33062', '2.30765', '2.30926', '2.32614', '2.32688', '2.33307', '2.32712', '2.30110', '2.30149', '2.29593', '2.30689'] | Gamma1 Grad: ['-0.05275', '-0.01099', '-0.00440', '-0.06510', '0.03724', '-0.04100', '0.01229', '0.03739', '-0.01471', '-0.00213', '-0.00590', '0.02515', '-0.04149']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.096% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.096% | ğŸ† Best Train Accuracy: 82.096%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.096% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 40 | TrainLossHist: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.54it/s, Test_acc=62.4, Test_loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.410% | ğŸ† Best Test Accuracy: 63.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.72it/s, Train_acc=82.7, Train_loss=0.58] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00125 | Gamma1: ['2.32947', '2.37216', '2.31669', '2.29645', '2.30316', '2.33228', '2.33981', '2.35248', '2.32938', '2.29806', '2.30943', '2.27575', '2.31141'] | Gamma1 Grad: ['0.00911', '0.02416', '0.07433', '0.08517', '-0.00794', '-0.01520', '0.00338', '0.01145', '-0.01955', '0.00032', '0.00807', '0.00497', '0.05881']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.658% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.658% | ğŸ† Best Train Accuracy: 82.658%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.658% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.94it/s, Test_acc=63, Test_loss=1.6]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.000% | ğŸ† Best Test Accuracy: 63.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.07it/s, Train_acc=83.1, Train_loss=0.561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.33806', '2.35344', '2.32601', '2.25947', '2.29650', '2.35721', '2.34956', '2.36621', '2.34551', '2.29538', '2.30789', '2.28610', '2.31971'] | Gamma1 Grad: ['0.02828', '0.05752', '-0.00135', '-0.01550', '0.03976', '0.00242', '-0.00724', '0.01148', '-0.00573', '0.00162', '0.01132', '-0.01146', '-0.02340']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.126% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.126% | ğŸ† Best Train Accuracy: 83.126%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.126% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.32it/s, Test_acc=63, Test_loss=1.58]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.970% | ğŸ† Best Test Accuracy: 63.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.69it/s, Train_acc=83.5, Train_loss=0.544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00115 | Gamma1: ['2.34547', '2.36205', '2.33635', '2.28150', '2.28338', '2.34283', '2.34492', '2.36602', '2.35828', '2.29453', '2.30034', '2.28539', '2.30051'] | Gamma1 Grad: ['-0.00490', '0.02505', '-0.00041', '0.02546', '0.03747', '0.00611', '-0.00683', '0.01822', '0.01341', '-0.00563', '-0.01589', '0.00425', '-0.00947']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.506% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.506% | ğŸ† Best Train Accuracy: 83.506%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.506% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.88it/s, Test_acc=63.3, Test_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.310% | ğŸ† Best Test Accuracy: 63.310%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.75it/s, Train_acc=84.7, Train_loss=0.512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['2.32292', '2.35981', '2.33737', '2.26570', '2.29331', '2.31645', '2.33163', '2.35698', '2.33542', '2.29131', '2.28067', '2.29501', '2.28425'] | Gamma1 Grad: ['0.00546', '-0.03363', '-0.01908', '-0.04748', '0.02454', '0.02952', '-0.02530', '0.00984', '0.00685', '0.00112', '0.00477', '-0.03587', '0.00356']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 84.650% (Updated)\n",
      "ğŸ“Š Train Accuracy: 84.650% | ğŸ† Best Train Accuracy: 84.650%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 84.650% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.11it/s, Test_acc=63.9, Test_loss=1.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.930% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.85it/s, Train_acc=84.8, Train_loss=0.51] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00105 | Gamma1: ['2.30832', '2.33956', '2.32100', '2.27127', '2.31466', '2.34667', '2.33196', '2.36407', '2.33022', '2.31035', '2.29988', '2.30045', '2.31439'] | Gamma1 Grad: ['-0.00338', '-0.00610', '0.10069', '-0.00239', '0.12990', '0.05062', '0.06462', '0.00490', '0.01372', '-0.01074', '-0.00317', '0.03003', '0.05946']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 84.816% (Updated)\n",
      "ğŸ“Š Train Accuracy: 84.816% | ğŸ† Best Train Accuracy: 84.816%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 84.816% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.97it/s, Test_acc=62.9, Test_loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.890% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.67it/s, Train_acc=85.4, Train_loss=0.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.33222', '2.35139', '2.30130', '2.29470', '2.31325', '2.33473', '2.32854', '2.36175', '2.33447', '2.29676', '2.30092', '2.29102', '2.31822'] | Gamma1 Grad: ['0.02481', '-0.03977', '0.01905', '-0.02581', '0.04392', '0.00275', '0.02594', '0.00822', '-0.00621', '0.00180', '-0.00219', '0.01818', '0.02164']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.432% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.432% | ğŸ† Best Train Accuracy: 85.432%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.432% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.29it/s, Test_acc=63.6, Test_loss=1.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.620% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.72it/s, Train_acc=86, Train_loss=0.467]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00094 | Gamma1: ['2.30738', '2.32976', '2.29650', '2.29535', '2.26237', '2.30227', '2.35612', '2.34873', '2.33897', '2.28824', '2.30356', '2.28593', '2.32352'] | Gamma1 Grad: ['-0.08330', '-0.04527', '-0.03087', '-0.02044', '-0.02348', '0.00715', '0.00311', '0.03315', '0.03919', '-0.00889', '-0.00901', '0.02717', '-0.03922']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.010% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.010% | ğŸ† Best Train Accuracy: 86.010%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.010% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.03it/s, Test_acc=63.7, Test_loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.730% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.05it/s, Train_acc=86.3, Train_loss=0.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['2.32525', '2.32393', '2.29301', '2.28244', '2.27520', '2.32190', '2.33149', '2.35802', '2.32900', '2.30604', '2.29741', '2.27436', '2.31522'] | Gamma1 Grad: ['-0.03080', '-0.08135', '0.04696', '0.07100', '-0.02280', '0.10068', '-0.00006', '-0.00260', '-0.00239', '-0.02667', '-0.01663', '-0.01116', '0.06763']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.298% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.298% | ğŸ† Best Train Accuracy: 86.298%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.298% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.65it/s, Test_acc=63.9, Test_loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.850% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.80it/s, Train_acc=87, Train_loss=0.434]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00083 | Gamma1: ['2.31640', '2.33403', '2.29200', '2.28098', '2.25696', '2.30195', '2.34088', '2.35507', '2.34269', '2.29620', '2.28025', '2.28765', '2.30027'] | Gamma1 Grad: ['0.00012', '-0.06000', '-0.11861', '0.03184', '0.09177', '-0.13533', '-0.02341', '-0.00848', '0.00339', '-0.00136', '-0.00270', '0.02982', '0.09063']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.020% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.020% | ğŸ† Best Train Accuracy: 87.020%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.020% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.49it/s, Test_acc=62.6, Test_loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.590% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.51it/s, Train_acc=87.4, Train_loss=0.426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.33893', '2.35919', '2.32121', '2.26243', '2.27724', '2.31700', '2.33431', '2.34820', '2.32272', '2.29583', '2.30447', '2.29731', '2.31759'] | Gamma1 Grad: ['-0.02327', '-0.01306', '-0.00604', '-0.02763', '0.01516', '0.02276', '-0.04957', '-0.00701', '-0.02247', '-0.00387', '0.00838', '-0.02550', '-0.05998']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.390% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.390% | ğŸ† Best Train Accuracy: 87.390%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.390% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 50 | TrainLossHist: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.09it/s, Test_acc=63.5, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.510% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 51 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 20.90it/s, Train_acc=87.7, Train_loss=0.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00072 | Gamma1: ['2.33369', '2.35158', '2.32529', '2.27995', '2.25155', '2.31727', '2.33873', '2.34407', '2.33675', '2.29343', '2.28250', '2.29582', '2.32162'] | Gamma1 Grad: ['0.01790', '0.04359', '0.00748', '-0.02848', '0.02535', '0.00482', '-0.01370', '0.04269', '0.00326', '0.00323', '0.00699', '0.02388', '0.00016']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.686% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.686% | ğŸ† Best Train Accuracy: 87.686%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.686% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.33it/s, Test_acc=63.4, Test_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.400% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 52 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 21.01it/s, Train_acc=88, Train_loss=0.402]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['2.33229', '2.34038', '2.31033', '2.28408', '2.27300', '2.31502', '2.35191', '2.34731', '2.33524', '2.29279', '2.29282', '2.28044', '2.29876'] | Gamma1 Grad: ['-0.03734', '-0.01635', '-0.04697', '0.01099', '0.00168', '-0.05262', '-0.00483', '0.03606', '-0.02178', '0.00265', '0.00613', '0.00999', '0.00967']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.992% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.992% | ğŸ† Best Train Accuracy: 87.992%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.992% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.16it/s, Test_acc=63, Test_loss=1.73]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.020% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 53 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 20.95it/s, Train_acc=88.5, Train_loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00061 | Gamma1: ['2.32263', '2.34895', '2.30280', '2.26835', '2.26605', '2.31732', '2.34300', '2.35898', '2.33221', '2.30129', '2.29644', '2.27754', '2.30537'] | Gamma1 Grad: ['0.04171', '-0.05552', '0.03754', '0.02378', '-0.00302', '-0.00801', '0.00577', '0.03592', '-0.00275', '0.00614', '0.01265', '0.01221', '-0.01024']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.468% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.468% | ğŸ† Best Train Accuracy: 88.468%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.468% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.16it/s, Test_acc=63.6, Test_loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.620% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 54 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.32it/s, Train_acc=88.6, Train_loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.31460', '2.35510', '2.29959', '2.26647', '2.27287', '2.29635', '2.33171', '2.34861', '2.32575', '2.30803', '2.30424', '2.28975', '2.30018'] | Gamma1 Grad: ['0.00024', '-0.00267', '-0.05106', '-0.00876', '0.09718', '-0.06602', '0.03791', '-0.01075', '-0.00168', '-0.00449', '-0.00089', '-0.04363', '-0.08607']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.600% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.600% | ğŸ† Best Train Accuracy: 88.600%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.600% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 75.87it/s, Test_acc=63.2, Test_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.160% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 20.91it/s, Train_acc=89.2, Train_loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00050 | Gamma1: ['2.32922', '2.34969', '2.29436', '2.27653', '2.26761', '2.29653', '2.34285', '2.35249', '2.33239', '2.30971', '2.30428', '2.28685', '2.29486'] | Gamma1 Grad: ['-0.00319', '0.03100', '-0.06059', '0.01875', '0.02231', '0.05549', '-0.00964', '0.01978', '-0.00020', '-0.00595', '0.00052', '0.01783', '0.03661']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.214% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.214% | ğŸ† Best Train Accuracy: 89.214%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.214% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.26it/s, Test_acc=63.7, Test_loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.710% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 21.04it/s, Train_acc=89.5, Train_loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.31959', '2.33937', '2.30709', '2.27375', '2.26156', '2.29229', '2.33777', '2.35670', '2.31811', '2.29626', '2.30865', '2.28139', '2.28868'] | Gamma1 Grad: ['-0.04274', '-0.05651', '-0.02914', '-0.02687', '-0.04504', '-0.07680', '-0.02483', '-0.00664', '-0.02073', '0.00999', '0.01415', '0.01049', '0.02464']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.536% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.536% | ğŸ† Best Train Accuracy: 89.536%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.536% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.61it/s, Test_acc=63.9, Test_loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.900% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.41it/s, Train_acc=89.8, Train_loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00040 | Gamma1: ['2.31389', '2.32869', '2.30086', '2.28091', '2.26016', '2.28838', '2.34183', '2.35188', '2.31572', '2.30699', '2.30294', '2.27387', '2.29280'] | Gamma1 Grad: ['0.00865', '-0.03294', '-0.01895', '-0.00679', '-0.00128', '0.00487', '-0.02717', '0.02518', '0.00528', '-0.00288', '0.01173', '-0.00562', '-0.01642']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.814% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.814% | ğŸ† Best Train Accuracy: 89.814%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.814% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.47it/s, Test_acc=63.4, Test_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.420% | ğŸ† Best Test Accuracy: 63.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.32it/s, Train_acc=89.9, Train_loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.31371', '2.32491', '2.30433', '2.26407', '2.24695', '2.29005', '2.34352', '2.36345', '2.32192', '2.29905', '2.29554', '2.26659', '2.28116'] | Gamma1 Grad: ['-0.00582', '0.01063', '0.04126', '0.02849', '0.01244', '0.02821', '0.03146', '-0.00647', '-0.00036', '0.00087', '0.00956', '-0.00443', '0.00460']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.854% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.854% | ğŸ† Best Train Accuracy: 89.854%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.854% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.92it/s, Test_acc=64, Test_loss=1.77]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.040% | ğŸ† Best Test Accuracy: 64.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.39it/s, Train_acc=90.2, Train_loss=0.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00030 | Gamma1: ['2.31018', '2.31457', '2.29031', '2.29314', '2.24215', '2.31774', '2.32638', '2.35305', '2.33287', '2.30256', '2.29936', '2.27195', '2.29365'] | Gamma1 Grad: ['0.01909', '-0.01555', '-0.02121', '-0.00298', '-0.01574', '0.01247', '-0.06947', '0.01617', '-0.01527', '0.00747', '-0.00692', '-0.00718', '-0.00610']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.222% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.222% | ğŸ† Best Train Accuracy: 90.222%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.222% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 78.94it/s, Test_acc=64, Test_loss=1.74]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.990% | ğŸ† Best Test Accuracy: 64.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.37it/s, Train_acc=90.7, Train_loss=0.319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['2.33697', '2.32474', '2.28349', '2.28441', '2.25168', '2.30406', '2.33694', '2.36668', '2.35135', '2.29994', '2.30853', '2.27139', '2.28657'] | Gamma1 Grad: ['-0.00195', '-0.03110', '-0.06345', '-0.04370', '0.04514', '-0.01007', '0.00419', '0.01363', '0.02175', '-0.00956', '0.00472', '-0.00050', '0.02000']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.660% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.660% | ğŸ† Best Train Accuracy: 90.660%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.660% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 60 | TrainLossHist: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.30it/s, Test_acc=63.7, Test_loss=1.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.670% | ğŸ† Best Test Accuracy: 64.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.42it/s, Train_acc=90.9, Train_loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00022 | Gamma1: ['2.34364', '2.33039', '2.29353', '2.24286', '2.24573', '2.30299', '2.34840', '2.35485', '2.36064', '2.29772', '2.29708', '2.28305', '2.29380'] | Gamma1 Grad: ['0.06610', '0.02264', '0.03101', '-0.01425', '-0.05080', '0.07446', '0.03297', '0.00462', '0.04005', '0.01536', '-0.01548', '-0.01146', '0.01561']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.868% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.868% | ğŸ† Best Train Accuracy: 90.868%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.868% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.26it/s, Test_acc=63.8, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.800% | ğŸ† Best Test Accuracy: 64.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 20.82it/s, Train_acc=91.1, Train_loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.34745', '2.36885', '2.29500', '2.24491', '2.23677', '2.26914', '2.36499', '2.37821', '2.35563', '2.29854', '2.29246', '2.28033', '2.30334'] | Gamma1 Grad: ['-0.05486', '-0.08639', '0.04261', '-0.14461', '-0.03979', '0.02026', '0.01896', '-0.05187', '0.01693', '0.01838', '0.01688', '-0.01030', '0.00269']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.098% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.098% | ğŸ† Best Train Accuracy: 91.098%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.098% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.40it/s, Test_acc=63.7, Test_loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.680% | ğŸ† Best Test Accuracy: 64.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.42it/s, Train_acc=91.1, Train_loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00016 | Gamma1: ['2.32612', '2.35291', '2.29314', '2.26358', '2.23786', '2.27426', '2.36108', '2.38199', '2.36280', '2.28715', '2.30125', '2.26587', '2.29496'] | Gamma1 Grad: ['0.01912', '-0.01033', '0.05708', '-0.02031', '-0.04062', '0.04534', '-0.03251', '-0.02463', '0.00545', '-0.00666', '-0.00461', '-0.01482', '-0.03502']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.138% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.138% | ğŸ† Best Train Accuracy: 91.138%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.138% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.67it/s, Test_acc=63.7, Test_loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.710% | ğŸ† Best Test Accuracy: 64.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.44it/s, Train_acc=91.6, Train_loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['2.31613', '2.36160', '2.30468', '2.26827', '2.25158', '2.28318', '2.35115', '2.38056', '2.35536', '2.29402', '2.29749', '2.27888', '2.29723'] | Gamma1 Grad: ['-0.00132', '-0.01195', '0.00443', '-0.02012', '-0.01132', '0.03921', '-0.00276', '0.03757', '-0.00173', '-0.00552', '0.00285', '0.01311', '-0.05308']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.564% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.564% | ğŸ† Best Train Accuracy: 91.564%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.564% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.47it/s, Test_acc=64.1, Test_loss=1.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.120% | ğŸ† Best Test Accuracy: 64.120%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 20.76it/s, Train_acc=91.9, Train_loss=0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00011 | Gamma1: ['2.32406', '2.34925', '2.28337', '2.26574', '2.24468', '2.28301', '2.36652', '2.36713', '2.35487', '2.29423', '2.30374', '2.26947', '2.30243'] | Gamma1 Grad: ['-0.00336', '0.00997', '0.02199', '0.01837', '0.01185', '0.02109', '0.00781', '-0.00713', '0.00361', '-0.00271', '0.00029', '-0.01081', '0.00059']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.938% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.938% | ğŸ† Best Train Accuracy: 91.938%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.938% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.83it/s, Test_acc=64, Test_loss=1.88]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.010% | ğŸ† Best Test Accuracy: 64.120%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.66it/s, Train_acc=91.8, Train_loss=0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.34001', '2.34005', '2.29981', '2.27769', '2.25349', '2.27694', '2.38203', '2.36135', '2.34559', '2.30334', '2.30719', '2.24792', '2.31778'] | Gamma1 Grad: ['-0.01410', '0.00862', '-0.01811', '-0.04982', '-0.00525', '-0.03479', '0.02484', '0.00271', '0.01376', '0.00383', '-0.00412', '0.01112', '-0.00944']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 91.834% | ğŸ† Best Train Accuracy: 91.938%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.938% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.10it/s, Test_acc=63.9, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.870% | ğŸ† Best Test Accuracy: 64.120%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.99it/s, Train_acc=92.2, Train_loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.34021', '2.35490', '2.29262', '2.26062', '2.24908', '2.30574', '2.35279', '2.37506', '2.33590', '2.29941', '2.30474', '2.25658', '2.30366'] | Gamma1 Grad: ['0.02084', '0.01900', '0.09571', '0.05800', '-0.18367', '0.12135', '-0.00632', '0.07135', '-0.00213', '0.01309', '0.00720', '0.02695', '-0.04646']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.212% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.212% | ğŸ† Best Train Accuracy: 92.212%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.212% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.13it/s, Test_acc=64.1, Test_loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.100% | ğŸ† Best Test Accuracy: 64.120%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 21.11it/s, Train_acc=92.2, Train_loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['2.34014', '2.35859', '2.27240', '2.25421', '2.24613', '2.29693', '2.34855', '2.38552', '2.34613', '2.30652', '2.29792', '2.26885', '2.30656'] | Gamma1 Grad: ['0.00582', '0.00973', '-0.02377', '0.06208', '0.17851', '0.00455', '0.06635', '-0.02286', '0.00329', '-0.00849', '-0.01343', '0.00571', '0.00144']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 92.176% | ğŸ† Best Train Accuracy: 92.212%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.212% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.06it/s, Test_acc=63.9, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.920% | ğŸ† Best Test Accuracy: 64.120%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.52it/s, Train_acc=92.5, Train_loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['2.32122', '2.34012', '2.29124', '2.26336', '2.23540', '2.29037', '2.33842', '2.36707', '2.34677', '2.30690', '2.28999', '2.26327', '2.30635'] | Gamma1 Grad: ['0.01594', '-0.01083', '0.00425', '0.00526', '0.04814', '-0.00091', '-0.02510', '0.00887', '0.00018', '0.01467', '0.02602', '0.03532', '0.02775']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.480% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.480% | ğŸ† Best Train Accuracy: 92.480%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.480% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.08it/s, Test_acc=64.6, Test_loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.620% | ğŸ† Best Test Accuracy: 64.620%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.37it/s, Train_acc=92.6, Train_loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.30545', '2.33442', '2.27823', '2.26254', '2.24131', '2.28277', '2.34181', '2.39341', '2.34433', '2.31208', '2.29703', '2.26844', '2.30509'] | Gamma1 Grad: ['0.00697', '-0.00249', '0.01463', '0.00654', '0.04178', '-0.02857', '-0.00328', '-0.00475', '-0.00472', '-0.00816', '-0.01298', '0.04575', '-0.03413']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.646% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.646% | ğŸ† Best Train Accuracy: 92.646%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.646% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 70 | TrainLossHist: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.33it/s, Test_acc=65, Test_loss=1.81]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.980% | ğŸ† Best Test Accuracy: 64.980%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 21.08it/s, Train_acc=92.8, Train_loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.32794', '2.30354', '2.29691', '2.25643', '2.20995', '2.26944', '2.35060', '2.38362', '2.35230', '2.30891', '2.30532', '2.25975', '2.31863'] | Gamma1 Grad: ['0.01866', '-0.11580', '-0.14390', '0.08699', '-0.13632', '-0.02838', '0.01616', '-0.03857', '-0.01350', '-0.00301', '0.01224', '-0.01363', '0.03816']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.792% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.792% | ğŸ† Best Train Accuracy: 92.792%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.792% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.30it/s, Test_acc=64.1, Test_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.090% | ğŸ† Best Test Accuracy: 64.980%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.56it/s, Train_acc=93.2, Train_loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.32931', '2.38123', '2.26136', '2.22991', '2.22147', '2.25832', '2.35155', '2.35293', '2.34575', '2.28711', '2.30890', '2.26607', '2.30704'] | Gamma1 Grad: ['-0.00707', '-0.07154', '0.04354', '0.03576', '0.05187', '0.02799', '0.04378', '-0.00363', '0.00218', '0.01161', '0.00617', '0.02543', '0.01579']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.170% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.170% | ğŸ† Best Train Accuracy: 93.170%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.170% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.37it/s, Test_acc=64.9, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.870% | ğŸ† Best Test Accuracy: 64.980%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.99it/s, Train_acc=93.3, Train_loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.33695', '2.34648', '2.27316', '2.27051', '2.20785', '2.26385', '2.34630', '2.36083', '2.37396', '2.29926', '2.29754', '2.29328', '2.26901'] | Gamma1 Grad: ['-0.00588', '0.01610', '0.00832', '0.03074', '-0.00874', '0.00183', '0.00013', '0.01394', '-0.00364', '-0.00231', '-0.00279', '0.00154', '0.00199']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.292% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.292% | ğŸ† Best Train Accuracy: 93.292%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.292% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.43it/s, Test_acc=64, Test_loss=1.93]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.970% | ğŸ† Best Test Accuracy: 64.980%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 21.02it/s, Train_acc=93.2, Train_loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.36150', '2.33043', '2.28126', '2.27136', '2.24790', '2.26190', '2.35047', '2.36064', '2.33744', '2.30882', '2.31245', '2.28230', '2.27283'] | Gamma1 Grad: ['-0.00930', '-0.01307', '0.01092', '0.04551', '0.04786', '-0.00822', '0.05925', '-0.00281', '-0.00608', '0.00394', '0.00369', '-0.00197', '0.01121']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.168% | ğŸ† Best Train Accuracy: 93.292%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.292% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.75it/s, Test_acc=64.1, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.100% | ğŸ† Best Test Accuracy: 64.980%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.65it/s, Train_acc=93.5, Train_loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.35919', '2.35784', '2.28357', '2.25574', '2.19108', '2.29100', '2.34762', '2.35804', '2.38096', '2.31450', '2.31965', '2.26070', '2.26488'] | Gamma1 Grad: ['-0.04477', '0.04517', '-0.11378', '0.13672', '0.00490', '-0.00261', '0.01595', '-0.05836', '-0.02389', '0.01011', '-0.01392', '-0.02383', '-0.12351']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.506% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.506% | ğŸ† Best Train Accuracy: 93.506%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.506% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.40it/s, Test_acc=65.1, Test_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 65.110% | ğŸ† Best Test Accuracy: 65.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.47it/s, Train_acc=93.5, Train_loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.31195', '2.36316', '2.28683', '2.26136', '2.23400', '2.27802', '2.35116', '2.35336', '2.37702', '2.30763', '2.32216', '2.25148', '2.26319'] | Gamma1 Grad: ['0.00592', '-0.01062', '0.03178', '0.05704', '0.07825', '-0.11803', '-0.02021', '-0.00137', '0.00209', '-0.00373', '-0.00638', '-0.00602', '0.00989']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.490% | ğŸ† Best Train Accuracy: 93.506%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.506% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.52it/s, Test_acc=64.8, Test_loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.780% | ğŸ† Best Test Accuracy: 65.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 21.09it/s, Train_acc=93.8, Train_loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00147 | Gamma1: ['2.33800', '2.38526', '2.31093', '2.27163', '2.22587', '2.27514', '2.33965', '2.36147', '2.35721', '2.31141', '2.31181', '2.24707', '2.26391'] | Gamma1 Grad: ['0.01074', '0.03917', '-0.00128', '-0.03091', '0.06385', '-0.05178', '0.03468', '0.02524', '0.01559', '-0.00692', '-0.01179', '0.01988', '-0.00696']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.824% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.824% | ğŸ† Best Train Accuracy: 93.824%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.824% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.47it/s, Test_acc=64.5, Test_loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.550% | ğŸ† Best Test Accuracy: 65.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.20it/s, Train_acc=93.7, Train_loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.29356', '2.35234', '2.30369', '2.24452', '2.24115', '2.23342', '2.31478', '2.34877', '2.35653', '2.30933', '2.30481', '2.26604', '2.27883'] | Gamma1 Grad: ['0.00943', '-0.00778', '0.03876', '-0.03222', '0.04461', '-0.06689', '0.05040', '-0.00337', '-0.00825', '0.00262', '-0.00205', '-0.01287', '0.03764']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.656% | ğŸ† Best Train Accuracy: 93.824%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.824% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 78.71it/s, Test_acc=64.5, Test_loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.530% | ğŸ† Best Test Accuracy: 65.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.71it/s, Train_acc=94.2, Train_loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00146 | Gamma1: ['2.30948', '2.31985', '2.26283', '2.27739', '2.23110', '2.28144', '2.31162', '2.34879', '2.37072', '2.30710', '2.30363', '2.29017', '2.27658'] | Gamma1 Grad: ['0.00396', '0.02752', '0.07368', '0.00102', '0.10248', '0.08216', '-0.05421', '-0.00661', '0.04421', '-0.00476', '-0.00813', '-0.02457', '-0.03628']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.152% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.152% | ğŸ† Best Train Accuracy: 94.152%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.152% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.68it/s, Test_acc=64.7, Test_loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.670% | ğŸ† Best Test Accuracy: 65.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.91it/s, Train_acc=96.8, Train_loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00144 | Gamma1: ['2.30907', '2.32794', '2.29944', '2.26653', '2.25184', '2.26398', '2.30702', '2.34646', '2.35113', '2.31080', '2.31153', '2.27406', '2.29852'] | Gamma1 Grad: ['-0.00735', '0.00618', '0.01833', '-0.01301', '0.00649', '-0.00015', '-0.01333', '0.01884', '-0.01043', '0.00224', '0.00494', '-0.00711', '0.00923']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 96.844% (Updated)\n",
      "ğŸ“Š Train Accuracy: 96.844% | ğŸ† Best Train Accuracy: 96.844%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 96.844% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 80 | TrainLossHist: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.23it/s, Test_acc=66.6, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 66.600% | ğŸ† Best Test Accuracy: 66.600%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.39it/s, Train_acc=97.9, Train_loss=0.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00143 | Gamma1: ['2.28615', '2.29087', '2.29080', '2.24641', '2.24223', '2.28953', '2.28651', '2.33123', '2.33477', '2.32000', '2.32573', '2.28883', '2.31323'] | Gamma1 Grad: ['-0.03688', '-0.02542', '-0.00783', '-0.01303', '-0.06273', '0.00709', '0.04225', '0.01675', '-0.02362', '-0.00324', '-0.00214', '-0.02113', '0.00355']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.856% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.856% | ğŸ† Best Train Accuracy: 97.856%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.856% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.16it/s, Test_acc=66.8, Test_loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 66.840% | ğŸ† Best Test Accuracy: 66.840%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.68it/s, Train_acc=98.1, Train_loss=0.093]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00142 | Gamma1: ['2.27952', '2.30453', '2.30568', '2.24577', '2.23415', '2.28307', '2.30737', '2.33046', '2.32197', '2.31406', '2.30904', '2.30232', '2.31504'] | Gamma1 Grad: ['0.03049', '0.03948', '0.05366', '-0.03950', '0.07887', '-0.00368', '0.03050', '-0.00061', '0.00981', '-0.00396', '-0.00522', '-0.01775', '-0.01393']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.078% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.078% | ğŸ† Best Train Accuracy: 98.078%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.078% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.83it/s, Test_acc=66.8, Test_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.830% | ğŸ† Best Test Accuracy: 66.840%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.33it/s, Train_acc=98.4, Train_loss=0.083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00141 | Gamma1: ['2.29121', '2.30306', '2.28636', '2.24929', '2.24753', '2.26661', '2.29692', '2.30939', '2.31687', '2.31534', '2.32360', '2.28888', '2.33417'] | Gamma1 Grad: ['0.00147', '0.00216', '0.00293', '-0.00118', '0.00137', '-0.00115', '-0.00252', '0.00053', '0.00024', '0.00079', '0.00100', '-0.00069', '0.00003']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.380% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.380% | ğŸ† Best Train Accuracy: 98.380%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.380% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.71it/s, Test_acc=67.2, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.210% | ğŸ† Best Test Accuracy: 67.210%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 20.75it/s, Train_acc=98.5, Train_loss=0.079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00139 | Gamma1: ['2.31400', '2.29874', '2.31245', '2.26021', '2.22405', '2.28416', '2.30101', '2.32445', '2.31049', '2.31087', '2.31186', '2.28795', '2.32672'] | Gamma1 Grad: ['0.04266', '-0.01011', '-0.00415', '0.02957', '-0.01702', '-0.10470', '0.04399', '-0.00780', '0.01316', '-0.01101', '-0.02477', '0.01251', '0.01825']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.498% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.498% | ğŸ† Best Train Accuracy: 98.498%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.498% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 75.40it/s, Test_acc=67.2, Test_loss=1.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.190% | ğŸ† Best Test Accuracy: 67.210%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.74it/s, Train_acc=98.7, Train_loss=0.074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00138 | Gamma1: ['2.30892', '2.31296', '2.29673', '2.26282', '2.25064', '2.26014', '2.30177', '2.30865', '2.30843', '2.31018', '2.29963', '2.29614', '2.33356'] | Gamma1 Grad: ['-0.01384', '0.00244', '0.02956', '-0.07967', '-0.00006', '-0.00980', '-0.03538', '0.01469', '-0.00537', '0.00057', '-0.00045', '0.01142', '0.02358']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.662% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.662% | ğŸ† Best Train Accuracy: 98.662%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.662% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.69it/s, Test_acc=67.2, Test_loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.180% | ğŸ† Best Test Accuracy: 67.210%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.91it/s, Train_acc=98.7, Train_loss=0.073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00136 | Gamma1: ['2.31068', '2.29794', '2.31941', '2.25750', '2.27813', '2.26842', '2.28209', '2.31272', '2.29938', '2.31257', '2.30144', '2.29098', '2.34638'] | Gamma1 Grad: ['0.00344', '-0.03935', '-0.05970', '0.06970', '-0.08581', '0.04786', '-0.00437', '0.09794', '-0.01193', '0.00355', '0.01066', '0.00257', '0.02442']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.724% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.724% | ğŸ† Best Train Accuracy: 98.724%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.724% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 78.06it/s, Test_acc=67.1, Test_loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.140% | ğŸ† Best Test Accuracy: 67.210%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.34it/s, Train_acc=98.7, Train_loss=0.072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00134 | Gamma1: ['2.30869', '2.28977', '2.29382', '2.27802', '2.24986', '2.29358', '2.30152', '2.30082', '2.30160', '2.31448', '2.31782', '2.29556', '2.32387'] | Gamma1 Grad: ['-0.00458', '-0.04827', '0.01055', '-0.01534', '-0.08850', '0.03851', '-0.04001', '0.01483', '-0.00360', '-0.00110', '-0.00702', '0.01079', '-0.03844']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.690% | ğŸ† Best Train Accuracy: 98.724%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.724% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.55it/s, Test_acc=66.6, Test_loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.560% | ğŸ† Best Test Accuracy: 67.210%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.76it/s, Train_acc=98.8, Train_loss=0.069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00133 | Gamma1: ['2.29131', '2.30652', '2.31387', '2.26355', '2.23352', '2.25271', '2.28716', '2.30284', '2.32271', '2.31137', '2.31020', '2.30994', '2.32210'] | Gamma1 Grad: ['0.02395', '-0.01813', '0.01365', '-0.04143', '0.00378', '0.01164', '-0.01559', '0.00232', '0.00866', '-0.00127', '-0.00744', '-0.00249', '0.00098']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.768% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.768% | ğŸ† Best Train Accuracy: 98.768%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.768% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.54it/s, Test_acc=67.2, Test_loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.150% | ğŸ† Best Test Accuracy: 67.210%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.78it/s, Train_acc=99, Train_loss=0.066]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00131 | Gamma1: ['2.28457', '2.29004', '2.28849', '2.25522', '2.25682', '2.24561', '2.28529', '2.29855', '2.33101', '2.31240', '2.30921', '2.31417', '2.31386'] | Gamma1 Grad: ['-0.00297', '0.04325', '-0.01272', '0.01641', '0.10173', '0.00715', '-0.03403', '0.02426', '-0.02766', '-0.00770', '-0.00116', '0.00791', '0.01949']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.958% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.958% | ğŸ† Best Train Accuracy: 98.958%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.958% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.49it/s, Test_acc=67.4, Test_loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.360% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.72it/s, Train_acc=98.9, Train_loss=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00129 | Gamma1: ['2.28901', '2.29182', '2.27956', '2.26287', '2.23172', '2.28411', '2.30319', '2.31091', '2.32086', '2.32431', '2.30150', '2.30274', '2.33514'] | Gamma1 Grad: ['0.04214', '0.03013', '-0.02017', '0.00904', '0.08531', '-0.06045', '0.00398', '-0.01859', '-0.01923', '-0.00719', '0.00812', '-0.00247', '-0.01411']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.852% | ğŸ† Best Train Accuracy: 98.958%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.958% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 90 | TrainLossHist: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.73it/s, Test_acc=67.1, Test_loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.110% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.17it/s, Train_acc=99, Train_loss=0.063]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00127 | Gamma1: ['2.28524', '2.29388', '2.26587', '2.25877', '2.23738', '2.27802', '2.28306', '2.31052', '2.32055', '2.31600', '2.31004', '2.30557', '2.33402'] | Gamma1 Grad: ['0.05959', '0.00448', '0.05936', '-0.00311', '0.00133', '-0.02284', '-0.00853', '-0.05939', '-0.00255', '0.01214', '-0.00180', '0.01604', '-0.00548']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.956% | ğŸ† Best Train Accuracy: 98.958%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.958% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.38it/s, Test_acc=67.2, Test_loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.220% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.56it/s, Train_acc=99, Train_loss=0.063]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00125 | Gamma1: ['2.27949', '2.28006', '2.28624', '2.27924', '2.23812', '2.27885', '2.27639', '2.29756', '2.32526', '2.31897', '2.31561', '2.30119', '2.33513'] | Gamma1 Grad: ['0.00392', '0.00278', '0.00300', '0.00533', '0.00598', '-0.01328', '-0.00260', '0.00457', '-0.00100', '0.00259', '-0.00064', '0.00225', '-0.00415']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.954% | ğŸ† Best Train Accuracy: 98.958%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.958% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.38it/s, Test_acc=66.8, Test_loss=2.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.850% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 21.06it/s, Train_acc=99, Train_loss=0.061]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00122 | Gamma1: ['2.26502', '2.30065', '2.26982', '2.27799', '2.25947', '2.25976', '2.30330', '2.30736', '2.31840', '2.30874', '2.30261', '2.30767', '2.32759'] | Gamma1 Grad: ['-0.01325', '0.04302', '0.02858', '0.01014', '0.07914', '-0.06517', '0.01882', '0.04076', '-0.00082', '-0.00235', '0.01883', '-0.01385', '0.01420']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.044% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.044% | ğŸ† Best Train Accuracy: 99.044%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.044% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.06it/s, Test_acc=67.1, Test_loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.060% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.63it/s, Train_acc=99.1, Train_loss=0.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00120 | Gamma1: ['2.28047', '2.27998', '2.27321', '2.28995', '2.23737', '2.28754', '2.31016', '2.31747', '2.32611', '2.29915', '2.32297', '2.30790', '2.34771'] | Gamma1 Grad: ['-0.02324', '-0.01410', '-0.14582', '0.09072', '-0.04499', '-0.09775', '0.00958', '-0.03899', '0.01382', '0.01076', '-0.02622', '-0.01533', '-0.02060']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.068% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.068% | ğŸ† Best Train Accuracy: 99.068%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.068% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 77.46it/s, Test_acc=67.1, Test_loss=2.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.060% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.40it/s, Train_acc=99.1, Train_loss=0.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00118 | Gamma1: ['2.31605', '2.30026', '2.29586', '2.27584', '2.23367', '2.26898', '2.27823', '2.31184', '2.30754', '2.30142', '2.31525', '2.29886', '2.33866'] | Gamma1 Grad: ['0.04772', '0.06674', '0.02145', '-0.08574', '0.01888', '-0.05386', '0.08026', '0.09831', '0.02723', '-0.00748', '-0.00979', '-0.03429', '0.04812']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.122% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.122% | ğŸ† Best Train Accuracy: 99.122%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.122% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.37it/s, Test_acc=67.2, Test_loss=2.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.240% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.15it/s, Train_acc=99.1, Train_loss=0.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00115 | Gamma1: ['2.29998', '2.29078', '2.27964', '2.25088', '2.25601', '2.27568', '2.29012', '2.30983', '2.31301', '2.30921', '2.30729', '2.29520', '2.33429'] | Gamma1 Grad: ['-0.00965', '0.00813', '0.01864', '0.00528', '-0.05187', '-0.00871', '-0.01837', '-0.03916', '0.01645', '0.00191', '-0.01074', '-0.02120', '-0.00950']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.124% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.124% | ğŸ† Best Train Accuracy: 99.124%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.124% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.01it/s, Test_acc=67, Test_loss=2.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.950% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.43it/s, Train_acc=99.1, Train_loss=0.057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00113 | Gamma1: ['2.30183', '2.30049', '2.27112', '2.27129', '2.24520', '2.25626', '2.29974', '2.30072', '2.29109', '2.29603', '2.31197', '2.29312', '2.34257'] | Gamma1 Grad: ['0.00262', '0.00004', '0.00159', '-0.00258', '0.00247', '-0.00305', '-0.00237', '-0.00452', '0.00122', '0.00034', '-0.00065', '-0.00140', '-0.00129']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.130% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.130% | ğŸ† Best Train Accuracy: 99.130%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.130% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.04it/s, Test_acc=67, Test_loss=2.23]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.990% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.33it/s, Train_acc=99.2, Train_loss=0.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00110 | Gamma1: ['2.28515', '2.30993', '2.26961', '2.27409', '2.23455', '2.26399', '2.29407', '2.27890', '2.30730', '2.29886', '2.30423', '2.30035', '2.33372'] | Gamma1 Grad: ['0.01881', '-0.03924', '-0.00403', '0.09587', '0.09416', '-0.14202', '0.01020', '-0.11497', '0.00830', '-0.01994', '0.00270', '-0.02118', '0.02163']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.200% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.200% | ğŸ† Best Train Accuracy: 99.200%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.200% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.19it/s, Test_acc=66.9, Test_loss=2.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.920% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.45it/s, Train_acc=99.2, Train_loss=0.055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00108 | Gamma1: ['2.28094', '2.29716', '2.26919', '2.26924', '2.26263', '2.23910', '2.30054', '2.29072', '2.30368', '2.30117', '2.30139', '2.29712', '2.31699'] | Gamma1 Grad: ['0.00981', '0.00244', '0.01411', '0.00308', '0.01451', '-0.02723', '0.03999', '-0.00386', '0.00226', '-0.00101', '-0.00268', '0.00221', '0.00245']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.228% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.228% | ğŸ† Best Train Accuracy: 99.228%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.228% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.64it/s, Test_acc=66.6, Test_loss=2.29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.640% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\NonMonotonicDecay-ExponentialDecay\\Results\\CIFAR100_Test_Exponential_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "Best Test Accuracy:  67.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 11. TRAIN MODEL WITH SHEDULAR | XXX ----------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Set Seed for Reproducibility BEFORE training starts\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)  \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "# âœ… Training Loop\n",
    "num_epochs = 100 # Example: Set the total number of epochs\n",
    "for epoch in range(start_epoch, num_epochs):   # Runs training for 100 epochs\n",
    "\n",
    "    train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler, WARMUP_ACTIVATION_EPOCHS) # âœ… Pass required arguments\n",
    "\n",
    "    test(epoch)  # âœ… Test the model\n",
    "    tqdm.write(\"\")  # âœ… Clear leftover progress bar from test()\n",
    "\n",
    "\n",
    "print(\"Best Test Accuracy: \", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb02afb-d7c4-4e78-8cdc-3700ca91f79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'âœ… Annotated comparison plots with BEST accuracy markers saved to ./Results/Plots'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 550x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 13.  NON-MONOTONIC DECAY VS EXPONENTIAL DECAY| XXX --------------âœ…âœ…MAIN âœ…âœ…--####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.patheffects as path_effects\n",
    "import os\n",
    "from matplotlib.lines import Line2D  # âœ… Required for custom legend handles\n",
    "\n",
    "def read_test_log(file_path):\n",
    "    test_loss_history = []\n",
    "    test_acc_history = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if \"Test Loss\" in line and \"Test Acc\" in line:\n",
    "                try:\n",
    "                    loss = float(line.split(\"Test Loss:\")[1].split(\"|\")[0].strip())\n",
    "                    acc = float(line.split(\"Test Acc:\")[1].split(\"%\")[0].strip())\n",
    "                    test_loss_history.append(loss)\n",
    "                    test_acc_history.append(acc)\n",
    "                except:\n",
    "                    continue\n",
    "    return test_loss_history, test_acc_history\n",
    "\n",
    "def plot_train_test_metrics(save_dir=\"./Results/Plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    Linear_test_log_path  = f'./Results_NonMonotonicDecay/CIFAR100_Test_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "    Exponential_test_log_path = f'./Results/CIFAR100_Test_{decay_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "\n",
    "\n",
    "    Linear_test_loss, Linear_test_acc = read_test_log(Linear_test_log_path)\n",
    "    Exponential_test_loss, Exponential_test_acc = read_test_log(Exponential_test_log_path)\n",
    "\n",
    "    num_epochs = min(len(Linear_test_loss), len(Exponential_test_loss))\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    COLOR_SCALE = ['#00295B', '#CF0A66']  # TargetPenalty, noTargetPenalty\n",
    "    rcParams.update({\n",
    "        \"font.size\": 11,\n",
    "        \"axes.titlesize\": 11,\n",
    "        \"axes.labelsize\": 13,\n",
    "        \"xtick.labelsize\": 11,\n",
    "        \"ytick.labelsize\": 11,\n",
    "        \"axes.labelweight\": \"bold\",\n",
    "        \"xtick.color\": \"black\",\n",
    "        \"ytick.color\": \"black\",\n",
    "    })\n",
    "\n",
    "    # Custom settings\n",
    "    custom_yticks_test_loss = [1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
    "    custom_yticks_test_acc = [10, 20, 30, 40, 50, 60, 70]\n",
    "    custom_xticks = [0, 20, 40, 60, 80, 100]\n",
    "    custom_yaxis_test_loss = [1.2, 4.2]\n",
    "    custom_yaxis_test_acc = [35, 72]\n",
    "    custom_xaxis = [0, 105]\n",
    "\n",
    "    # Offsets\n",
    "    y_offset_loss_tp = 0.2\n",
    "    y_offset_loss_ntp = 0.07\n",
    "    x_offset_loss_tp = 3.5\n",
    "    x_offset_loss_ntp = 3.5\n",
    "\n",
    "    y_offset_acc_tp = 1\n",
    "    y_offset_acc_ntp = 3.2\n",
    "    x_offset_acc_tp = 8.5\n",
    "    x_offset_acc_ntp = 6.5\n",
    "\n",
    "    # ğŸ”· Plot Test Loss\n",
    "    fig1, ax1 = plt.subplots(figsize=(5.5, 3.5))\n",
    "    ax1.plot(epochs, Linear_test_loss[:num_epochs], label=\"Non-Monotonic\", color=COLOR_SCALE[0], linewidth=2)\n",
    "    ax1.plot(epochs, Exponential_test_loss[:num_epochs], label=\"Exponential\", color=COLOR_SCALE[1], linestyle='--', linewidth=2)\n",
    "    ax1.set_xlabel(\"Epoch\", fontweight='bold')\n",
    "    ax1.set_ylabel(\"Test Loss\", fontweight='bold')\n",
    "    ax1.set_xticks(custom_xticks)\n",
    "    ax1.set_yticks(custom_yticks_test_loss)\n",
    "    ax1.set_xlim(custom_xaxis)\n",
    "    ax1.set_ylim(custom_yaxis_test_loss)\n",
    "    ax1.tick_params(axis='x', width=1.5)\n",
    "    ax1.tick_params(axis='y', width=1.5)\n",
    "    for label in ax1.get_xticklabels() + ax1.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    leg1 = ax1.legend(fontsize='small', loc=\"upper right\")\n",
    "    for text in leg1.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax1.plot(epochs[-1], Linear_test_loss[-1], marker='o', color=COLOR_SCALE[0], markersize=4)\n",
    "    ax1.text(epochs[-1] - x_offset_loss_tp, Linear_test_loss[-1] - y_offset_loss_tp,\n",
    "             f\"{Linear_test_loss[-1]:.2f}\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "    ax1.plot(epochs[-1], Exponential_test_loss[-1], marker='o', color=COLOR_SCALE[1], markersize=4)\n",
    "    ax1.text(epochs[-1] - x_offset_loss_ntp, Exponential_test_loss[-1] + y_offset_loss_ntp,\n",
    "             f\"{Exponential_test_loss[-1]:.2f}\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"compare_test_loss_non-monotonic_vs_linear.svg\"),\n",
    "                format='svg', transparent=True, bbox_inches='tight')\n",
    "    plt.close(fig1)\n",
    "\n",
    "    # ğŸ”¶ Plot Test Accuracy â€” Marker at Best Accuracy\n",
    "    best_epoch_tp = Linear_test_acc.index(max(Linear_test_acc)) + 1\n",
    "    best_acc_tp = max(Linear_test_acc)\n",
    "    best_epoch_ntp = Exponential_test_acc.index(max(Exponential_test_acc)) + 1\n",
    "    best_acc_ntp = max(Exponential_test_acc)\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(5.5, 3.5))\n",
    "    ax2.plot(epochs, Linear_test_acc[:num_epochs], label=\"Non-Monotonic\", color=COLOR_SCALE[0], linewidth=2)\n",
    "    ax2.plot(epochs, Exponential_test_acc[:num_epochs], label=\"Exponential\", color=COLOR_SCALE[1], linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel(\"Epoch\", fontweight='bold')\n",
    "    ax2.set_ylabel(\"Test Accuracy (%)\", fontweight='bold')\n",
    "    ax2.set_xticks(custom_xticks)\n",
    "    ax2.set_yticks(custom_yticks_test_acc)\n",
    "    ax2.set_xlim(custom_xaxis)\n",
    "    ax2.set_ylim(custom_yaxis_test_acc)\n",
    "    ax2.tick_params(axis='x', width=1.5)\n",
    "    ax2.tick_params(axis='y', width=1.5)\n",
    "    for label in ax2.get_xticklabels() + ax2.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    leg2 = ax2.legend(fontsize='small', loc=\"lower right\")\n",
    "    for text in leg2.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Markers for best accuracy\n",
    "    ax2.plot(best_epoch_tp, best_acc_tp - 0.21, marker='o', color=COLOR_SCALE[0], markersize=5.5, markeredgecolor='black', markeredgewidth=1)\n",
    "    ax2.text(best_epoch_tp - x_offset_acc_tp, best_acc_tp + y_offset_acc_tp,\n",
    "             f\"{best_acc_tp:.2f}%\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "    ax2.plot(best_epoch_ntp, best_acc_ntp - 0.4, marker='o', color=COLOR_SCALE[1], markersize=5.5, markeredgecolor='black', markeredgewidth=1)\n",
    "    ax2.text(best_epoch_ntp - x_offset_acc_ntp, best_acc_ntp - y_offset_acc_ntp,\n",
    "             f\"{best_acc_ntp:.2f}%\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"compare_test_accuracy_non-monotonic_vs_linear.svg\"),\n",
    "                format='svg', transparent=True, bbox_inches='tight')\n",
    "    # plt.close(fig2)\n",
    "\n",
    "    return f\"âœ… Annotated comparison plots with BEST accuracy markers saved to {save_dir}\"\n",
    "\n",
    "# ğŸ”· Call the function\n",
    "plot_train_test_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
