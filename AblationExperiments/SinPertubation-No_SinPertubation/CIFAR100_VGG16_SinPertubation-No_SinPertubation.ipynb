{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba28f55-a6bc-4bc6-a16c-efb19746b4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Current working directory: C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\n",
      "âœ… sys.path updated:\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\python310.zip\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\DLLs\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\n",
      "   ğŸ“‚ \n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\Pythonwin\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\models\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\activation\n",
      "âœ… FFTGate imported successfully!\n",
      "âœ… FFTGate instance created successfully!\n",
      "âœ… FFTGate_VGG imported successfully!\n",
      "CIFAR100 Training Script Initialized...\n",
      "Using device: cuda\n",
      "Parsed learning rate: 0.001 (type: <class 'float'>)\n",
      "Formatted learning rate for filenames: 0_001\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Length of training dataset: 50000\n",
      "Length of testing dataset: 10000\n",
      "Number of classes in CIFAR-100: 100\n",
      "==> Building model..\n",
      "âœ… Found 13 FFTGate layers.\n",
      "âœ… Collected 13 trainable activation parameters.\n",
      "   ğŸ”¹ Layer 0: FFTGate()\n",
      "   ğŸ”¹ Layer 1: FFTGate()\n",
      "   ğŸ”¹ Layer 2: FFTGate()\n",
      "   ğŸ”¹ Layer 3: FFTGate()\n",
      "   ğŸ”¹ Layer 4: FFTGate()\n",
      "   ğŸ”¹ Layer 5: FFTGate()\n",
      "   ğŸ”¹ Layer 6: FFTGate()\n",
      "   ğŸ”¹ Layer 7: FFTGate()\n",
      "   ğŸ”¹ Layer 8: FFTGate()\n",
      "   ğŸ”¹ Layer 9: FFTGate()\n",
      "   ğŸ”¹ Layer 10: FFTGate()\n",
      "   ğŸ”¹ Layer 11: FFTGate()\n",
      "   ğŸ”¹ Layer 12: FFTGate()\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 1.A. IMPORTS LIBRARIES | XXX -----------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\"\"\"Train CIFAR100 with PyTorch.\"\"\"\n",
    "\n",
    "# Python 2/3 compatibility\n",
    "# from __future__ import print_function\n",
    "\n",
    "\n",
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# PyTorch and related modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# torchvision for datasets and transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch_optimizer as torch_opt  # Use 'torch_opt' for torch_optimizer\n",
    "from timm.scheduler import CosineLRScheduler \n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Define currect working directory to ensure on right directory\n",
    "VGG16_PATH = r\"C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\"\n",
    "if os.getcwd() != VGG16_PATH:\n",
    "    os.chdir(VGG16_PATH)\n",
    "print(f\"âœ… Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# âœ… Define absolute paths\n",
    "PROJECT_PATH = VGG16_PATH\n",
    "MODELS_PATH = os.path.join(VGG16_PATH, \"models\")\n",
    "ACTIVATION_PATH = os.path.join(VGG16_PATH, \"activation\")\n",
    "# PAU_PATH = os.path.join(VGG16_PATH, \"pau\")\n",
    "\n",
    "# âœ… Ensure necessary paths are in sys.path\n",
    "for path in [PROJECT_PATH, MODELS_PATH, ACTIVATION_PATH]:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "# âœ… Print updated sys.path for debugging\n",
    "print(\"âœ… sys.path updated:\")\n",
    "for path in sys.path:\n",
    "    print(\"   ğŸ“‚\", path)\n",
    "\n",
    "# âœ… Import FFTGate (Check if the module exists)\n",
    "try:\n",
    "    from activation.FFTGate import FFTGate  # type: ignore\n",
    "    print(\"âœ… FFTGate imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ Import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'Activation4.py' exists inside: {ACTIVATION_PATH}\")\n",
    "\n",
    "# âœ… Test if FFTGate is callable\n",
    "try:\n",
    "    activation_test = FFTGate()\n",
    "    print(\"âœ… FFTGate instance created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error while initializing FFTGate: {e}\")\n",
    "\n",
    "# âœ… Now import FFTGate_VGG (Ensure module exists inside models/)\n",
    "try:\n",
    "    from models.FFTGate_VGG import FFTGate_VGG  # type: ignore\n",
    "    print(\"âœ… FFTGate_VGG imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ FFTGate_VGG import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'FFTGate_VGG.py' exists inside: {MODELS_PATH}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 1.B. SEEDING FOR REPRODUCIBILITY | XXX -------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def set_seed_torch(seed):\n",
    "    torch.manual_seed(seed)                          \n",
    "\n",
    "\n",
    "\n",
    "def set_seed_main(seed):\n",
    "    random.seed(seed)                                ## Python's random module\n",
    "    np.random.seed(seed)                             ## NumPy's random module\n",
    "    torch.cuda.manual_seed(seed)                     ## PyTorch's random module for CUDA\n",
    "    torch.cuda.manual_seed_all(seed)                 ## Seed for all CUDA devices\n",
    "    torch.backends.cudnn.deterministic = True        ## Ensure deterministic behavior for CuDNN\n",
    "    torch.backends.cudnn.benchmark = False           ## Disable CuDNN's autotuning for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)   \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (Optional) Import Optimizers - Uncomment as needed\n",
    "# from Opt import opt\n",
    "# from diffGrad import diffGrad\n",
    "# from diffRGrad import diffRGrad, SdiffRGrad, BetaDiffRGrad, Beta12DiffRGrad, BetaDFCDiffRGrad\n",
    "# from RADAM import Radam, BetaRadam\n",
    "# from BetaAdam import BetaAdam, BetaAdam1, BetaAdam2, BetaAdam3, BetaAdam4, BetaAdam5, BetaAdam6, BetaAdam7, BetaAdam4A\n",
    "# from AdamRM import AdamRM, AdamRM1, AdamRM2, AdamRM3, AdamRM4, AdamRM5\n",
    "# from sadam import sadam\n",
    "# from SdiffGrad import SdiffGrad\n",
    "# from SRADAM import SRADAM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 2. DEFINE MODEL Lr | XXX ---------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Main Execution (Placeholder)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"CIFAR100 Training Script Initialized...\")\n",
    "    # Add your training pipeline here\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Argument parser to get user inputs\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR100 Training')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "\n",
    "args, unknown = parser.parse_known_args()  # Avoids Jupyter argument issues\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Ensure lr is correctly parsed\n",
    "lr = args.lr  # Get learning rate from argparse\n",
    "lr_str = str(lr).replace('.', '_')  # Convert to string and replace '.' for filenames\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Parsed learning rate: {lr} (type: {type(lr)})\")\n",
    "print(f\"Formatted learning rate for filenames: {lr_str}\")\n",
    "\n",
    "# Initialize training variables\n",
    "best_acc = 0  # Best test accuracy\n",
    "start_epoch = 0  # Start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 3. LOAD DATASET | XXX ------------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "bs = 64 #set batch size\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Length of train and test datasets\n",
    "len_train = len(trainset)\n",
    "len_test = len(testset)\n",
    "print(f\"Length of training dataset: {len_train}\")\n",
    "print(f\"Length of testing dataset: {len_test}\")\n",
    "\n",
    "# âœ… Print number of classes\n",
    "num_classes_Print = len(trainset.classes)\n",
    "print(f\"Number of classes in CIFAR-100: {num_classes_Print}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 4. DYNAMIC REGULARIZATION| XXX ---------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def apply_dynamic_regularization(inputs, feature_activations, epoch,\n",
    "                                  prev_params, layer_index_map, batch_idx):\n",
    "\n",
    "\n",
    "    global activation_layers  # âœ… Reference already-collected layers\n",
    "\n",
    "    # âœ… Print gamma1 stats early in training for monitoring\n",
    "    if batch_idx == 0 and epoch <= 4:\n",
    "        print(f\"\\nğŸš¨ ENTERED apply_dynamic_regularization | Epoch={epoch} | Batch={batch_idx}\", flush=True)\n",
    "\n",
    "        # ğŸ§  Print gamma1 details\n",
    "        all_layer_info = []\n",
    "        for idx, layer in enumerate(activation_layers):\n",
    "            param = getattr(layer, \"gamma1\")\n",
    "            all_layer_info.append(f\"Layer {idx}: ID={id(param)} | Mean={param.mean().item():.5f}\")\n",
    "        print(\"ğŸ§  GAMMA1 INFO:\", \" | \".join(all_layer_info), flush=True)\n",
    "\n",
    "    # âœ… Initialize gamma1 regularization accumulator\n",
    "    gamma1_reg = 0.0\n",
    "\n",
    "    # âœ… Compute batch std and define regularization strength\n",
    "    batch_std = torch.std(inputs) + 1e-6\n",
    "    regularization_strength = 0.05 if epoch < 40 else (0.01 if epoch < 60 else 0.005)\n",
    "\n",
    "    # âœ… Track layers where noise is injected (informative)\n",
    "    noisy_layers = []\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        if idx not in layer_index_map:\n",
    "            continue\n",
    "\n",
    "        prev_layer_params = prev_params[layer_index_map[idx]]\n",
    "        param_name = \"gamma1\"\n",
    "        param = getattr(layer, param_name)\n",
    "        prev_param = prev_layer_params[param_name]\n",
    "\n",
    "        # âœ… Target based on input stats\n",
    "        target = compute_target(param_name, batch_std)\n",
    "\n",
    "        # âœ… Adaptive Target Regularization\n",
    "        gamma1_reg += regularization_strength * (param - target).pow(2).mean() * 1.2\n",
    "\n",
    "        # âœ… Adaptive Cohesion Regularization\n",
    "        cohesion = (param - prev_param).pow(2)\n",
    "        gamma1_reg += 0.005 * cohesion.mean()\n",
    "\n",
    "        # âœ… Adaptive Noise Regularization\n",
    "        epoch_AddNoise = 50\n",
    "        if epoch > epoch_AddNoise:\n",
    "            param_variation = torch.abs(param - prev_param).mean()\n",
    "            if param_variation < 0.015:\n",
    "                noise = (0.001 + 0.0004 * batch_std.item()) * torch.randn_like(param)\n",
    "                penalty = (param - (prev_param + noise)).pow(2).sum()\n",
    "                gamma1_reg += 0.00015 * penalty\n",
    "                noisy_layers.append(f\"{idx} (Î”={param_variation.item():.5f})\") # Collect index and variation\n",
    "\n",
    "    # âœ… Print noise injection summary\n",
    "    if batch_idx == 0 and epoch <= (epoch_AddNoise + 4) and noisy_layers:\n",
    "        print(f\"ğŸ”¥ Stable Noise Injected | Epoch {epoch} | Batch {batch_idx} | Layers: \" + \", \".join(noisy_layers), flush=True)\n",
    "    mags = feature_activations.abs().mean(dim=(0, 2, 3))\n",
    "    m = mags / mags.sum()\n",
    "    gamma1_reg += 0.005 * (-(m * torch.log(m + 1e-6)).sum())\n",
    "\n",
    "    return gamma1_reg\n",
    "\n",
    "\n",
    "def compute_target(param_name, batch_std):\n",
    "    if param_name == \"gamma1\":\n",
    "        return 2.0 + 0.2 * batch_std.item()  \n",
    "\n",
    "    raise ValueError(f\"Unknown param {param_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 5. INITIALIZE MODEL | XXX --------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "#net = Elliott_VGG('VGG16'); net1 = 'Elliott_VGG16'\n",
    "#net = GELU_MobileNet(); net1 = 'GELU_MobileNet'\n",
    "#net = GELU_SENet18(); net1 = 'GELU_SENet18'\n",
    "#net = PDELU_ResNet50(); net1 = 'PDELU_ResNet50'\n",
    "# net = Sigmoid_GoogLeNet(); net1 = 'Sigmoid_GoogLeNet'\n",
    "#net = GELU_DenseNet121(); net1 = 'GELU_DenseNet121'\n",
    "# net = ReLU_VGG('VGG16'); net1 = 'ReLU_VGG16'\n",
    "net = FFTGate_VGG('VGG16'); net1 = 'FFTGate_VGG16'\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9); optimizer1 = 'SGDM5'\n",
    "#optimizer = optim.Adagrad(net.parameters()); optimizer1 = 'AdaGrad'\n",
    "#optimizer = optim.Adadelta(net.parameters()); optimizer1 = 'AdaDelta'\n",
    "#optimizer = optim.RMSprop(net.parameters()); optimizer1 = 'RMSprop'\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr); optimizer1 = 'Adam'\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr, amsgrad=True); optimizer1 = 'amsgrad'\n",
    "#optimizer = diffGrad(net.parameters(), lr=args.lr); optimizer1 = 'diffGrad'\n",
    "#optimizer = Radam(net.parameters(), lr=args.lr); optimizer1 = 'Radam'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 6. INITIALIZE ACTIVATION PARAMETERS, OPTIMIZERS & SCHEDULERS | XXX ---------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# âœ… Step 1: Collect Activation Parameters from ALL Layers (Ensure Compatibility with DataParallel)\n",
    "if isinstance(net, torch.nn.DataParallel):\n",
    "    features = net.module.features\n",
    "else:\n",
    "    features = net.features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 2: Recursively search for FFTGate layers\n",
    "activation_params = []\n",
    "activation_layers = []\n",
    "\n",
    "for layer in features:\n",
    "    if isinstance(layer, FFTGate):  \n",
    "        activation_layers.append(layer)\n",
    "        activation_params.append(layer.gamma1)  # âœ… Only gamma1 is trainable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 3: Define Unfreeze Epoch\n",
    "unfreeze_activation_epoch = 1  # âœ… Change this value if needed\n",
    "# unfreeze_activation_epoch = 10  # âœ… Delay unfreezing until epoch 10\n",
    "\n",
    "\n",
    "# âœ… Define the warm-up epoch value\n",
    "# WARMUP_ACTIVATION_EPOCHS = 5  # The number of epochs for warm-up\n",
    "WARMUP_ACTIVATION_EPOCHS = 0  # The number of epochs for warm-up\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initially Freeze Activation Parameters\n",
    "for param in activation_params:\n",
    "    param.requires_grad = False  # ğŸš« Keep frozen before the unfreeze epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initialize Activation Optimizers (Using AdamW for Better Weight Decay)\n",
    "activation_optimizers = {\n",
    "    \"gamma1\": torch.optim.AdamW(activation_params, lr=0.0015, weight_decay=1e-6)  # ğŸ”º Reduce LR from 0.005 â†’ 0.0025\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 5: Initialize Activation Schedulers with Warm Restarts (Per Parameter Type)\n",
    "activation_schedulers = {\n",
    "    \"gamma1\": CosineAnnealingWarmRestarts(\n",
    "        activation_optimizers[\"gamma1\"],\n",
    "        T_0=10,      # Shorter cycle to explore aggressively\n",
    "        T_mult=2,    # Increase cycle length gradually\n",
    "        eta_min=5e-5  # âœ… recommended safer modification\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 6: Print collected activation layers and parameters\n",
    "if activation_layers and activation_params:\n",
    "    print(f\"âœ… Found {len(activation_layers)} FFTGate layers.\")\n",
    "    print(f\"âœ… Collected {len(activation_params)} trainable activation parameters.\")\n",
    "    \n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        print(f\"   ğŸ”¹ Layer {idx}: {layer}\")\n",
    "\n",
    "elif activation_layers and not activation_params:\n",
    "    print(f\"âš  Warning: Found {len(activation_layers)} FFTGate layers, but no trainable parameters were collected.\")\n",
    "\n",
    "elif activation_params and not activation_layers:\n",
    "    print(f\"âš  Warning: Collected {len(activation_params)} activation parameters, but no FFTGate layers were recorded.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš  Warning: No FFTGate layers or activation parameters found! Skipping activation optimizer.\")\n",
    "    activation_optimizers = None\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 7. INITIALIZE MAIN OPTIMIZER SCHEDULER | XXX -------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Step 6: Define MultiStepLR for Main Optimizer\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "main_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 8. MODEL CHECK POINT | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists('checkpoint'):\n",
    "    os.makedirs('checkpoint')\n",
    "\n",
    "if not os.path.exists('Results'):\n",
    "    os.makedirs('Results')\n",
    "\n",
    "# Construct checkpoint path\n",
    "checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr}_{net1}_{optimizer1}.t7'\n",
    "\n",
    "# Resume checkpoint only if file exists\n",
    "if args.resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 9. DEFINE TRAIN LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Used for naming files \n",
    "sinPertubation_mode = \"no_sinPertubation\"  # Options: \"no_sinPertubation\", \"no_sinPertubation\"\n",
    "\n",
    "# Training\n",
    "\n",
    "def train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler , WARMUP_ACTIVATION_EPOCHS):\n",
    "    global train_loss_history, best_train_acc, prev_params, recent_test_acc, gamma1_history, activation_layers, test_acc_history, train_acc_history, sinPertubation_mode  # ğŸŸ¢ğŸŸ¢ğŸŸ¢\n",
    "\n",
    "    if epoch == 0:\n",
    "        train_loss_history = []\n",
    "        train_acc_history = []\n",
    "        best_train_acc = 0.0\n",
    "        recent_test_acc = 0.0\n",
    "        gamma1_history = {}         # âœ… Initialize history\n",
    "        test_acc_history = []       # âœ… test accuracy history\n",
    "\n",
    "\n",
    "\n",
    "    prev_params = {}\n",
    "    layer_index_map = {idx: idx for idx in range(len(activation_layers))}  \n",
    "\n",
    "    # âœ… Cache previous gamma1 values from activation layers\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_accuracy = 0.0\n",
    "\n",
    "    # âœ… Initialize log history\n",
    "    log_history = []\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Define path to store Training log\n",
    "    save_paths = {\n",
    "       \n",
    "        \"log_history\": f\"C:\\\\Users\\\\emeka\\\\Research\\\\ModelCUDA\\\\Big_Data_Journal\\\\Comparison\\\\Code\\\\Paper\\\\github2\\\\AblationExperiments\\\\SinPertubation-No_SinPertubation\\\\Results\\\\FFTGate\\\\FFTGate_training_logs.txt\"  # âœ… Training log_history \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 1: Unfreeze Activation Parameters (Only Once Per Epoch)\n",
    "    if epoch == unfreeze_activation_epoch:\n",
    "        print(\"\\nğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\")\n",
    "        for layer in net.module.features if isinstance(net, torch.nn.DataParallel) else net.features:\n",
    "            if isinstance(layer, FFTGate):   \n",
    "                layer.gamma1.requires_grad = True  # âœ… Only gamma1 is trainable\n",
    "        print(\"âœ… Activation Parameters Unfrozen! ğŸš€\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 2: Gradual Warm-up for Activation Learning Rates (AFTER Unfreezing)\n",
    "    warmup_start = unfreeze_activation_epoch  # ğŸ”¹ Start warm-up when unfreezing happens\n",
    "    warmup_end = unfreeze_activation_epoch + WARMUP_ACTIVATION_EPOCHS  # ğŸ”¹ End warm-up period\n",
    "\n",
    "    # âœ… Adjust learning rates **only** during the warm-up phase\n",
    "    if warmup_start <= epoch < warmup_end:\n",
    "        warmup_factor = (epoch - warmup_start + 1) / WARMUP_ACTIVATION_EPOCHS  \n",
    "\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            for param_group in act_scheduler.optimizer.param_groups:\n",
    "                if \"initial_lr\" not in param_group:\n",
    "                    param_group[\"initial_lr\"] = param_group[\"lr\"]  # ğŸ”¹ Store initial LR\n",
    "                param_group[\"lr\"] = param_group[\"initial_lr\"] * warmup_factor  # ğŸ”¹ Scale LR\n",
    "\n",
    "        # âœ… Debugging output to track warm-up process\n",
    "        print(f\"ğŸ”¥ Warm-up Epoch {epoch}: Scaling LR by {warmup_factor:.3f}\")\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            print(f\"  ğŸ”¹ {name} LR: {act_scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    activation_history = []  # ğŸ”´ Initialize empty history at start of epoch (outside batch loop)\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Training Loop\n",
    "    with tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch}\") as progress:\n",
    "        for batch_idx, (inputs, targets) in progress:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # zero_grad activation parameter\n",
    "            for opt in activation_optimizers.values():\n",
    "                opt.zero_grad()\n",
    "\n",
    "\n",
    "            # âœ… Forward Pass\n",
    "            outputs = net(inputs, epoch=epoch, train_accuracy=train_accuracy, targets=targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            \n",
    "            feature_activations = features(inputs)  # Feature activations\n",
    "\n",
    "\n",
    "            # âœ… Collect Activation History | âœ… Per-layer mean activations\n",
    "            batch_means = [layer.saved_output.mean().item() for layer in activation_layers]\n",
    "            activation_history.extend(batch_means)\n",
    "\n",
    "            # âœ… Apply Decay strategy to history for each activation layer\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    if isinstance(layer, FFTGate):\n",
    "                        layer.decay_spectral_history(epoch, num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Compute Training Accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            train_accuracy = 100. * correct / total if total > 0 else 0.0  # Compute training accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Call Regularization Function for the Activation Parameter\n",
    "            if epoch > 0:\n",
    "                gamma1_reg = apply_dynamic_regularization(\n",
    "                    inputs, feature_activations, epoch,\n",
    "                    prev_params, layer_index_map, batch_idx\n",
    "                )\n",
    "                loss += gamma1_reg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… ğŸ¯ Adaptive Gradient Clipping of gamma1  \n",
    "            for layer in features:\n",
    "                if isinstance(layer, FFTGate):  # âœ… Ensure layer has gamma1 before clipping\n",
    "                    torch.nn.utils.clip_grad_norm_([layer.gamma1], max_norm=0.7)\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Apply Optimizer Step for Model Parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # âœ… Apply Optimizer Steps for Activation Parameters (Only if Unfrozen)\n",
    "            if epoch >= unfreeze_activation_epoch:\n",
    "                for opt in activation_optimizers.values():\n",
    "                    opt.step()\n",
    "\n",
    "\n",
    "            # âœ… Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Clamping of gamma1 (Applied AFTER Optimizer Step)\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    layer.gamma1.clamp_(0.1, 6.0)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Update progress bar\n",
    "            progress.set_postfix(Train_loss=round(train_loss / (batch_idx + 1), 3),\n",
    "                                 Train_acc=train_accuracy)  \n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step the main optimizer scheduler (ONLY for model parameters)\n",
    "    main_scheduler.step()\n",
    "\n",
    "    # âœ… Step the activation parameter schedulers (ONLY for activation parameters) | Epoch-wise stepping\n",
    "    if epoch >= unfreeze_activation_epoch:\n",
    "        for name, act_scheduler in activation_schedulers.items():  \n",
    "            act_scheduler.step()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… ONLY update prev_params here AFTER all updates | âœ… Update prev_params AFTER training epoch\n",
    "    for idx, layer in enumerate(activation_layers):      \n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Logging Activation Parameters & Gradients\n",
    "    last_batch_grads = {\"Gamma1 Grad\": []}\n",
    "    current_params = {\"Gamma1\": []}\n",
    "\n",
    "    for layer in features:\n",
    "        if isinstance(layer, FFTGate):  \n",
    "            # âœ… Convert gradients to scalar floats and format to 5 decimal places (removes device='cuda:0' and tensor(...))\n",
    "            last_batch_grads[\"Gamma1 Grad\"].append(f\"{layer.gamma1.grad.item():.5f}\" if layer.gamma1.grad is not None else \"None\")\n",
    "\n",
    "            # âœ… Collect current parameter values (already scalar), formatted to 5 decimal places\n",
    "            current_params[\"Gamma1\"].append(f\"{layer.gamma1.item():.5f}\")\n",
    "\n",
    "    # âœ… Build log message (showing params and gradients for ALL layers)\n",
    "    log_msg = (\n",
    "        f\"Epoch {epoch}: M_Optimizer LR => {optimizer.param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1 LR => {activation_optimizers['gamma1'].param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1: {current_params['Gamma1']} | \"\n",
    "        f\"Gamma1 Grad: {last_batch_grads['Gamma1 Grad']}\"\n",
    "    )\n",
    "\n",
    "    log_history.append(log_msg)\n",
    "    print(log_msg)  # âœ… Prints only once per epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (Clear old logs)\n",
    "    if epoch == 0:  # âœ… Only clear at the start of training\n",
    "        with open(save_paths[\"log_history\"], \"w\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Save logs once per epoch (Append new logs)\n",
    "    if log_history:\n",
    "        with open(save_paths[\"log_history\"], \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_history) + \"\\n\")         # âœ… Ensure each entry is on a new line\n",
    "        print(f\"ğŸ“œ Logs saved to {save_paths['log_history']}!\")  # âœ… Only prints once per epoch\n",
    "    else:\n",
    "        print(\"âš  No logs to save!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final training accuracy for the epoch\n",
    "    final_train_loss = train_loss / len(trainloader)\n",
    "    final_train_acc = 100. * correct / total\n",
    "\n",
    "    # âœ… Append to history\n",
    "    train_loss_history.append(final_train_loss)\n",
    "\n",
    "    # Append per-epoch training accuracy\n",
    "    train_acc_history.append(final_train_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save training results (without affecting best accuracy tracking)\n",
    "    train_results_path = f'./Results/CIFAR100_Train_{sinPertubation_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "    # âœ… Clear the log file at the start of training (Epoch 0)\n",
    "    if epoch == 0 and os.path.exists(train_results_path):\n",
    "        with open(train_results_path, 'w') as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs only once\n",
    "\n",
    "    # âœ… Append new training results for each epoch\n",
    "    with open(train_results_path, 'a') as f:\n",
    "        f.write(f\"Epoch {epoch} | Train Loss: {final_train_loss:.3f} | Train Acc: {final_train_acc:.3f}%\\n\")\n",
    "\n",
    "    if final_train_acc > best_train_acc:\n",
    "        best_train_acc = final_train_acc  # âœ… Update best training accuracy\n",
    "        print(f\"ğŸ† New Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "    # âœ… Append the best training accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):  # Only log once at the final epoch\n",
    "        with open(train_results_path, 'a') as f:\n",
    "            f.write(f\"\\nğŸ† Best Training Accuracy: {best_train_acc:.3f}%\\n\")  \n",
    "\n",
    "    # âœ… Print both Final and Best Training Accuracy\n",
    "    print(f\"ğŸ“Š Train Accuracy: {final_train_acc:.3f}% | ğŸ† Best Train Accuracy: {best_train_acc:.3f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"ğŸ“œ Training logs saved to {train_results_path}!\")\n",
    "    print(f\"ğŸ† Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"ğŸ“ Sizes â†’ ActivationHist: {len(activation_history)} | TestAccHist: {len(test_acc_history)} | TrainLossHist: {len(train_loss_history)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # return final_train_loss, final_train_acc, feature_activations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 10. DEFINE TEST LOOP | XXX --------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(epoch, save_results=True):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and optionally saves the results.\n",
    "    \n",
    "    Args:\n",
    "    - epoch (int): The current epoch number.\n",
    "    - save_results (bool): Whether to save results to a file.\n",
    "\n",
    "    Returns:\n",
    "    - acc (float): Test accuracy percentage.\n",
    "    \"\"\"\n",
    "    global best_acc, val_accuracy, sinPertubation_mode  \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # âœ… Ensure activation function parameters are clamped before evaluation\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(testloader), total=len(testloader), desc=f\"Testing Epoch {epoch}\") as progress:\n",
    "            for batch_idx, (inputs, targets) in progress:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                # âœ… Pass validation accuracy to activation function\n",
    "                val_accuracy = 100. * correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "                # âœ… Update progress bar with loss & accuracy\n",
    "                progress.set_postfix(Test_loss=round(test_loss / (batch_idx + 1), 3),\n",
    "                                     Test_acc=round(val_accuracy, 3))\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final test accuracy\n",
    "    final_test_loss = test_loss / len(testloader)\n",
    "    final_test_acc = 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Ensure \"Results\" folder exists (just like training logs)\n",
    "    results_dir = os.path.join(PROJECT_PATH, \"Results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # âœ… Define log file path for test results\n",
    "    test_results_path = os.path.join(results_dir, f'CIFAR100_Test_{sinPertubation_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt')\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (clear old logs)\n",
    "    if epoch == 0:\n",
    "        with open(test_results_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Append new test results for each epoch (same style as training)\n",
    "    with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Epoch {epoch} | Test Loss: {final_test_loss:.3f} | Test Acc: {final_test_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save checkpoint if accuracy improves (does NOT interfere with logging)\n",
    "    if final_test_acc > best_acc:\n",
    "        print('ğŸ† Saving best model...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': final_test_acc,  # âœ… Ensures the best test accuracy is saved in checkpoint\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Ensure checkpoint directory exists\n",
    "        checkpoint_dir = \"checkpoint\"\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "        # âœ… Format learning rate properly before saving filename\n",
    "        lr_str = str(lr).replace('.', '_')\n",
    "        checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr_str}_{net1}_{optimizer1}.t7'\n",
    "        torch.save(state, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "        best_acc = final_test_acc  # âœ… Update best accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Append the best test accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):\n",
    "        with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\nğŸ† Best Test Accuracy: {best_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Print both Final and Best Test Accuracy (always executed)\n",
    "    print(f\"ğŸ“Š Test Accuracy: {final_test_acc:.3f}% | ğŸ† Best Test Accuracy: {best_acc:.3f}%\")\n",
    "    print(f\"ğŸ“œ Test logs saved to {test_results_path}!\")\n",
    "\n",
    "\n",
    "    global recent_test_acc\n",
    "    recent_test_acc = final_test_acc  # Capture latest test accuracy for next train() call | Store latest test accuracy\n",
    "\n",
    "    test_acc_history.append(final_test_acc)\n",
    "\n",
    "    return final_test_acc  # âœ… Return the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8db9b9-0414-4637-b985-0af7b8dc8277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:23<00:00, 32.60it/s, Train_acc=3.47, Train_loss=4.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000'] | Gamma1 Grad: ['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 3.468% (Updated)\n",
      "ğŸ“Š Train Accuracy: 3.468% | ğŸ† Best Train Accuracy: 3.468%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 3.468% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.35it/s, Test_acc=5.27, Test_loss=4.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 5.270% | ğŸ† Best Test Accuracy: 5.270%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "\n",
      "ğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\n",
      "âœ… Activation Parameters Unfrozen! ğŸš€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=1 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2127360493264 | Mean=1.50000 | Layer 1: ID=2127360542816 | Mean=1.50000 | Layer 2: ID=2127328825152 | Mean=1.50000 | Layer 3: ID=2127328824192 | Mean=1.50000 | Layer 4: ID=2127328823312 | Mean=1.50000 | Layer 5: ID=2127328822192 | Mean=1.50000 | Layer 6: ID=2127328827472 | Mean=1.50000 | Layer 7: ID=2128028320448 | Mean=1.50000 | Layer 8: ID=2128028321408 | Mean=1.50000 | Layer 9: ID=2128028322368 | Mean=1.50000 | Layer 10: ID=2128028323328 | Mean=1.50000 | Layer 11: ID=2128028324288 | Mean=1.50000 | Layer 12: ID=2128028325248 | Mean=1.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.99it/s, Train_acc=6.92, Train_loss=4.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.23146', '2.23244', '2.23102', '2.22755', '2.22775', '2.22656', '2.22698', '2.22599', '2.22641', '2.22818', '2.22363', '2.22026', '2.21007'] | Gamma1 Grad: ['0.00163', '-0.01197', '-0.00168', '-0.00619', '-0.00587', '-0.00996', '-0.00767', '-0.00325', '-0.01600', '-0.00704', '-0.00754', '-0.02460', '-0.01083']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 6.918% (Updated)\n",
      "ğŸ“Š Train Accuracy: 6.918% | ğŸ† Best Train Accuracy: 6.918%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 6.918% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.23it/s, Test_acc=7.97, Test_loss=3.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 7.970% | ğŸ† Best Test Accuracy: 7.970%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=2 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2127360493264 | Mean=2.23146 | Layer 1: ID=2127360542816 | Mean=2.23244 | Layer 2: ID=2127328825152 | Mean=2.23102 | Layer 3: ID=2127328824192 | Mean=2.22755 | Layer 4: ID=2127328823312 | Mean=2.22775 | Layer 5: ID=2127328822192 | Mean=2.22656 | Layer 6: ID=2127328827472 | Mean=2.22698 | Layer 7: ID=2128028320448 | Mean=2.22599 | Layer 8: ID=2128028321408 | Mean=2.22641 | Layer 9: ID=2128028322368 | Mean=2.22818 | Layer 10: ID=2128028323328 | Mean=2.22363 | Layer 11: ID=2128028324288 | Mean=2.22026 | Layer 12: ID=2128028325248 | Mean=2.21007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.62it/s, Train_acc=11.2, Train_loss=3.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.29563', '2.29630', '2.29553', '2.28382', '2.28377', '2.28173', '2.28043', '2.28325', '2.27989', '2.28056', '2.27821', '2.27558', '2.25993'] | Gamma1 Grad: ['-0.03168', '-0.00290', '0.00642', '-0.00696', '-0.00122', '-0.00377', '0.00352', '-0.00089', '0.00276', '-0.00017', '0.00099', '-0.00320', '-0.00047']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 11.236% (Updated)\n",
      "ğŸ“Š Train Accuracy: 11.236% | ğŸ† Best Train Accuracy: 11.236%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 11.236% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.24it/s, Test_acc=12.9, Test_loss=3.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 12.940% | ğŸ† Best Test Accuracy: 12.940%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=3 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2127360493264 | Mean=2.29563 | Layer 1: ID=2127360542816 | Mean=2.29630 | Layer 2: ID=2127328825152 | Mean=2.29553 | Layer 3: ID=2127328824192 | Mean=2.28382 | Layer 4: ID=2127328823312 | Mean=2.28377 | Layer 5: ID=2127328822192 | Mean=2.28173 | Layer 6: ID=2127328827472 | Mean=2.28043 | Layer 7: ID=2128028320448 | Mean=2.28325 | Layer 8: ID=2128028321408 | Mean=2.27989 | Layer 9: ID=2128028322368 | Mean=2.28056 | Layer 10: ID=2128028323328 | Mean=2.27821 | Layer 11: ID=2128028324288 | Mean=2.27558 | Layer 12: ID=2128028325248 | Mean=2.25993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.65it/s, Train_acc=15.3, Train_loss=3.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.30678', '2.30954', '2.30508', '2.29452', '2.29356', '2.29246', '2.29170', '2.29017', '2.29063', '2.28886', '2.28829', '2.28348', '2.28376'] | Gamma1 Grad: ['-0.02631', '0.00666', '0.00530', '-0.00202', '0.00902', '-0.00830', '-0.00685', '-0.00839', '-0.00526', '-0.01156', '-0.02149', '-0.04896', '-0.03622']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 15.308% (Updated)\n",
      "ğŸ“Š Train Accuracy: 15.308% | ğŸ† Best Train Accuracy: 15.308%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 15.308% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.96it/s, Test_acc=18.8, Test_loss=3.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 18.790% | ğŸ† Best Test Accuracy: 18.790%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=4 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2127360493264 | Mean=2.30678 | Layer 1: ID=2127360542816 | Mean=2.30954 | Layer 2: ID=2127328825152 | Mean=2.30508 | Layer 3: ID=2127328824192 | Mean=2.29452 | Layer 4: ID=2127328823312 | Mean=2.29356 | Layer 5: ID=2127328822192 | Mean=2.29246 | Layer 6: ID=2127328827472 | Mean=2.29170 | Layer 7: ID=2128028320448 | Mean=2.29017 | Layer 8: ID=2128028321408 | Mean=2.29063 | Layer 9: ID=2128028322368 | Mean=2.28886 | Layer 10: ID=2128028323328 | Mean=2.28829 | Layer 11: ID=2128028324288 | Mean=2.28348 | Layer 12: ID=2128028325248 | Mean=2.28376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.66it/s, Train_acc=18.9, Train_loss=3.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.30767', '2.31467', '2.29648', '2.29814', '2.29627', '2.29757', '2.29487', '2.29244', '2.29502', '2.29342', '2.29307', '2.28495', '2.28134'] | Gamma1 Grad: ['0.00275', '-0.00357', '0.01833', '0.00010', '0.01075', '0.00303', '-0.00197', '0.00695', '-0.00230', '0.00835', '-0.00266', '0.00343', '0.03427']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 18.910% (Updated)\n",
      "ğŸ“Š Train Accuracy: 18.910% | ğŸ† Best Train Accuracy: 18.910%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 18.910% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.96it/s, Test_acc=21.8, Test_loss=2.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 21.800% | ğŸ† Best Test Accuracy: 21.800%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.30it/s, Train_acc=23.2, Train_loss=2.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.31964', '2.32035', '2.30908', '2.30085', '2.29983', '2.29701', '2.29220', '2.29584', '2.29237', '2.29126', '2.29175', '2.28566', '2.28012'] | Gamma1 Grad: ['-0.00169', '0.01128', '0.00566', '-0.00557', '-0.01439', '0.00598', '0.01323', '0.00402', '0.00225', '0.00334', '0.01306', '0.00563', '-0.02012']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 23.244% (Updated)\n",
      "ğŸ“Š Train Accuracy: 23.244% | ğŸ† Best Train Accuracy: 23.244%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 23.244% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.26it/s, Test_acc=26.5, Test_loss=2.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 26.480% | ğŸ† Best Test Accuracy: 26.480%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.14it/s, Train_acc=27.5, Train_loss=2.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.32416', '2.32425', '2.30423', '2.29431', '2.30371', '2.29706', '2.29404', '2.29373', '2.29168', '2.29085', '2.29021', '2.28901', '2.28375'] | Gamma1 Grad: ['-0.02106', '-0.02698', '-0.00018', '-0.00508', '-0.00214', '-0.00225', '-0.00119', '-0.00069', '-0.00423', '0.00359', '0.00445', '-0.01774', '0.01124']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 27.536% (Updated)\n",
      "ğŸ“Š Train Accuracy: 27.536% | ğŸ† Best Train Accuracy: 27.536%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 27.536% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.40it/s, Test_acc=27.9, Test_loss=2.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 27.860% | ğŸ† Best Test Accuracy: 27.860%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.95it/s, Train_acc=32.4, Train_loss=2.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.31486', '2.31402', '2.30515', '2.29368', '2.29879', '2.30046', '2.29262', '2.29570', '2.29195', '2.28844', '2.28548', '2.28478', '2.28760'] | Gamma1 Grad: ['-0.01125', '0.00381', '-0.00367', '-0.00029', '-0.00382', '-0.00575', '-0.00538', '0.00291', '-0.01134', '-0.01735', '-0.00891', '-0.03400', '0.06458']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 32.398% (Updated)\n",
      "ğŸ“Š Train Accuracy: 32.398% | ğŸ† Best Train Accuracy: 32.398%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 32.398% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.03it/s, Test_acc=35.3, Test_loss=2.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 35.270% | ğŸ† Best Test Accuracy: 35.270%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.22it/s, Train_acc=36.5, Train_loss=2.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.31146', '2.32683', '2.31005', '2.29775', '2.29575', '2.29653', '2.29167', '2.29737', '2.29804', '2.29969', '2.29666', '2.28343', '2.29309'] | Gamma1 Grad: ['0.00059', '-0.00650', '-0.02160', '0.00102', '0.01365', '0.00406', '0.00693', '0.00050', '-0.00214', '0.00113', '-0.00005', '0.00106', '0.02706']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 36.498% (Updated)\n",
      "ğŸ“Š Train Accuracy: 36.498% | ğŸ† Best Train Accuracy: 36.498%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 36.498% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.42it/s, Test_acc=36.7, Test_loss=2.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 36.660% | ğŸ† Best Test Accuracy: 36.660%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.66it/s, Train_acc=39.8, Train_loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.32212', '2.32421', '2.31204', '2.29345', '2.29895', '2.29404', '2.29149', '2.30182', '2.29743', '2.30086', '2.29781', '2.27970', '2.29877'] | Gamma1 Grad: ['-0.02836', '-0.02659', '0.00119', '-0.02045', '0.00442', '0.01982', '0.00370', '-0.00212', '-0.00104', '0.00044', '0.00068', '-0.00425', '0.01225']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 39.766% (Updated)\n",
      "ğŸ“Š Train Accuracy: 39.766% | ğŸ† Best Train Accuracy: 39.766%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 39.766% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.33it/s, Test_acc=43, Test_loss=2.04]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 43.040% | ğŸ† Best Test Accuracy: 43.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.72it/s, Train_acc=43.2, Train_loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.32282', '2.32150', '2.30924', '2.29468', '2.30007', '2.30376', '2.29473', '2.30074', '2.29497', '2.29573', '2.29791', '2.28608', '2.29638'] | Gamma1 Grad: ['-0.02302', '-0.00908', '-0.04815', '-0.02886', '-0.00751', '-0.00420', '0.00498', '0.00774', '0.01136', '0.00924', '0.00445', '-0.00769', '-0.00825']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 43.222% (Updated)\n",
      "ğŸ“Š Train Accuracy: 43.222% | ğŸ† Best Train Accuracy: 43.222%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 43.222% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 10 | TrainLossHist: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.45it/s, Test_acc=42.6, Test_loss=2.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 42.600% | ğŸ† Best Test Accuracy: 43.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.74it/s, Train_acc=46.1, Train_loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.31425', '2.31421', '2.32303', '2.30402', '2.30098', '2.29720', '2.29538', '2.29841', '2.30044', '2.29782', '2.29177', '2.27575', '2.29852'] | Gamma1 Grad: ['-0.00607', '0.02293', '-0.05030', '0.02227', '-0.03403', '-0.00430', '0.01624', '0.00418', '0.01182', '0.01577', '0.01069', '-0.02906', '0.05024']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 46.074% (Updated)\n",
      "ğŸ“Š Train Accuracy: 46.074% | ğŸ† Best Train Accuracy: 46.074%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 46.074% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.19it/s, Test_acc=46.9, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 46.870% | ğŸ† Best Test Accuracy: 46.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.45it/s, Train_acc=48.3, Train_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.30493', '2.31588', '2.31444', '2.29673', '2.31026', '2.30358', '2.29101', '2.30364', '2.29517', '2.29080', '2.28658', '2.28423', '2.30169'] | Gamma1 Grad: ['0.00591', '0.01218', '-0.00155', '-0.01016', '-0.00479', '-0.01093', '-0.00645', '-0.00238', '-0.01705', '-0.00710', '-0.00130', '-0.00499', '0.07167']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 48.314% (Updated)\n",
      "ğŸ“Š Train Accuracy: 48.314% | ğŸ† Best Train Accuracy: 48.314%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 48.314% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.95it/s, Test_acc=47.2, Test_loss=1.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 47.210% | ğŸ† Best Test Accuracy: 47.210%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.21it/s, Train_acc=51.1, Train_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.31746', '2.32333', '2.31146', '2.29182', '2.29519', '2.30298', '2.29198', '2.29790', '2.30074', '2.29117', '2.29050', '2.28624', '2.30384'] | Gamma1 Grad: ['0.02046', '0.01882', '-0.01973', '0.04581', '0.01233', '0.00718', '0.00487', '-0.00324', '-0.00484', '-0.00518', '0.00176', '0.02322', '0.02397']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 51.064% (Updated)\n",
      "ğŸ“Š Train Accuracy: 51.064% | ğŸ† Best Train Accuracy: 51.064%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 51.064% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.89it/s, Test_acc=50.4, Test_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 50.360% | ğŸ† Best Test Accuracy: 50.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.22it/s, Train_acc=52.7, Train_loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.29935', '2.31970', '2.30518', '2.28672', '2.29419', '2.31003', '2.29173', '2.29666', '2.29652', '2.29728', '2.29190', '2.28979', '2.29015'] | Gamma1 Grad: ['0.01196', '-0.00108', '0.01767', '-0.00919', '-0.01377', '-0.01721', '-0.00314', '-0.00419', '0.00976', '-0.01244', '-0.01169', '0.02673', '-0.01723']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 52.658% (Updated)\n",
      "ğŸ“Š Train Accuracy: 52.658% | ğŸ† Best Train Accuracy: 52.658%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 52.658% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.06it/s, Test_acc=51.2, Test_loss=1.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 51.240% | ğŸ† Best Test Accuracy: 51.240%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.67it/s, Train_acc=54.5, Train_loss=1.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['2.30256', '2.31298', '2.30607', '2.30227', '2.29331', '2.29888', '2.29701', '2.30916', '2.29218', '2.28844', '2.30149', '2.28222', '2.29009'] | Gamma1 Grad: ['0.02216', '0.01595', '0.00737', '-0.00388', '-0.02429', '-0.00303', '0.00267', '-0.00368', '0.01228', '0.01353', '0.01461', '0.00536', '0.03785']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 54.526% (Updated)\n",
      "ğŸ“Š Train Accuracy: 54.526% | ğŸ† Best Train Accuracy: 54.526%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 54.526% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.92it/s, Test_acc=52.7, Test_loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 52.690% | ğŸ† Best Test Accuracy: 52.690%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.72it/s, Train_acc=56.4, Train_loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.29803', '2.31970', '2.30353', '2.29487', '2.29711', '2.30323', '2.27723', '2.29304', '2.28531', '2.28689', '2.28412', '2.29535', '2.30016'] | Gamma1 Grad: ['0.01801', '-0.00951', '0.03132', '-0.00785', '-0.01877', '-0.01618', '-0.00946', '0.00524', '-0.00782', '-0.00415', '0.00059', '0.01259', '0.00812']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 56.410% (Updated)\n",
      "ğŸ“Š Train Accuracy: 56.410% | ğŸ† Best Train Accuracy: 56.410%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 56.410% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:04<00:00, 32.07it/s, Test_acc=53.6, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 53.620% | ğŸ† Best Test Accuracy: 53.620%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [01:17<00:00, 10.12it/s, Train_acc=58.2, Train_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['2.32411', '2.31292', '2.29566', '2.29474', '2.29767', '2.30188', '2.29340', '2.30439', '2.29241', '2.29254', '2.29064', '2.28863', '2.30508'] | Gamma1 Grad: ['-0.03342', '-0.00638', '-0.01936', '-0.02055', '-0.01921', '-0.01048', '0.00186', '0.01159', '0.01529', '-0.00341', '0.00392', '0.00990', '0.01730']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 58.198% (Updated)\n",
      "ğŸ“Š Train Accuracy: 58.198% | ğŸ† Best Train Accuracy: 58.198%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 58.198% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:04<00:00, 34.55it/s, Test_acc=54.5, Test_loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 54.500% | ğŸ† Best Test Accuracy: 54.500%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:56<00:00, 13.95it/s, Train_acc=59.9, Train_loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.30947', '2.31169', '2.29424', '2.30074', '2.31178', '2.30002', '2.30055', '2.30522', '2.30163', '2.29063', '2.28179', '2.28310', '2.31538'] | Gamma1 Grad: ['0.02469', '-0.01419', '0.00647', '-0.02912', '-0.01085', '-0.01476', '0.01454', '-0.01729', '-0.01039', '-0.00336', '0.00209', '-0.00817', '0.09006']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 59.856% (Updated)\n",
      "ğŸ“Š Train Accuracy: 59.856% | ğŸ† Best Train Accuracy: 59.856%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 59.856% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.24it/s, Test_acc=55.9, Test_loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 55.870% | ğŸ† Best Test Accuracy: 55.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.41it/s, Train_acc=61.2, Train_loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['2.31461', '2.31164', '2.31239', '2.29829', '2.30162', '2.30298', '2.29807', '2.30512', '2.30212', '2.29843', '2.29430', '2.28353', '2.30790'] | Gamma1 Grad: ['0.01281', '0.01039', '0.01728', '-0.01003', '0.03096', '0.01835', '-0.00435', '-0.00054', '-0.00366', '0.00080', '0.00639', '0.00745', '-0.01966']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 61.220% (Updated)\n",
      "ğŸ“Š Train Accuracy: 61.220% | ğŸ† Best Train Accuracy: 61.220%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 61.220% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.87it/s, Test_acc=56.5, Test_loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 56.460% | ğŸ† Best Test Accuracy: 56.460%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:30<00:00, 25.24it/s, Train_acc=62.6, Train_loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.30727', '2.31528', '2.30788', '2.30666', '2.29453', '2.30346', '2.29136', '2.29905', '2.29878', '2.28597', '2.28603', '2.27972', '2.30593'] | Gamma1 Grad: ['0.00840', '-0.02143', '0.01051', '0.08056', '-0.04984', '0.01812', '-0.01013', '0.00295', '-0.00569', '-0.00484', '0.00519', '-0.02282', '-0.04232']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 62.566% (Updated)\n",
      "ğŸ“Š Train Accuracy: 62.566% | ğŸ† Best Train Accuracy: 62.566%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 62.566% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 20 | TrainLossHist: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.26it/s, Test_acc=57.4, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 57.420% | ğŸ† Best Test Accuracy: 57.420%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:30<00:00, 25.47it/s, Train_acc=64, Train_loss=1.25]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['2.30387', '2.30998', '2.30807', '2.29209', '2.29286', '2.29853', '2.29722', '2.30468', '2.29629', '2.29289', '2.29570', '2.28981', '2.30009'] | Gamma1 Grad: ['0.03838', '0.00584', '0.03096', '-0.01325', '0.04146', '-0.00830', '-0.01252', '-0.00397', '0.01188', '0.00305', '0.01212', '0.01578', '0.01472']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 63.992% (Updated)\n",
      "ğŸ“Š Train Accuracy: 63.992% | ğŸ† Best Train Accuracy: 63.992%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 63.992% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.13it/s, Test_acc=58, Test_loss=1.54]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 57.950% | ğŸ† Best Test Accuracy: 57.950%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:30<00:00, 25.43it/s, Train_acc=65.3, Train_loss=1.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.31008', '2.31021', '2.31279', '2.29521', '2.29980', '2.30442', '2.29853', '2.30418', '2.29254', '2.28827', '2.29326', '2.28572', '2.31381'] | Gamma1 Grad: ['0.00770', '-0.00680', '0.00952', '0.01866', '0.00715', '0.03211', '0.01323', '0.00836', '-0.01435', '-0.02063', '-0.01539', '0.01988', '0.01751']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 65.330% (Updated)\n",
      "ğŸ“Š Train Accuracy: 65.330% | ğŸ† Best Train Accuracy: 65.330%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 65.330% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.69it/s, Test_acc=58.5, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 58.530% | ğŸ† Best Test Accuracy: 58.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.25it/s, Train_acc=66.7, Train_loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.29133', '2.31250', '2.30135', '2.29176', '2.30315', '2.29961', '2.30171', '2.30848', '2.30874', '2.29871', '2.30309', '2.29337', '2.31053'] | Gamma1 Grad: ['-0.01169', '-0.01729', '-0.00941', '-0.00557', '-0.01561', '0.00888', '0.00465', '0.01180', '0.00144', '0.00030', '-0.00106', '-0.00389', '0.06244']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 66.700% (Updated)\n",
      "ğŸ“Š Train Accuracy: 66.700% | ğŸ† Best Train Accuracy: 66.700%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 66.700% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.53it/s, Test_acc=59.5, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 59.520% | ğŸ† Best Test Accuracy: 59.520%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 25.13it/s, Train_acc=67.6, Train_loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.29693', '2.30941', '2.29726', '2.29556', '2.29772', '2.30723', '2.29947', '2.31073', '2.30211', '2.30045', '2.30276', '2.30007', '2.30451'] | Gamma1 Grad: ['-0.01025', '0.02108', '0.04978', '0.00091', '0.05874', '0.03300', '0.01378', '0.00621', '-0.00025', '-0.00245', '-0.00361', '0.00493', '0.01817']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 67.584% (Updated)\n",
      "ğŸ“Š Train Accuracy: 67.584% | ğŸ† Best Train Accuracy: 67.584%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 67.584% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.18it/s, Test_acc=58.7, Test_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 58.660% | ğŸ† Best Test Accuracy: 59.520%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.33it/s, Train_acc=68.8, Train_loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['2.30590', '2.31640', '2.31363', '2.28861', '2.30422', '2.30904', '2.29906', '2.30831', '2.29670', '2.29263', '2.29572', '2.28433', '2.30987'] | Gamma1 Grad: ['0.00122', '0.01523', '0.02765', '-0.03068', '0.01739', '-0.02237', '0.00259', '-0.00325', '-0.00808', '0.00305', '-0.00288', '0.01277', '-0.02541']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 68.834% (Updated)\n",
      "ğŸ“Š Train Accuracy: 68.834% | ğŸ† Best Train Accuracy: 68.834%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 68.834% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.65it/s, Test_acc=60.2, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.230% | ğŸ† Best Test Accuracy: 60.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.39it/s, Train_acc=70, Train_loss=1.03]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.30563', '2.31351', '2.31388', '2.27962', '2.30274', '2.31364', '2.29409', '2.30672', '2.30019', '2.29185', '2.29590', '2.27681', '2.30131'] | Gamma1 Grad: ['-0.05187', '-0.07469', '0.02861', '-0.08120', '-0.05292', '-0.03939', '-0.02358', '-0.01638', '-0.00377', '-0.01681', '-0.02377', '-0.01471', '-0.01646']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 69.986% (Updated)\n",
      "ğŸ“Š Train Accuracy: 69.986% | ğŸ† Best Train Accuracy: 69.986%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 69.986% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.85it/s, Test_acc=59.6, Test_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 59.610% | ğŸ† Best Test Accuracy: 60.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.47it/s, Train_acc=71.6, Train_loss=0.971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['2.29755', '2.31210', '2.29618', '2.28800', '2.30249', '2.30494', '2.29256', '2.30377', '2.29882', '2.29571', '2.29379', '2.29322', '2.30104'] | Gamma1 Grad: ['-0.07559', '0.01163', '-0.07763', '-0.01944', '0.00746', '-0.01421', '0.01430', '0.00858', '0.00880', '0.02660', '0.01164', '-0.00519', '0.07489']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 71.598% (Updated)\n",
      "ğŸ“Š Train Accuracy: 71.598% | ğŸ† Best Train Accuracy: 71.598%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 71.598% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.27it/s, Test_acc=60.6, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.640% | ğŸ† Best Test Accuracy: 60.640%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 25.13it/s, Train_acc=72.3, Train_loss=0.947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.30869', '2.31007', '2.29786', '2.29007', '2.30189', '2.30698', '2.30877', '2.31098', '2.29889', '2.29116', '2.29189', '2.28781', '2.30876'] | Gamma1 Grad: ['0.00055', '-0.00951', '0.02376', '-0.02085', '0.03789', '0.01287', '0.01174', '-0.00222', '0.00112', '0.01092', '0.00459', '0.00444', '-0.03452']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 72.272% (Updated)\n",
      "ğŸ“Š Train Accuracy: 72.272% | ğŸ† Best Train Accuracy: 72.272%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 72.272% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.11it/s, Test_acc=60.3, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 60.300% | ğŸ† Best Test Accuracy: 60.640%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:30<00:00, 25.25it/s, Train_acc=73.4, Train_loss=0.908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['2.29541', '2.31403', '2.29746', '2.29339', '2.29498', '2.30346', '2.30035', '2.31040', '2.30354', '2.29420', '2.29756', '2.28207', '2.30453'] | Gamma1 Grad: ['0.02475', '0.00380', '0.00322', '-0.01440', '-0.03385', '0.02090', '-0.00726', '-0.00340', '-0.01221', '0.00289', '0.00221', '-0.01824', '0.06819']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 73.380% (Updated)\n",
      "ğŸ“Š Train Accuracy: 73.380% | ğŸ† Best Train Accuracy: 73.380%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 73.380% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.06it/s, Test_acc=61.4, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.350% | ğŸ† Best Test Accuracy: 61.350%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.25it/s, Train_acc=74.2, Train_loss=0.878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.30171', '2.31520', '2.30611', '2.28635', '2.29743', '2.30478', '2.30580', '2.31045', '2.29672', '2.29180', '2.29235', '2.28501', '2.30780'] | Gamma1 Grad: ['-0.01978', '0.00268', '-0.03614', '0.01591', '0.00494', '0.00926', '0.00552', '0.00135', '-0.01947', '-0.00608', '0.00512', '0.02138', '-0.03413']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 74.162% (Updated)\n",
      "ğŸ“Š Train Accuracy: 74.162% | ğŸ† Best Train Accuracy: 74.162%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 74.162% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 30 | TrainLossHist: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.08it/s, Test_acc=61.5, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.480% | ğŸ† Best Test Accuracy: 61.480%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:30<00:00, 25.38it/s, Train_acc=75.4, Train_loss=0.835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.28983', '2.30955', '2.30351', '2.28937', '2.28825', '2.30665', '2.30474', '2.30468', '2.29267', '2.28263', '2.28856', '2.29509', '2.29270'] | Gamma1 Grad: ['-0.00162', '-0.00931', '0.03223', '-0.05096', '0.09600', '0.00266', '0.00407', '-0.00516', '-0.00892', '-0.00465', '0.00217', '0.01574', '-0.00026']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 75.408% (Updated)\n",
      "ğŸ“Š Train Accuracy: 75.408% | ğŸ† Best Train Accuracy: 75.408%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 75.408% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.40it/s, Test_acc=61.5, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.540% | ğŸ† Best Test Accuracy: 61.540%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:30<00:00, 25.27it/s, Train_acc=76.4, Train_loss=0.805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.29630', '2.30419', '2.29749', '2.30578', '2.29028', '2.29832', '2.28057', '2.28939', '2.29041', '2.29330', '2.28747', '2.27406', '2.30885'] | Gamma1 Grad: ['0.01384', '0.00094', '-0.04340', '-0.05509', '-0.06787', '-0.00019', '-0.00910', '-0.01236', '-0.01112', '-0.00619', '-0.01941', '-0.02888', '0.02242']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 76.362% (Updated)\n",
      "ğŸ“Š Train Accuracy: 76.362% | ğŸ† Best Train Accuracy: 76.362%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 76.362% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.45it/s, Test_acc=61.3, Test_loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 61.290% | ğŸ† Best Test Accuracy: 61.540%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 25.14it/s, Train_acc=76.9, Train_loss=0.78] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.31339', '2.30784', '2.30239', '2.29452', '2.29927', '2.29125', '2.29287', '2.30087', '2.30529', '2.29859', '2.29560', '2.29593', '2.29204'] | Gamma1 Grad: ['-0.01052', '-0.02639', '-0.00029', '-0.02854', '-0.00723', '-0.01093', '-0.00714', '0.00224', '-0.01114', '-0.00606', '-0.00474', '0.00352', '0.01020']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 76.932% (Updated)\n",
      "ğŸ“Š Train Accuracy: 76.932% | ğŸ† Best Train Accuracy: 76.932%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 76.932% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.05it/s, Test_acc=62.2, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.160% | ğŸ† Best Test Accuracy: 62.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.89it/s, Train_acc=77.6, Train_loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.29641', '2.31861', '2.30059', '2.28251', '2.30196', '2.30904', '2.31640', '2.31012', '2.30946', '2.29899', '2.29895', '2.28305', '2.29220'] | Gamma1 Grad: ['0.00433', '-0.04732', '-0.00496', '0.03384', '-0.02684', '-0.00201', '-0.02813', '-0.00808', '-0.01275', '0.00287', '0.01050', '0.01162', '0.00243']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 77.562% (Updated)\n",
      "ğŸ“Š Train Accuracy: 77.562% | ğŸ† Best Train Accuracy: 77.562%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 77.562% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.80it/s, Test_acc=62.4, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.420% | ğŸ† Best Test Accuracy: 62.420%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.83it/s, Train_acc=78.4, Train_loss=0.723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00144 | Gamma1: ['2.30606', '2.30695', '2.30088', '2.28075', '2.28380', '2.29928', '2.30807', '2.31136', '2.30075', '2.29825', '2.29968', '2.28126', '2.29168'] | Gamma1 Grad: ['-0.00309', '-0.01589', '-0.02392', '-0.03992', '-0.03251', '0.06098', '0.00314', '-0.01281', '-0.00044', '-0.00963', '-0.00068', '-0.00333', '0.00673']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 78.410% (Updated)\n",
      "ğŸ“Š Train Accuracy: 78.410% | ğŸ† Best Train Accuracy: 78.410%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 78.410% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.03it/s, Test_acc=62.2, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.210% | ğŸ† Best Test Accuracy: 62.420%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.68it/s, Train_acc=79.4, Train_loss=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.31599', '2.31484', '2.29380', '2.28816', '2.29695', '2.29641', '2.30010', '2.30156', '2.30022', '2.30064', '2.29769', '2.28821', '2.29474'] | Gamma1 Grad: ['0.01898', '0.01496', '-0.03974', '-0.00620', '0.07535', '-0.02106', '0.01847', '0.01389', '-0.02144', '-0.00417', '0.00295', '-0.01961', '0.02459']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 79.430% (Updated)\n",
      "ğŸ“Š Train Accuracy: 79.430% | ğŸ† Best Train Accuracy: 79.430%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 79.430% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 92.04it/s, Test_acc=62.6, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.630% | ğŸ† Best Test Accuracy: 62.630%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.69it/s, Train_acc=79.8, Train_loss=0.67] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00139 | Gamma1: ['2.30902', '2.31529', '2.28401', '2.28610', '2.29142', '2.30697', '2.30072', '2.29590', '2.29366', '2.29447', '2.29306', '2.29651', '2.31665'] | Gamma1 Grad: ['0.04790', '-0.06260', '0.02669', '-0.02954', '0.06087', '0.06331', '0.01133', '0.02414', '-0.00630', '0.00563', '-0.00060', '0.01585', '0.01453']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 79.756% (Updated)\n",
      "ğŸ“Š Train Accuracy: 79.756% | ğŸ† Best Train Accuracy: 79.756%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 79.756% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.42it/s, Test_acc=63.1, Test_loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.060% | ğŸ† Best Test Accuracy: 63.060%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.53it/s, Train_acc=80.8, Train_loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.28982', '2.30552', '2.29929', '2.29695', '2.29860', '2.31201', '2.31629', '2.30462', '2.28759', '2.28642', '2.29093', '2.28776', '2.30208'] | Gamma1 Grad: ['-0.01318', '-0.01411', '-0.05526', '0.00592', '-0.04548', '-0.04135', '0.04303', '-0.03463', '-0.00833', '-0.00061', '-0.00974', '-0.01649', '0.05153']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 80.784% (Updated)\n",
      "ğŸ“Š Train Accuracy: 80.784% | ğŸ† Best Train Accuracy: 80.784%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 80.784% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.24it/s, Test_acc=63.5, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.540% | ğŸ† Best Test Accuracy: 63.540%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:30<00:00, 25.27it/s, Train_acc=81.5, Train_loss=0.62] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00133 | Gamma1: ['2.30430', '2.31168', '2.30421', '2.28513', '2.29134', '2.29335', '2.31391', '2.30602', '2.29159', '2.29707', '2.28785', '2.29265', '2.30198'] | Gamma1 Grad: ['-0.04388', '0.00755', '-0.00108', '0.00236', '0.05768', '0.04370', '-0.01321', '0.00756', '-0.00484', '0.00426', '-0.00268', '-0.01785', '0.02012']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 81.454% (Updated)\n",
      "ğŸ“Š Train Accuracy: 81.454% | ğŸ† Best Train Accuracy: 81.454%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 81.454% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.71it/s, Test_acc=63.8, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.830% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 25.20it/s, Train_acc=82.3, Train_loss=0.596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['2.31429', '2.34061', '2.31328', '2.29643', '2.28298', '2.31101', '2.35524', '2.33598', '2.33813', '2.29832', '2.29781', '2.27822', '2.31498'] | Gamma1 Grad: ['-0.04547', '-0.01172', '0.04148', '-0.12473', '0.00162', '-0.05398', '-0.03054', '0.00571', '-0.01247', '-0.01514', '-0.00746', '0.01473', '-0.04958']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.294% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.294% | ğŸ† Best Train Accuracy: 82.294%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.294% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 40 | TrainLossHist: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.37it/s, Test_acc=63.4, Test_loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.360% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.17it/s, Train_acc=82.6, Train_loss=0.58] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00125 | Gamma1: ['2.31477', '2.37353', '2.33631', '2.30637', '2.29962', '2.33528', '2.36587', '2.37020', '2.32427', '2.29814', '2.28630', '2.29414', '2.30216'] | Gamma1 Grad: ['-0.00716', '0.04276', '-0.02482', '0.09243', '-0.03863', '-0.00991', '0.01016', '-0.02608', '-0.01206', '0.00796', '0.01146', '0.00118', '0.01986']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.596% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.596% | ğŸ† Best Train Accuracy: 82.596%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.596% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.25it/s, Test_acc=63.1, Test_loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.140% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:30<00:00, 25.28it/s, Train_acc=83.4, Train_loss=0.558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.34838', '2.34645', '2.28608', '2.30731', '2.28035', '2.30982', '2.36521', '2.35277', '2.33280', '2.29061', '2.30945', '2.27670', '2.31228'] | Gamma1 Grad: ['-0.01457', '0.06011', '0.02453', '-0.02316', '0.07679', '0.02251', '0.01048', '0.00989', '0.00297', '-0.00334', '-0.00394', '-0.02537', '0.04438']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.378% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.378% | ğŸ† Best Train Accuracy: 83.378%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.378% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.42it/s, Test_acc=63.4, Test_loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.400% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:30<00:00, 25.49it/s, Train_acc=83.6, Train_loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00115 | Gamma1: ['2.33170', '2.32537', '2.30804', '2.28214', '2.28169', '2.32291', '2.35997', '2.34248', '2.33302', '2.28765', '2.30198', '2.27958', '2.31084'] | Gamma1 Grad: ['0.06639', '0.03837', '0.00615', '0.01653', '-0.02367', '0.01267', '-0.01422', '-0.00043', '0.01368', '-0.00481', '-0.00176', '0.00588', '-0.02157']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.584% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.584% | ğŸ† Best Train Accuracy: 83.584%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.584% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.08it/s, Test_acc=63.2, Test_loss=1.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.200% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.59it/s, Train_acc=84.7, Train_loss=0.516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['2.32981', '2.34063', '2.32590', '2.29042', '2.26673', '2.30262', '2.35922', '2.35316', '2.32653', '2.29492', '2.29268', '2.29121', '2.31714'] | Gamma1 Grad: ['-0.02270', '-0.07430', '0.02483', '-0.08092', '0.02280', '0.08510', '-0.03377', '0.03746', '0.01649', '0.00336', '0.00921', '-0.00862', '-0.11721']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 84.662% (Updated)\n",
      "ğŸ“Š Train Accuracy: 84.662% | ğŸ† Best Train Accuracy: 84.662%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 84.662% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.21it/s, Test_acc=63.7, Test_loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.740% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.79it/s, Train_acc=84.7, Train_loss=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00105 | Gamma1: ['2.31585', '2.34411', '2.33370', '2.27407', '2.28812', '2.30582', '2.36474', '2.36359', '2.33781', '2.29859', '2.30597', '2.30023', '2.30610'] | Gamma1 Grad: ['0.00749', '-0.00132', '0.01015', '0.02172', '0.00694', '0.04532', '0.00812', '-0.03513', '0.00524', '-0.00809', '-0.00779', '-0.03285', '0.09402']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 84.740% (Updated)\n",
      "ğŸ“Š Train Accuracy: 84.740% | ğŸ† Best Train Accuracy: 84.740%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 84.740% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.22it/s, Test_acc=62.9, Test_loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.940% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.92it/s, Train_acc=85.4, Train_loss=0.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.34123', '2.35597', '2.30925', '2.29621', '2.32025', '2.31349', '2.39066', '2.36485', '2.33227', '2.30715', '2.31601', '2.29765', '2.30154'] | Gamma1 Grad: ['-0.01451', '0.00230', '-0.01253', '-0.01164', '0.00875', '0.01258', '-0.01569', '-0.01302', '0.00794', '0.00489', '-0.00223', '0.00722', '-0.01150']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.436% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.436% | ğŸ† Best Train Accuracy: 85.436%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.436% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.21it/s, Test_acc=62.6, Test_loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.600% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.36it/s, Train_acc=85.9, Train_loss=0.472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00094 | Gamma1: ['2.33605', '2.34349', '2.31133', '2.29844', '2.28441', '2.33731', '2.38628', '2.36610', '2.34344', '2.29613', '2.30874', '2.29515', '2.32280'] | Gamma1 Grad: ['-0.02451', '0.00218', '0.02390', '0.00868', '-0.08697', '-0.00102', '0.05577', '0.01786', '0.00404', '0.00265', '-0.00538', '-0.00953', '-0.00898']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.936% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.936% | ğŸ† Best Train Accuracy: 85.936%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.936% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.97it/s, Test_acc=63.2, Test_loss=1.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.230% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.85it/s, Train_acc=86.3, Train_loss=0.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['2.33799', '2.33325', '2.30924', '2.29482', '2.27188', '2.29118', '2.37613', '2.36905', '2.32687', '2.28598', '2.29150', '2.27230', '2.33118'] | Gamma1 Grad: ['-0.07553', '-0.03106', '-0.00859', '0.08813', '-0.11673', '-0.00519', '0.05912', '-0.03371', '0.01845', '-0.00077', '0.00227', '-0.00906', '0.05899']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.298% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.298% | ğŸ† Best Train Accuracy: 86.298%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.298% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.18it/s, Test_acc=64.1, Test_loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.080% | ğŸ† Best Test Accuracy: 64.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.48it/s, Train_acc=86.6, Train_loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00083 | Gamma1: ['2.28272', '2.32897', '2.30225', '2.28844', '2.24468', '2.30736', '2.33811', '2.36451', '2.34157', '2.30130', '2.30111', '2.29378', '2.32004'] | Gamma1 Grad: ['0.00851', '-0.06029', '-0.03104', '0.00040', '0.02465', '-0.08611', '0.02199', '0.00081', '-0.00175', '-0.00511', '0.00540', '0.01522', '0.01777']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.576% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.576% | ğŸ† Best Train Accuracy: 86.576%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.576% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.63it/s, Test_acc=63.2, Test_loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.210% | ğŸ† Best Test Accuracy: 64.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:30<00:00, 25.33it/s, Train_acc=87.3, Train_loss=0.424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.30480', '2.32709', '2.32491', '2.25455', '2.25483', '2.32879', '2.33078', '2.35124', '2.31840', '2.29494', '2.29020', '2.29565', '2.31403'] | Gamma1 Grad: ['-0.02261', '-0.03950', '-0.05273', '-0.00931', '-0.08555', '0.00973', '-0.02167', '-0.01504', '-0.01173', '-0.00574', '-0.00564', '-0.01525', '-0.02356']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.296% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.296% | ğŸ† Best Train Accuracy: 87.296%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.296% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 50 | TrainLossHist: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.64it/s, Test_acc=63.4, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.440% | ğŸ† Best Test Accuracy: 64.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 51 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.54it/s, Train_acc=87.7, Train_loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00072 | Gamma1: ['2.31859', '2.35199', '2.29185', '2.28934', '2.26462', '2.30580', '2.35087', '2.37124', '2.32661', '2.29324', '2.29483', '2.29986', '2.32263'] | Gamma1 Grad: ['-0.04976', '-0.03789', '-0.00838', '-0.00596', '-0.08482', '-0.06421', '0.00869', '0.01300', '0.02568', '0.01999', '0.00489', '0.00864', '0.02477']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.678% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.678% | ğŸ† Best Train Accuracy: 87.678%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.678% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.40it/s, Test_acc=63.9, Test_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.850% | ğŸ† Best Test Accuracy: 64.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 52 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.14it/s, Train_acc=87.8, Train_loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['2.30794', '2.31889', '2.30601', '2.25413', '2.27015', '2.29542', '2.33217', '2.37287', '2.33214', '2.29456', '2.29863', '2.28660', '2.30067'] | Gamma1 Grad: ['0.01165', '-0.00434', '0.03638', '0.01742', '0.03098', '0.00067', '-0.07910', '-0.00096', '0.01865', '0.01182', '0.00379', '-0.00054', '-0.06715']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.786% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.786% | ğŸ† Best Train Accuracy: 87.786%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.786% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 92.25it/s, Test_acc=63.8, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.790% | ğŸ† Best Test Accuracy: 64.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 53 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.95it/s, Train_acc=88.4, Train_loss=0.392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00061 | Gamma1: ['2.32755', '2.35123', '2.30671', '2.26164', '2.27657', '2.31412', '2.32385', '2.34678', '2.32553', '2.30497', '2.29732', '2.28310', '2.29307'] | Gamma1 Grad: ['-0.01542', '0.00086', '-0.01436', '0.00989', '-0.15817', '0.03943', '-0.00353', '0.01127', '0.00235', '-0.00164', '-0.01207', '-0.00284', '-0.02360']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.392% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.392% | ğŸ† Best Train Accuracy: 88.392%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.392% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.50it/s, Test_acc=63.5, Test_loss=1.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.500% | ğŸ† Best Test Accuracy: 64.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 54 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.87it/s, Train_acc=88.7, Train_loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.32583', '2.33303', '2.30092', '2.28278', '2.23986', '2.27991', '2.33455', '2.35469', '2.32389', '2.30077', '2.28119', '2.28000', '2.29527'] | Gamma1 Grad: ['0.05484', '0.01125', '-0.06953', '-0.06513', '-0.05602', '0.07852', '0.10328', '-0.02272', '-0.01698', '-0.00379', '0.00515', '-0.04548', '0.01778']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.718% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.718% | ğŸ† Best Train Accuracy: 88.718%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.718% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.71it/s, Test_acc=64.3, Test_loss=1.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.320% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.04it/s, Train_acc=88.8, Train_loss=0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00050 | Gamma1: ['2.32080', '2.33814', '2.27608', '2.28194', '2.24768', '2.27985', '2.34117', '2.36189', '2.32616', '2.29434', '2.29266', '2.28427', '2.29739'] | Gamma1 Grad: ['-0.02141', '0.02100', '-0.01341', '0.02854', '-0.08707', '0.03928', '0.02462', '0.00937', '-0.02300', '-0.00483', '0.00357', '0.01737', '0.02910']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.826% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.826% | ğŸ† Best Train Accuracy: 88.826%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.826% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.50it/s, Test_acc=63.3, Test_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.290% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.45it/s, Train_acc=89.6, Train_loss=0.354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.31020', '2.32429', '2.27758', '2.28849', '2.26919', '2.27108', '2.34338', '2.34521', '2.32904', '2.30393', '2.30304', '2.28721', '2.31776'] | Gamma1 Grad: ['0.03510', '-0.03580', '0.05352', '-0.04941', '0.13898', '-0.07576', '-0.03895', '0.02435', '-0.01193', '-0.00156', '0.02067', '0.00233', '0.02262']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.564% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.564% | ğŸ† Best Train Accuracy: 89.564%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.564% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.42it/s, Test_acc=64.2, Test_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.180% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.04it/s, Train_acc=89.7, Train_loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00040 | Gamma1: ['2.32272', '2.32524', '2.29783', '2.27855', '2.24659', '2.29307', '2.32872', '2.34857', '2.32889', '2.30195', '2.30540', '2.28179', '2.31218'] | Gamma1 Grad: ['-0.04966', '0.01904', '-0.00849', '0.05820', '-0.00436', '-0.00042', '-0.01050', '0.02943', '-0.01140', '-0.00553', '0.00339', '-0.00460', '0.03427']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.676% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.676% | ğŸ† Best Train Accuracy: 89.676%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.676% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 93.48it/s, Test_acc=64, Test_loss=1.74]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.030% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.05it/s, Train_acc=89.9, Train_loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.30703', '2.32425', '2.29402', '2.27254', '2.26212', '2.28544', '2.33204', '2.35496', '2.31538', '2.30198', '2.30021', '2.29521', '2.30505'] | Gamma1 Grad: ['-0.00519', '0.08491', '-0.04561', '-0.00168', '-0.08446', '-0.01336', '-0.03134', '0.03390', '0.02247', '-0.01161', '-0.00027', '-0.00163', '0.03353']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.870% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.870% | ğŸ† Best Train Accuracy: 89.870%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.870% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.34it/s, Test_acc=64.3, Test_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.260% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.95it/s, Train_acc=90.1, Train_loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00030 | Gamma1: ['2.31177', '2.32582', '2.29287', '2.28132', '2.24943', '2.29945', '2.33092', '2.35268', '2.32667', '2.30027', '2.28872', '2.29991', '2.30215'] | Gamma1 Grad: ['-0.03003', '0.07003', '-0.03799', '-0.03272', '-0.01569', '0.00277', '-0.02399', '0.02639', '-0.01296', '0.01277', '0.00856', '-0.00268', '0.00934']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.122% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.122% | ğŸ† Best Train Accuracy: 90.122%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.122% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.30it/s, Test_acc=63.7, Test_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.710% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.86it/s, Train_acc=90.4, Train_loss=0.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['2.32534', '2.33358', '2.28338', '2.27251', '2.25242', '2.28606', '2.33240', '2.38246', '2.34202', '2.30196', '2.28882', '2.27408', '2.30828'] | Gamma1 Grad: ['-0.00322', '-0.01073', '-0.03100', '0.03324', '-0.04071', '-0.06583', '0.02282', '-0.00629', '0.01485', '-0.00196', '-0.00798', '-0.00407', '-0.00537']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.380% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.380% | ğŸ† Best Train Accuracy: 90.380%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.380% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 60 | TrainLossHist: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.07it/s, Test_acc=64, Test_loss=1.79]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.990% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.45it/s, Train_acc=90.8, Train_loss=0.315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00022 | Gamma1: ['2.35176', '2.35647', '2.30769', '2.28188', '2.26445', '2.29697', '2.35425', '2.39542', '2.35878', '2.29910', '2.30014', '2.29223', '2.30761'] | Gamma1 Grad: ['-0.02554', '-0.01911', '-0.04601', '0.00915', '0.04428', '-0.06263', '-0.00309', '-0.00966', '0.00058', '-0.00894', '0.01018', '-0.01882', '-0.04263']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.796% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.796% | ğŸ† Best Train Accuracy: 90.796%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.796% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.09it/s, Test_acc=63.3, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.320% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.95it/s, Train_acc=91, Train_loss=0.308]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.32994', '2.37157', '2.28161', '2.28071', '2.24197', '2.29359', '2.33653', '2.38559', '2.36688', '2.30544', '2.29974', '2.28219', '2.30280'] | Gamma1 Grad: ['-0.01933', '0.01994', '-0.01775', '0.02336', '0.08305', '0.02483', '-0.00348', '0.00641', '-0.00101', '-0.01491', '-0.01320', '-0.00429', '-0.04146']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.984% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.984% | ğŸ† Best Train Accuracy: 90.984%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.984% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.01it/s, Test_acc=64.2, Test_loss=1.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.220% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.86it/s, Train_acc=91.4, Train_loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00016 | Gamma1: ['2.32677', '2.36387', '2.30948', '2.26614', '2.24074', '2.26594', '2.34881', '2.38017', '2.36279', '2.29854', '2.30199', '2.26652', '2.30374'] | Gamma1 Grad: ['-0.04840', '0.01169', '-0.08574', '0.00039', '0.00787', '-0.00658', '0.03086', '-0.00143', '-0.00763', '0.00401', '0.00762', '-0.00092', '-0.07544']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.392% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.392% | ğŸ† Best Train Accuracy: 91.392%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.392% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.05it/s, Test_acc=63.9, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.870% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.63it/s, Train_acc=91.2, Train_loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['2.31991', '2.36695', '2.29043', '2.26867', '2.24011', '2.25367', '2.36748', '2.39827', '2.35427', '2.30101', '2.29470', '2.26141', '2.30399'] | Gamma1 Grad: ['-0.03398', '0.01436', '-0.00972', '0.03451', '-0.06705', '-0.04862', '0.00915', '0.01911', '-0.00641', '0.01337', '0.00815', '0.01308', '0.03797']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 91.214% | ğŸ† Best Train Accuracy: 91.392%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.392% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.33it/s, Test_acc=64, Test_loss=1.83]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.990% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.60it/s, Train_acc=91.7, Train_loss=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00011 | Gamma1: ['2.31769', '2.35877', '2.29610', '2.26615', '2.24156', '2.26095', '2.36220', '2.39438', '2.35204', '2.29392', '2.29414', '2.27632', '2.30034'] | Gamma1 Grad: ['0.00818', '-0.01641', '-0.00860', '-0.02418', '0.03682', '0.02853', '-0.01254', '0.01180', '0.00069', '-0.00173', '0.00490', '-0.00354', '0.00255']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.664% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.664% | ğŸ† Best Train Accuracy: 91.664%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.664% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.94it/s, Test_acc=63.9, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.850% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.42it/s, Train_acc=92, Train_loss=0.277]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.34421', '2.37291', '2.30306', '2.27867', '2.21458', '2.25525', '2.37986', '2.38545', '2.35870', '2.29339', '2.29837', '2.27025', '2.29761'] | Gamma1 Grad: ['0.01483', '-0.02781', '0.01044', '0.01166', '-0.04643', '0.04239', '-0.00299', '0.00583', '0.00483', '0.00793', '-0.00399', '0.00068', '-0.01499']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.012% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.012% | ğŸ† Best Train Accuracy: 92.012%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.012% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.84it/s, Test_acc=63.9, Test_loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.920% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.20it/s, Train_acc=92.1, Train_loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.33268', '2.35658', '2.29671', '2.25229', '2.23561', '2.25479', '2.36896', '2.38558', '2.34257', '2.29831', '2.30352', '2.27227', '2.29392'] | Gamma1 Grad: ['-0.06704', '0.12688', '0.02481', '0.00621', '-0.00026', '0.05239', '-0.03739', '0.09580', '0.01077', '-0.00067', '0.00846', '-0.02874', '0.00551']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.060% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.060% | ğŸ† Best Train Accuracy: 92.060%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.060% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 92.35it/s, Test_acc=63.3, Test_loss=1.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.330% | ğŸ† Best Test Accuracy: 64.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.42it/s, Train_acc=92, Train_loss=0.275]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['2.31509', '2.33898', '2.30060', '2.27022', '2.20398', '2.26520', '2.35455', '2.37755', '2.34161', '2.29673', '2.30093', '2.28212', '2.28944'] | Gamma1 Grad: ['0.01805', '-0.03482', '-0.01356', '0.01430', '0.01877', '-0.03308', '-0.01757', '-0.01917', '0.00913', '0.00251', '0.00235', '-0.01767', '-0.00006']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 92.032% | ğŸ† Best Train Accuracy: 92.060%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.060% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.18it/s, Test_acc=64.9, Test_loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.910% | ğŸ† Best Test Accuracy: 64.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.73it/s, Train_acc=92.8, Train_loss=0.257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['2.32348', '2.34458', '2.28501', '2.26529', '2.21909', '2.26615', '2.34403', '2.36358', '2.35196', '2.29366', '2.29638', '2.27284', '2.28484'] | Gamma1 Grad: ['-0.00447', '-0.02880', '-0.00684', '-0.00208', '0.01304', '0.02132', '-0.01671', '0.01090', '0.00397', '0.00640', '0.01023', '0.01385', '0.01673']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.788% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.788% | ğŸ† Best Train Accuracy: 92.788%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.788% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.60it/s, Test_acc=64, Test_loss=1.89]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.030% | ğŸ† Best Test Accuracy: 64.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.42it/s, Train_acc=92.7, Train_loss=0.257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.32043', '2.34638', '2.29628', '2.25242', '2.21304', '2.28000', '2.33436', '2.36992', '2.34609', '2.29927', '2.29663', '2.27383', '2.28609'] | Gamma1 Grad: ['-0.00261', '-0.03752', '-0.03740', '-0.02093', '-0.06421', '0.05687', '0.01110', '-0.04944', '0.01590', '0.01106', '0.01166', '-0.02655', '0.00862']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 92.680% | ğŸ† Best Train Accuracy: 92.788%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.788% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 70 | TrainLossHist: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.69it/s, Test_acc=64.7, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.650% | ğŸ† Best Test Accuracy: 64.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.91it/s, Train_acc=92.9, Train_loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.34331', '2.36216', '2.30279', '2.27319', '2.21331', '2.28842', '2.36991', '2.38071', '2.36758', '2.28275', '2.28100', '2.30473', '2.28810'] | Gamma1 Grad: ['0.03084', '-0.02091', '-0.01403', '-0.01444', '0.01495', '0.10163', '0.00429', '-0.00164', '-0.00453', '-0.00253', '0.00631', '-0.00644', '0.00805']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.912% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.912% | ğŸ† Best Train Accuracy: 92.912%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.912% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.45it/s, Test_acc=64.2, Test_loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.160% | ğŸ† Best Test Accuracy: 64.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.10it/s, Train_acc=92.7, Train_loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.34502', '2.37701', '2.32705', '2.26229', '2.21994', '2.25289', '2.36206', '2.40709', '2.37500', '2.27813', '2.27695', '2.29266', '2.28465'] | Gamma1 Grad: ['0.00985', '0.02292', '0.00838', '-0.03038', '-0.00401', '-0.02985', '-0.01328', '-0.00628', '-0.00491', '0.00088', '-0.00386', '-0.00482', '-0.00228']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 92.742% | ğŸ† Best Train Accuracy: 92.912%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.912% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.19it/s, Test_acc=64.9, Test_loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.930% | ğŸ† Best Test Accuracy: 64.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.10it/s, Train_acc=93.3, Train_loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.31401', '2.36371', '2.27891', '2.25799', '2.21111', '2.27107', '2.34527', '2.37812', '2.35491', '2.29714', '2.29961', '2.28970', '2.31101'] | Gamma1 Grad: ['0.00390', '0.02691', '-0.02525', '0.01115', '0.03088', '-0.00253', '0.01308', '-0.00216', '0.00703', '0.00214', '0.00528', '0.00342', '-0.03550']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.292% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.292% | ğŸ† Best Train Accuracy: 93.292%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.292% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.32it/s, Test_acc=64.6, Test_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.600% | ğŸ† Best Test Accuracy: 64.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.03it/s, Train_acc=93.3, Train_loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.32131', '2.35830', '2.29757', '2.28064', '2.21961', '2.24323', '2.32901', '2.38708', '2.34643', '2.31198', '2.27803', '2.26337', '2.30668'] | Gamma1 Grad: ['-0.05106', '-0.04039', '-0.09849', '-0.02085', '-0.00114', '-0.10667', '0.00469', '-0.00881', '0.00666', '0.01384', '0.00434', '0.02169', '0.03002']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.310% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.310% | ğŸ† Best Train Accuracy: 93.310%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.310% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 92.00it/s, Test_acc=64.2, Test_loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.220% | ğŸ† Best Test Accuracy: 64.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.40it/s, Train_acc=93.3, Train_loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.31177', '2.34525', '2.30989', '2.26070', '2.23028', '2.22485', '2.31850', '2.37972', '2.33467', '2.31934', '2.29152', '2.27793', '2.29341'] | Gamma1 Grad: ['0.05776', '-0.04014', '-0.00423', '-0.00445', '-0.03446', '0.05912', '0.04808', '-0.00973', '-0.01040', '-0.00850', '0.00036', '0.02245', '-0.00847']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.284% | ğŸ† Best Train Accuracy: 93.310%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.310% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.59it/s, Test_acc=64.3, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.290% | ğŸ† Best Test Accuracy: 64.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.04it/s, Train_acc=93.3, Train_loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.31830', '2.34308', '2.32841', '2.26362', '2.21224', '2.23959', '2.35997', '2.36884', '2.31721', '2.31241', '2.29561', '2.26719', '2.30367'] | Gamma1 Grad: ['0.01188', '0.02825', '-0.02398', '-0.00256', '-0.05602', '0.08312', '-0.02368', '-0.02554', '0.00664', '0.00062', '-0.00354', '-0.01186', '0.00656']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.328% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.328% | ğŸ† Best Train Accuracy: 93.328%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.328% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.07it/s, Test_acc=64.9, Test_loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.860% | ğŸ† Best Test Accuracy: 64.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.04it/s, Train_acc=93.7, Train_loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00147 | Gamma1: ['2.29610', '2.34859', '2.33623', '2.27559', '2.20066', '2.23391', '2.31869', '2.37242', '2.31371', '2.28467', '2.28639', '2.27396', '2.29145'] | Gamma1 Grad: ['0.01253', '0.02395', '0.01154', '-0.06809', '-0.08646', '0.02954', '0.02853', '-0.00441', '-0.02720', '0.01001', '-0.00607', '0.00711', '-0.00321']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.732% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.732% | ğŸ† Best Train Accuracy: 93.732%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.732% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.87it/s, Test_acc=64.5, Test_loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.500% | ğŸ† Best Test Accuracy: 64.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.92it/s, Train_acc=93.7, Train_loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.31850', '2.37764', '2.30665', '2.26787', '2.21268', '2.23865', '2.34199', '2.37126', '2.32706', '2.29043', '2.29833', '2.26814', '2.28296'] | Gamma1 Grad: ['-0.01899', '0.00486', '0.04485', '-0.01527', '0.08549', '-0.01710', '0.03505', '-0.00004', '0.00294', '0.00102', '0.00913', '0.00876', '0.00259']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.682% | ğŸ† Best Train Accuracy: 93.732%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.732% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.91it/s, Test_acc=64.7, Test_loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.650% | ğŸ† Best Test Accuracy: 64.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.06it/s, Train_acc=93.9, Train_loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00146 | Gamma1: ['2.32764', '2.35071', '2.25734', '2.25361', '2.20805', '2.24296', '2.32575', '2.39605', '2.33136', '2.30007', '2.30887', '2.27995', '2.31086'] | Gamma1 Grad: ['0.03284', '-0.00844', '0.06324', '-0.03679', '0.06259', '-0.05661', '0.02302', '-0.00434', '0.01404', '-0.00003', '-0.00329', '-0.00514', '-0.02224']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.886% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.886% | ğŸ† Best Train Accuracy: 93.886%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.886% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.26it/s, Test_acc=65.4, Test_loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 65.410% | ğŸ† Best Test Accuracy: 65.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.57it/s, Train_acc=96.5, Train_loss=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00144 | Gamma1: ['2.31072', '2.33664', '2.30747', '2.23589', '2.21429', '2.24480', '2.32725', '2.36171', '2.33122', '2.31937', '2.31879', '2.28693', '2.30114'] | Gamma1 Grad: ['0.01697', '0.00900', '-0.01609', '-0.02055', '0.03123', '0.02441', '-0.01159', '-0.01436', '-0.01115', '0.00061', '0.00900', '0.00610', '0.01663']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 96.498% (Updated)\n",
      "ğŸ“Š Train Accuracy: 96.498% | ğŸ† Best Train Accuracy: 96.498%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 96.498% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 80 | TrainLossHist: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.65it/s, Test_acc=66.9, Test_loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 66.920% | ğŸ† Best Test Accuracy: 66.920%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.77it/s, Train_acc=97.5, Train_loss=0.107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00143 | Gamma1: ['2.31457', '2.34241', '2.31203', '2.23765', '2.22495', '2.25583', '2.30147', '2.33309', '2.32510', '2.31811', '2.32596', '2.28789', '2.31468'] | Gamma1 Grad: ['-0.00281', '-0.00188', '0.01210', '-0.03139', '-0.04090', '-0.01581', '0.00355', '0.00003', '-0.00754', '0.00575', '0.00509', '-0.00267', '0.01472']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.480% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.480% | ğŸ† Best Train Accuracy: 97.480%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.480% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.74it/s, Test_acc=67.2, Test_loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.190% | ğŸ† Best Test Accuracy: 67.190%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.87it/s, Train_acc=98, Train_loss=0.094]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00142 | Gamma1: ['2.28816', '2.32154', '2.29793', '2.26432', '2.23713', '2.26081', '2.32516', '2.34559', '2.32961', '2.31401', '2.32771', '2.28798', '2.32601'] | Gamma1 Grad: ['-0.01987', '0.05063', '-0.02693', '0.12790', '-0.03312', '-0.09389', '0.02205', '-0.05463', '0.01287', '0.01299', '0.01008', '-0.02455', '0.00173']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.040% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.040% | ğŸ† Best Train Accuracy: 98.040%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.040% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.72it/s, Test_acc=67.2, Test_loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.210% | ğŸ† Best Test Accuracy: 67.210%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.66it/s, Train_acc=98.3, Train_loss=0.087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00141 | Gamma1: ['2.28268', '2.30604', '2.30266', '2.23609', '2.22819', '2.26474', '2.30725', '2.34079', '2.32184', '2.31616', '2.32583', '2.29527', '2.32320'] | Gamma1 Grad: ['0.05643', '0.01302', '-0.02517', '-0.00891', '-0.00172', '-0.00233', '-0.04950', '0.03642', '0.02106', '-0.00353', '0.01129', '0.01480', '0.02400']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.312% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.312% | ğŸ† Best Train Accuracy: 98.312%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.312% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.82it/s, Test_acc=67.4, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.420% | ğŸ† Best Test Accuracy: 67.420%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.07it/s, Train_acc=98.4, Train_loss=0.083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00139 | Gamma1: ['2.29644', '2.31047', '2.29941', '2.22247', '2.23324', '2.23905', '2.32093', '2.33457', '2.30618', '2.31793', '2.33245', '2.29609', '2.32121'] | Gamma1 Grad: ['-0.01330', '0.00208', '0.01848', '0.01345', '-0.00637', '-0.05476', '0.04439', '-0.03177', '0.00485', '-0.00112', '-0.00333', '-0.00213', '0.00990']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.354% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.354% | ğŸ† Best Train Accuracy: 98.354%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.354% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.05it/s, Test_acc=67.4, Test_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.370% | ğŸ† Best Test Accuracy: 67.420%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.38it/s, Train_acc=98.4, Train_loss=0.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00138 | Gamma1: ['2.30304', '2.31542', '2.31208', '2.23983', '2.22127', '2.27408', '2.31074', '2.34774', '2.31010', '2.31514', '2.31610', '2.30238', '2.31964'] | Gamma1 Grad: ['0.02801', '0.05064', '0.03971', '-0.04604', '0.03181', '0.02337', '0.01394', '0.02660', '0.02124', '-0.01473', '-0.00871', '-0.01811', '-0.00456']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.438% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.438% | ğŸ† Best Train Accuracy: 98.438%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.438% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.60it/s, Test_acc=67.3, Test_loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.340% | ğŸ† Best Test Accuracy: 67.420%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.60it/s, Train_acc=98.5, Train_loss=0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00136 | Gamma1: ['2.28811', '2.31748', '2.30157', '2.25301', '2.22944', '2.25489', '2.31895', '2.32995', '2.31132', '2.31830', '2.30687', '2.29736', '2.33577'] | Gamma1 Grad: ['-0.04295', '-0.00602', '-0.01373', '0.16398', '0.05494', '-0.00861', '0.04219', '-0.00380', '-0.00538', '-0.00779', '-0.00826', '-0.00614', '0.00711']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.512% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.512% | ğŸ† Best Train Accuracy: 98.512%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.512% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.61it/s, Test_acc=67.4, Test_loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.420% | ğŸ† Best Test Accuracy: 67.420%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.37it/s, Train_acc=98.7, Train_loss=0.073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00134 | Gamma1: ['2.30110', '2.29917', '2.28723', '2.25644', '2.22571', '2.24833', '2.30085', '2.32486', '2.31236', '2.31552', '2.31507', '2.31295', '2.32195'] | Gamma1 Grad: ['-0.02169', '-0.00182', '-0.04356', '0.06406', '-0.01736', '0.05379', '-0.01818', '-0.02168', '-0.00414', '0.00065', '-0.00091', '0.00062', '-0.01568']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.716% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.716% | ğŸ† Best Train Accuracy: 98.716%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.716% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.76it/s, Test_acc=67.4, Test_loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.420% | ğŸ† Best Test Accuracy: 67.420%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.53it/s, Train_acc=98.8, Train_loss=0.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00133 | Gamma1: ['2.30048', '2.31587', '2.30005', '2.25512', '2.26034', '2.26513', '2.29455', '2.32425', '2.29710', '2.31300', '2.30993', '2.30798', '2.33682'] | Gamma1 Grad: ['-0.03222', '0.05366', '-0.01758', '0.01809', '-0.02266', '-0.15314', '-0.02052', '-0.05356', '0.00424', '-0.00791', '-0.00553', '-0.02700', '0.04719']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.782% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.782% | ğŸ† Best Train Accuracy: 98.782%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.782% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.46it/s, Test_acc=67.5, Test_loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.510% | ğŸ† Best Test Accuracy: 67.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.83it/s, Train_acc=98.9, Train_loss=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00131 | Gamma1: ['2.28542', '2.31153', '2.30322', '2.25710', '2.25435', '2.26719', '2.27789', '2.31878', '2.30605', '2.29020', '2.30112', '2.30428', '2.32320'] | Gamma1 Grad: ['-0.02280', '0.06138', '-0.00947', '-0.07443', '0.02194', '0.08423', '-0.02064', '0.03346', '0.01755', '0.02084', '0.00158', '-0.00615', '-0.00601']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.854% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.854% | ğŸ† Best Train Accuracy: 98.854%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.854% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.42it/s, Test_acc=67.3, Test_loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.290% | ğŸ† Best Test Accuracy: 67.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.46it/s, Train_acc=98.8, Train_loss=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00129 | Gamma1: ['2.29466', '2.28361', '2.30990', '2.24798', '2.21160', '2.27869', '2.26905', '2.29916', '2.30879', '2.29979', '2.30485', '2.31473', '2.32719'] | Gamma1 Grad: ['-0.02552', '0.06462', '-0.00864', '-0.02857', '-0.01251', '-0.05314', '-0.00061', '0.02510', '0.05018', '-0.00605', '0.00100', '-0.00596', '-0.01038']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.830% | ğŸ† Best Train Accuracy: 98.854%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.854% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 90 | TrainLossHist: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.13it/s, Test_acc=67.1, Test_loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.090% | ğŸ† Best Test Accuracy: 67.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.02it/s, Train_acc=98.9, Train_loss=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00127 | Gamma1: ['2.26341', '2.29067', '2.30478', '2.24816', '2.25026', '2.26753', '2.29813', '2.29731', '2.30773', '2.29633', '2.29986', '2.30366', '2.32987'] | Gamma1 Grad: ['0.02491', '-0.01386', '0.02777', '0.01030', '0.03438', '0.10066', '0.02387', '-0.05082', '0.01811', '-0.00276', '-0.00067', '-0.00547', '-0.00050']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.852% | ğŸ† Best Train Accuracy: 98.854%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.854% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.96it/s, Test_acc=67, Test_loss=2.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.050% | ğŸ† Best Test Accuracy: 67.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.70it/s, Train_acc=98.9, Train_loss=0.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00125 | Gamma1: ['2.27701', '2.30902', '2.29336', '2.24931', '2.24693', '2.25792', '2.29377', '2.30519', '2.29321', '2.29969', '2.30703', '2.30581', '2.31577'] | Gamma1 Grad: ['-0.01074', '-0.00848', '-0.00742', '0.02913', '-0.01376', '0.00170', '0.01421', '0.00218', '0.00184', '0.00001', '-0.00248', '-0.00535', '-0.01664']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.884% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.884% | ğŸ† Best Train Accuracy: 98.884%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.884% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.72it/s, Test_acc=67, Test_loss=2.18]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.010% | ğŸ† Best Test Accuracy: 67.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.91it/s, Train_acc=98.8, Train_loss=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00122 | Gamma1: ['2.29968', '2.30939', '2.29309', '2.24856', '2.23805', '2.24889', '2.31086', '2.30614', '2.28631', '2.29919', '2.30384', '2.29682', '2.30987'] | Gamma1 Grad: ['-0.05940', '0.01288', '0.01637', '0.02827', '0.02039', '-0.08048', '-0.08925', '-0.02171', '0.00927', '-0.00891', '0.01733', '0.00047', '0.11895']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.822% | ğŸ† Best Train Accuracy: 98.884%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.884% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.80it/s, Test_acc=67.1, Test_loss=2.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.090% | ğŸ† Best Test Accuracy: 67.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.05it/s, Train_acc=99, Train_loss=0.063]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00120 | Gamma1: ['2.31498', '2.29581', '2.28611', '2.26803', '2.22846', '2.26954', '2.29883', '2.31258', '2.30395', '2.29117', '2.31265', '2.29915', '2.32468'] | Gamma1 Grad: ['-0.00011', '0.00248', '-0.00183', '0.00440', '-0.00741', '0.00085', '-0.00049', '0.00105', '0.00093', '-0.00078', '0.00033', '-0.00022', '-0.00033']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.974% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.974% | ğŸ† Best Train Accuracy: 98.974%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.974% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.26it/s, Test_acc=67.2, Test_loss=2.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.150% | ğŸ† Best Test Accuracy: 67.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.27it/s, Train_acc=99, Train_loss=0.06]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00118 | Gamma1: ['2.29658', '2.29919', '2.27433', '2.24864', '2.21804', '2.24989', '2.31005', '2.31552', '2.32558', '2.31215', '2.31035', '2.30726', '2.33261'] | Gamma1 Grad: ['-0.01057', '-0.00074', '-0.00790', '-0.00454', '-0.02719', '-0.04356', '0.00144', '-0.03135', '-0.00195', '-0.00030', '0.00465', '-0.00352', '-0.00288']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.032% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.032% | ğŸ† Best Train Accuracy: 99.032%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.032% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.56it/s, Test_acc=67.4, Test_loss=2.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.390% | ğŸ† Best Test Accuracy: 67.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.58it/s, Train_acc=99.1, Train_loss=0.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00115 | Gamma1: ['2.29401', '2.30658', '2.28603', '2.26031', '2.22201', '2.26160', '2.29814', '2.32191', '2.31139', '2.29680', '2.30468', '2.29800', '2.33408'] | Gamma1 Grad: ['0.00800', '0.03259', '0.01507', '0.00678', '0.04002', '-0.01519', '0.02271', '0.04981', '-0.00760', '0.00461', '0.00709', '0.00732', '-0.00601']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.098% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.098% | ğŸ† Best Train Accuracy: 99.098%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.098% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.78it/s, Test_acc=67, Test_loss=2.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.000% | ğŸ† Best Test Accuracy: 67.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.04it/s, Train_acc=99, Train_loss=0.061]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00113 | Gamma1: ['2.30802', '2.29009', '2.28097', '2.25552', '2.21024', '2.25946', '2.27923', '2.31371', '2.30693', '2.30213', '2.29699', '2.28649', '2.32472'] | Gamma1 Grad: ['0.01033', '-0.05497', '0.03252', '-0.01815', '0.10218', '-0.01607', '-0.02928', '-0.02077', '-0.00618', '-0.00626', '0.01629', '-0.00736', '0.02121']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.998% | ğŸ† Best Train Accuracy: 99.098%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.098% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.75it/s, Test_acc=67.1, Test_loss=2.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.130% | ğŸ† Best Test Accuracy: 67.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.19it/s, Train_acc=99, Train_loss=0.061]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00110 | Gamma1: ['2.29579', '2.31968', '2.28726', '2.23876', '2.21363', '2.26329', '2.26617', '2.30441', '2.30555', '2.29507', '2.29881', '2.29145', '2.32902'] | Gamma1 Grad: ['-0.05535', '-0.00518', '-0.06397', '-0.02431', '0.02952', '0.03051', '0.04404', '0.01402', '0.01677', '0.00606', '0.00224', '-0.02253', '-0.01393']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.012% | ğŸ† Best Train Accuracy: 99.098%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.098% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 92.07it/s, Test_acc=67.6, Test_loss=2.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.590% | ğŸ† Best Test Accuracy: 67.590%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.80it/s, Train_acc=99.2, Train_loss=0.058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00108 | Gamma1: ['2.28163', '2.29431', '2.26810', '2.25900', '2.22667', '2.26527', '2.28022', '2.32425', '2.30274', '2.29193', '2.28731', '2.30355', '2.31691'] | Gamma1 Grad: ['0.00409', '0.00495', '0.02214', '-0.08894', '-0.05364', '0.02871', '0.00396', '0.01690', '0.02439', '0.00102', '-0.00337', '-0.00291', '-0.02741']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.196% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.196% | ğŸ† Best Train Accuracy: 99.196%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.196% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.07it/s, Test_acc=67.1, Test_loss=2.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.110% | ğŸ† Best Test Accuracy: 67.590%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\SinPertubation-No_SinPertubation\\Results\\CIFAR100_Test_no_sinPertubation_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "Best Test Accuracy:  67.59\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 11. TRAIN MODEL WITH SHEDULAR | XXX ----------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Set Seed for Reproducibility BEFORE training starts\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)  \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "# âœ… Training Loop\n",
    "num_epochs = 100 # Example: Set the total number of epochs\n",
    "for epoch in range(start_epoch, num_epochs):   # Runs training for 100 epochs\n",
    "\n",
    "    train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler, WARMUP_ACTIVATION_EPOCHS) # âœ… Pass required arguments\n",
    "\n",
    "    test(epoch)  # âœ… Test the model\n",
    "    tqdm.write(\"\")  # âœ… Clear leftover progress bar from test()\n",
    "\n",
    "\n",
    "print(\"Best Test Accuracy: \", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc93b3-4473-4cea-944f-8ac44d4eddae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'âœ… Annotated comparison plots with BEST accuracy markers saved to ./Results/Plots'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 550x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 13. SINUSOIDAL PERTUBATION VS NO SINUSOIDAL PERTUBATION | XXX --------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.patheffects as path_effects\n",
    "import os\n",
    "\n",
    "def read_test_log(file_path):\n",
    "    test_loss_history = []\n",
    "    test_acc_history = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if \"Test Loss\" in line and \"Test Acc\" in line:\n",
    "                try:\n",
    "                    loss = float(line.split(\"Test Loss:\")[1].split(\"|\")[0].strip())\n",
    "                    acc = float(line.split(\"Test Acc:\")[1].split(\"%\")[0].strip())\n",
    "                    test_loss_history.append(loss)\n",
    "                    test_acc_history.append(acc)\n",
    "                except:\n",
    "                    continue\n",
    "    return test_loss_history, test_acc_history\n",
    "\n",
    "def plot_train_test_metrics(save_dir=\"./Results/Plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    SinPertubation_test_log_path = f'./Results_SinPertubation/CIFAR100_Test_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "    noSinPertubation_test_log_path = f'./Results/CIFAR100_Test_{sinPertubation_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "\n",
    "    SinPertubation_test_loss, SinPertubation_test_acc = read_test_log(SinPertubation_test_log_path)\n",
    "    noSinPertubation_test_loss, noSinPertubation_test_acc = read_test_log(noSinPertubation_test_log_path)\n",
    "\n",
    "    num_epochs = min(len(SinPertubation_test_loss), len(noSinPertubation_test_loss))\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    COLOR_SCALE = ['#00295B', '#CF0A66']  # SinPertubation, noSinPertubation\n",
    "    rcParams.update({\n",
    "        \"font.size\": 11,\n",
    "        \"axes.titlesize\": 11,\n",
    "        \"axes.labelsize\": 13,\n",
    "        \"xtick.labelsize\": 11,\n",
    "        \"ytick.labelsize\": 11,\n",
    "        \"axes.labelweight\": \"bold\",\n",
    "        \"xtick.color\": \"black\",\n",
    "        \"ytick.color\": \"black\",\n",
    "    })\n",
    "\n",
    "    # Custom settings\n",
    "    custom_yticks_test_loss = [1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
    "    custom_yticks_test_acc = [10, 20, 30, 40, 50, 60, 70]\n",
    "    custom_xticks = [0, 20, 40, 60, 80, 100]\n",
    "    custom_yaxis_test_loss = [1.2, 4.2]\n",
    "    custom_yaxis_test_acc = [35, 72]\n",
    "    custom_xaxis = [0, 105]\n",
    "\n",
    "    # Offsets\n",
    "    y_offset_loss_tp = 0.2\n",
    "    y_offset_loss_ntp = 0.07\n",
    "    x_offset_loss_tp = 3.5\n",
    "    x_offset_loss_ntp = 3.5\n",
    "\n",
    "    y_offset_acc_tp = 1\n",
    "    y_offset_acc_ntp = 3.2\n",
    "    x_offset_acc_tp = 8.5\n",
    "    x_offset_acc_ntp = 6.5\n",
    "\n",
    "    # ğŸ”· Plot Test Loss\n",
    "    fig1, ax1 = plt.subplots(figsize=(5.5, 3.5))\n",
    "    ax1.plot(epochs, SinPertubation_test_loss[:num_epochs], label=\"Sinusoidal Perturbation\", color=COLOR_SCALE[0], linewidth=2)\n",
    "    ax1.plot(epochs, noSinPertubation_test_loss[:num_epochs], label=\"No Sinusoidal Perturbation\", color=COLOR_SCALE[1], linestyle='--', linewidth=2)\n",
    "    ax1.set_xlabel(\"Epoch\", fontweight='bold')\n",
    "    ax1.set_ylabel(\"Test Loss\", fontweight='bold')\n",
    "    ax1.set_xticks(custom_xticks)\n",
    "    ax1.set_yticks(custom_yticks_test_loss)\n",
    "    ax1.set_xlim(custom_xaxis)\n",
    "    ax1.set_ylim(custom_yaxis_test_loss)\n",
    "    ax1.tick_params(axis='x', width=1.5)\n",
    "    ax1.tick_params(axis='y', width=1.5)\n",
    "    for label in ax1.get_xticklabels() + ax1.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    leg1 = ax1.legend(fontsize='small', loc=\"upper right\")\n",
    "    for text in leg1.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Add final loss markers\n",
    "    ax1.plot(epochs[-1], SinPertubation_test_loss[-1], marker='o', color=COLOR_SCALE[0], markersize=4)\n",
    "    ax1.text(epochs[-1] - x_offset_loss_tp, SinPertubation_test_loss[-1] - y_offset_loss_tp,\n",
    "             f\"{SinPertubation_test_loss[-1]:.2f}\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "    ax1.plot(epochs[-1], noSinPertubation_test_loss[-1], marker='o', color=COLOR_SCALE[1], markersize=4)\n",
    "    ax1.text(epochs[-1] - x_offset_loss_ntp, noSinPertubation_test_loss[-1] + y_offset_loss_ntp,\n",
    "             f\"{noSinPertubation_test_loss[-1]:.2f}\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"compare_test_loss_sinPertubation_vs_no_sinPertubation.svg\"),\n",
    "                format='svg', transparent=True, bbox_inches='tight')\n",
    "    plt.close(fig1)\n",
    "\n",
    "    # ğŸ”¶ Plot Test Accuracy\n",
    "    best_epoch_tp = SinPertubation_test_acc.index(max(SinPertubation_test_acc)) + 1\n",
    "    best_acc_tp = max(SinPertubation_test_acc)\n",
    "    best_epoch_ntp = noSinPertubation_test_acc.index(max(noSinPertubation_test_acc)) + 1\n",
    "    best_acc_ntp = max(noSinPertubation_test_acc)\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(5.5, 3.5))\n",
    "    ax2.plot(epochs, SinPertubation_test_acc[:num_epochs], label=\"Sinusoidal Perturbation\", color=COLOR_SCALE[0], linewidth=2)\n",
    "    ax2.plot(epochs, noSinPertubation_test_acc[:num_epochs], label=\"No Sinusoidal Perturbation\", color=COLOR_SCALE[1], linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel(\"Epoch\", fontweight='bold')\n",
    "    ax2.set_ylabel(\"Test Accuracy (%)\", fontweight='bold')\n",
    "    ax2.set_xticks(custom_xticks)\n",
    "    ax2.set_yticks(custom_yticks_test_acc)\n",
    "    ax2.set_xlim(custom_xaxis)\n",
    "    ax2.set_ylim(custom_yaxis_test_acc)\n",
    "    ax2.tick_params(axis='x', width=1.5)\n",
    "    ax2.tick_params(axis='y', width=1.5)\n",
    "    for label in ax2.get_xticklabels() + ax2.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    leg2 = ax2.legend(fontsize='small', loc=\"lower right\")\n",
    "    for text in leg2.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Markers for best accuracy\n",
    "    ax2.plot(best_epoch_tp, best_acc_tp - 0.21, marker='o', color=COLOR_SCALE[0], markersize=5.5, markeredgecolor='black', markeredgewidth=1)\n",
    "    ax2.text(best_epoch_tp - x_offset_acc_tp, best_acc_tp + y_offset_acc_tp,\n",
    "             f\"{best_acc_tp:.2f}%\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "    ax2.plot(best_epoch_ntp, best_acc_ntp - 0.4, marker='o', color=COLOR_SCALE[1], markersize=5.5, markeredgecolor='black', markeredgewidth=1)\n",
    "    ax2.text(best_epoch_ntp - x_offset_acc_ntp, best_acc_ntp - y_offset_acc_ntp,\n",
    "             f\"{best_acc_ntp:.2f}%\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"compare_test_accuracy_sinPertubation_vs_no_sinPertubation.svg\"),\n",
    "                format='svg', transparent=True, bbox_inches='tight')\n",
    "    # plt.close(fig2)\n",
    "\n",
    "    return f\"âœ… Annotated comparison plots with BEST accuracy markers saved to {save_dir}\"\n",
    "\n",
    "# ğŸ”· Call the function\n",
    "plot_train_test_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
