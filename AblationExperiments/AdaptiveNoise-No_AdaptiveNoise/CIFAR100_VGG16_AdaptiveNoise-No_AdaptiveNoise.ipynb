{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37c700-5c67-4c44-bd57-251f092c340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Current working directory: C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\n",
      "âœ… sys.path updated:\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\python310.zip\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\DLLs\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\n",
      "   ğŸ“‚ \n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\Pythonwin\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\models\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\activation\n",
      "âœ… FFTGate imported successfully!\n",
      "âœ… FFTGate instance created successfully!\n",
      "âœ… FFTGate_VGG imported successfully!\n",
      "CIFAR100 Training Script Initialized...\n",
      "Using device: cuda\n",
      "Parsed learning rate: 0.001 (type: <class 'float'>)\n",
      "Formatted learning rate for filenames: 0_001\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Length of training dataset: 50000\n",
      "Length of testing dataset: 10000\n",
      "Number of classes in CIFAR-100: 100\n",
      "==> Building model..\n",
      "âœ… Found 13 FFTGate layers.\n",
      "âœ… Collected 13 trainable activation parameters.\n",
      "   ğŸ”¹ Layer 0: FFTGate()\n",
      "   ğŸ”¹ Layer 1: FFTGate()\n",
      "   ğŸ”¹ Layer 2: FFTGate()\n",
      "   ğŸ”¹ Layer 3: FFTGate()\n",
      "   ğŸ”¹ Layer 4: FFTGate()\n",
      "   ğŸ”¹ Layer 5: FFTGate()\n",
      "   ğŸ”¹ Layer 6: FFTGate()\n",
      "   ğŸ”¹ Layer 7: FFTGate()\n",
      "   ğŸ”¹ Layer 8: FFTGate()\n",
      "   ğŸ”¹ Layer 9: FFTGate()\n",
      "   ğŸ”¹ Layer 10: FFTGate()\n",
      "   ğŸ”¹ Layer 11: FFTGate()\n",
      "   ğŸ”¹ Layer 12: FFTGate()\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 1.A. IMPORTS LIBRARIES | XXX -----------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\"\"\"Train CIFAR100 with PyTorch.\"\"\"\n",
    "\n",
    "# Python 2/3 compatibility\n",
    "# from __future__ import print_function\n",
    "\n",
    "\n",
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# PyTorch and related modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# torchvision for datasets and transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch_optimizer as torch_opt  # Use 'torch_opt' for torch_optimizer\n",
    "from timm.scheduler import CosineLRScheduler \n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Define currect working directory to ensure on right directory\n",
    "VGG16_PATH = r\"C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\"\n",
    "if os.getcwd() != VGG16_PATH:\n",
    "    os.chdir(VGG16_PATH)\n",
    "print(f\"âœ… Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# âœ… Define absolute paths\n",
    "PROJECT_PATH = VGG16_PATH\n",
    "MODELS_PATH = os.path.join(VGG16_PATH, \"models\")\n",
    "ACTIVATION_PATH = os.path.join(VGG16_PATH, \"activation\")\n",
    "# PAU_PATH = os.path.join(VGG16_PATH, \"pau\")\n",
    "\n",
    "# âœ… Ensure necessary paths are in sys.path\n",
    "for path in [PROJECT_PATH, MODELS_PATH, ACTIVATION_PATH]:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "# âœ… Print updated sys.path for debugging\n",
    "print(\"âœ… sys.path updated:\")\n",
    "for path in sys.path:\n",
    "    print(\"   ğŸ“‚\", path)\n",
    "\n",
    "# âœ… Import FFTGate (Check if the module exists)\n",
    "try:\n",
    "    from activation.FFTGate import FFTGate  # type: ignore\n",
    "    print(\"âœ… FFTGate imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ Import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'Activation4.py' exists inside: {ACTIVATION_PATH}\")\n",
    "\n",
    "# âœ… Test if FFTGate is callable\n",
    "try:\n",
    "    activation_test = FFTGate()\n",
    "    print(\"âœ… FFTGate instance created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error while initializing FFTGate: {e}\")\n",
    "\n",
    "# âœ… Now import FFTGate_VGG (Ensure module exists inside models/)\n",
    "try:\n",
    "    from models.FFTGate_VGG import FFTGate_VGG  # type: ignore\n",
    "    print(\"âœ… FFTGate_VGG imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ FFTGate_VGG import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'FFTGate_VGG.py' exists inside: {MODELS_PATH}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 1.B. SEEDING FOR REPRODUCIBILITY | XXX -------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def set_seed_torch(seed):\n",
    "    torch.manual_seed(seed)                          \n",
    "\n",
    "\n",
    "\n",
    "def set_seed_main(seed):\n",
    "    random.seed(seed)                                ## Python's random module\n",
    "    np.random.seed(seed)                             ## NumPy's random module\n",
    "    torch.cuda.manual_seed(seed)                     ## PyTorch's random module for CUDA\n",
    "    torch.cuda.manual_seed_all(seed)                 ## Seed for all CUDA devices\n",
    "    torch.backends.cudnn.deterministic = True        ## Ensure deterministic behavior for CuDNN\n",
    "    torch.backends.cudnn.benchmark = False           ## Disable CuDNN's autotuning for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)   \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (Optional) Import Optimizers - Uncomment as needed\n",
    "# from Opt import opt\n",
    "# from diffGrad import diffGrad\n",
    "# from diffRGrad import diffRGrad, SdiffRGrad, BetaDiffRGrad, Beta12DiffRGrad, BetaDFCDiffRGrad\n",
    "# from RADAM import Radam, BetaRadam\n",
    "# from BetaAdam import BetaAdam, BetaAdam1, BetaAdam2, BetaAdam3, BetaAdam4, BetaAdam5, BetaAdam6, BetaAdam7, BetaAdam4A\n",
    "# from AdamRM import AdamRM, AdamRM1, AdamRM2, AdamRM3, AdamRM4, AdamRM5\n",
    "# from sadam import sadam\n",
    "# from SdiffGrad import SdiffGrad\n",
    "# from SRADAM import SRADAM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 2. DEFINE MODEL Lr | XXX ---------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Main Execution (Placeholder)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"CIFAR100 Training Script Initialized...\")\n",
    "    # Add your training pipeline here\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Argument parser to get user inputs\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR100 Training')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "\n",
    "args, unknown = parser.parse_known_args()  # Avoids Jupyter argument issues\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Ensure lr is correctly parsed\n",
    "lr = args.lr  # Get learning rate from argparse\n",
    "lr_str = str(lr).replace('.', '_')  # Convert to string and replace '.' for filenames\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Parsed learning rate: {lr} (type: {type(lr)})\")\n",
    "print(f\"Formatted learning rate for filenames: {lr_str}\")\n",
    "\n",
    "# Initialize training variables\n",
    "best_acc = 0  # Best test accuracy\n",
    "start_epoch = 0  # Start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 3. LOAD DATASET | XXX ------------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "bs = 64 #set batch size\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Length of train and test datasets\n",
    "len_train = len(trainset)\n",
    "len_test = len(testset)\n",
    "print(f\"Length of training dataset: {len_train}\")\n",
    "print(f\"Length of testing dataset: {len_test}\")\n",
    "\n",
    "# âœ… Print number of classes\n",
    "num_classes_Print = len(trainset.classes)\n",
    "print(f\"Number of classes in CIFAR-100: {num_classes_Print}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 4. DYNAMIC REGULARIZATION| XXX ---------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def apply_dynamic_regularization(inputs, feature_activations, epoch,\n",
    "                                  prev_params, layer_index_map, batch_idx):\n",
    "\n",
    "\n",
    "    global activation_layers  # âœ… Reference already-collected layers\n",
    "\n",
    "    # âœ… Print gamma1 stats early in training for monitoring\n",
    "    if batch_idx == 0 and epoch <= 4:\n",
    "        print(f\"\\nğŸš¨ ENTERED apply_dynamic_regularization | Epoch={epoch} | Batch={batch_idx}\", flush=True)\n",
    "\n",
    "        # ğŸ§  Print gamma1 details\n",
    "        all_layer_info = []\n",
    "        for idx, layer in enumerate(activation_layers):\n",
    "            param = getattr(layer, \"gamma1\")\n",
    "            all_layer_info.append(f\"Layer {idx}: ID={id(param)} | Mean={param.mean().item():.5f}\")\n",
    "        print(\"ğŸ§  GAMMA1 INFO:\", \" | \".join(all_layer_info), flush=True)\n",
    "\n",
    "    # âœ… Initialize gamma1 regularization accumulator\n",
    "    gamma1_reg = 0.0\n",
    "\n",
    "    # âœ… Compute batch std and define regularization strength\n",
    "    batch_std = torch.std(inputs) + 1e-6\n",
    "    regularization_strength = 0.05 if epoch < 40 else (0.01 if epoch < 60 else 0.005)\n",
    "\n",
    "    # âœ… Track layers where noise is injected (informative)\n",
    "    noisy_layers = []\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        if idx not in layer_index_map:\n",
    "            continue\n",
    "\n",
    "        prev_layer_params = prev_params[layer_index_map[idx]]\n",
    "        param_name = \"gamma1\"\n",
    "        param = getattr(layer, param_name)\n",
    "        prev_param = prev_layer_params[param_name]\n",
    "\n",
    "        # âœ… Target based on input stats\n",
    "        target = compute_target(param_name, batch_std)\n",
    "\n",
    "        # âœ… Adaptive Target Regularization\n",
    "        gamma1_reg += regularization_strength * (param - target).pow(2).mean() * 1.2\n",
    "\n",
    "        # âœ… Adaptive Cohesion Regularization\n",
    "        cohesion = (param - prev_param).pow(2)\n",
    "        gamma1_reg += 0.005 * cohesion.mean()\n",
    "\n",
    "    #     # âœ… Adaptive Noise Regularization\n",
    "    #     epoch_AddNoise = 50\n",
    "    #     if epoch > epoch_AddNoise:\n",
    "    #         param_variation = torch.abs(param - prev_param).mean()\n",
    "    #         if param_variation < 0.015:\n",
    "    #             noise = (0.001 + 0.0004 * batch_std.item()) * torch.randn_like(param)\n",
    "    #             penalty = (param - (prev_param + noise)).pow(2).sum()\n",
    "    #             gamma1_reg += 0.00015 * penalty\n",
    "    #             noisy_layers.append(f\"{idx} (Î”={param_variation.item():.5f})\") # Collect index and variation\n",
    "\n",
    "    # # âœ… Print noise injection summary\n",
    "    # if batch_idx == 0 and epoch <= (epoch_AddNoise + 4) and noisy_layers:\n",
    "    #     print(f\"ğŸ”¥ Stable Noise Injected | Epoch {epoch} | Batch {batch_idx} | Layers: \" + \", \".join(noisy_layers), flush=True)\n",
    "    mags = feature_activations.abs().mean(dim=(0, 2, 3))\n",
    "    m = mags / mags.sum()\n",
    "    gamma1_reg += 0.005 * (-(m * torch.log(m + 1e-6)).sum())\n",
    "\n",
    "    return gamma1_reg\n",
    "\n",
    "\n",
    "def compute_target(param_name, batch_std):\n",
    "    if param_name == \"gamma1\":\n",
    "        return 2.0 + 0.2 * batch_std.item()  \n",
    "\n",
    "    raise ValueError(f\"Unknown param {param_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 5. INITIALIZE MODEL | XXX --------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "#net = Elliott_VGG('VGG16'); net1 = 'Elliott_VGG16'\n",
    "#net = GELU_MobileNet(); net1 = 'GELU_MobileNet'\n",
    "#net = GELU_SENet18(); net1 = 'GELU_SENet18'\n",
    "#net = PDELU_ResNet50(); net1 = 'PDELU_ResNet50'\n",
    "# net = Sigmoid_GoogLeNet(); net1 = 'Sigmoid_GoogLeNet'\n",
    "#net = GELU_DenseNet121(); net1 = 'GELU_DenseNet121'\n",
    "# net = ReLU_VGG('VGG16'); net1 = 'ReLU_VGG16'\n",
    "net = FFTGate_VGG('VGG16'); net1 = 'FFTGate_VGG16'\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9); optimizer1 = 'SGDM5'\n",
    "#optimizer = optim.Adagrad(net.parameters()); optimizer1 = 'AdaGrad'\n",
    "#optimizer = optim.Adadelta(net.parameters()); optimizer1 = 'AdaDelta'\n",
    "#optimizer = optim.RMSprop(net.parameters()); optimizer1 = 'RMSprop'\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr); optimizer1 = 'Adam'\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr, amsgrad=True); optimizer1 = 'amsgrad'\n",
    "#optimizer = diffGrad(net.parameters(), lr=args.lr); optimizer1 = 'diffGrad'\n",
    "#optimizer = Radam(net.parameters(), lr=args.lr); optimizer1 = 'Radam'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 6. INITIALIZE ACTIVATION PARAMETERS, OPTIMIZERS & SCHEDULERS | XXX ---------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# âœ… Step 1: Collect Activation Parameters from ALL Layers (Ensure Compatibility with DataParallel)\n",
    "if isinstance(net, torch.nn.DataParallel):\n",
    "    features = net.module.features\n",
    "else:\n",
    "    features = net.features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 2: Recursively search for FFTGate layers\n",
    "activation_params = []\n",
    "activation_layers = []\n",
    "\n",
    "for layer in features:\n",
    "    if isinstance(layer, FFTGate):  \n",
    "        activation_layers.append(layer)\n",
    "        activation_params.append(layer.gamma1)  # âœ… Only gamma1 is trainable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 3: Define Unfreeze Epoch\n",
    "unfreeze_activation_epoch = 1  # âœ… Change this value if needed\n",
    "# unfreeze_activation_epoch = 10  # âœ… Delay unfreezing until epoch 10\n",
    "\n",
    "\n",
    "# âœ… Define the warm-up epoch value\n",
    "# WARMUP_ACTIVATION_EPOCHS = 5  # The number of epochs for warm-up\n",
    "WARMUP_ACTIVATION_EPOCHS = 0  # The number of epochs for warm-up\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initially Freeze Activation Parameters\n",
    "for param in activation_params:\n",
    "    param.requires_grad = False  # ğŸš« Keep frozen before the unfreeze epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initialize Activation Optimizers (Using AdamW for Better Weight Decay)\n",
    "activation_optimizers = {\n",
    "    \"gamma1\": torch.optim.AdamW(activation_params, lr=0.0015, weight_decay=1e-6)  # ğŸ”º Reduce LR from 0.005 â†’ 0.0025\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 5: Initialize Activation Schedulers with Warm Restarts (Per Parameter Type)\n",
    "activation_schedulers = {\n",
    "    \"gamma1\": CosineAnnealingWarmRestarts(\n",
    "        activation_optimizers[\"gamma1\"],\n",
    "        T_0=10,      # Shorter cycle to explore aggressively\n",
    "        T_mult=2,    # Increase cycle length gradually\n",
    "        eta_min=5e-5  # âœ… recommended safer modification\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 6: Print collected activation layers and parameters\n",
    "if activation_layers and activation_params:\n",
    "    print(f\"âœ… Found {len(activation_layers)} FFTGate layers.\")\n",
    "    print(f\"âœ… Collected {len(activation_params)} trainable activation parameters.\")\n",
    "    \n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        print(f\"   ğŸ”¹ Layer {idx}: {layer}\")\n",
    "\n",
    "elif activation_layers and not activation_params:\n",
    "    print(f\"âš  Warning: Found {len(activation_layers)} FFTGate layers, but no trainable parameters were collected.\")\n",
    "\n",
    "elif activation_params and not activation_layers:\n",
    "    print(f\"âš  Warning: Collected {len(activation_params)} activation parameters, but no FFTGate layers were recorded.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš  Warning: No FFTGate layers or activation parameters found! Skipping activation optimizer.\")\n",
    "    activation_optimizers = None\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 7. INITIALIZE MAIN OPTIMIZER SCHEDULER | XXX -------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Step 6: Define MultiStepLR for Main Optimizer\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "main_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 8. MODEL CHECK POINT | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists('checkpoint'):\n",
    "    os.makedirs('checkpoint')\n",
    "\n",
    "if not os.path.exists('Results'):\n",
    "    os.makedirs('Results')\n",
    "\n",
    "# Construct checkpoint path\n",
    "checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr}_{net1}_{optimizer1}.t7'\n",
    "\n",
    "# Resume checkpoint only if file exists\n",
    "if args.resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 9. DEFINE TRAIN LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Used for naming files \n",
    "noise_mode = \"no_noise\"  # Options: \"no_noise\", \"noise\"\n",
    "\n",
    "# Training\n",
    "\n",
    "def train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler , WARMUP_ACTIVATION_EPOCHS):\n",
    "    global train_loss_history, best_train_acc, prev_params, recent_test_acc, gamma1_history, activation_layers, test_acc_history, train_acc_history, noise_mode  # ğŸŸ¢ğŸŸ¢ğŸŸ¢\n",
    "\n",
    "    if epoch == 0:\n",
    "        train_loss_history = []\n",
    "        train_acc_history = []\n",
    "        best_train_acc = 0.0\n",
    "        recent_test_acc = 0.0\n",
    "        gamma1_history = {}         # âœ… Initialize history\n",
    "        test_acc_history = []       # âœ… test accuracy history\n",
    "\n",
    "\n",
    "\n",
    "    prev_params = {}\n",
    "    layer_index_map = {idx: idx for idx in range(len(activation_layers))}  \n",
    "\n",
    "    # âœ… Cache previous gamma1 values from activation layers\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_accuracy = 0.0\n",
    "\n",
    "    # âœ… Initialize log history\n",
    "    log_history = []\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Define path to store Training log\n",
    "    save_paths = {\n",
    "       \n",
    "        \"log_history\": f\"C:\\\\Users\\\\emeka\\\\Research\\\\ModelCUDA\\\\Big_Data_Journal\\\\Comparison\\\\Code\\\\Paper\\\\github2\\\\AblationExperiments\\\\AdaptiveNoise-No_AdaptiveNoise\\\\Results\\\\FFTGate\\\\FFTGate_training_logs.txt\"  # âœ… Training log_history \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 1: Unfreeze Activation Parameters (Only Once Per Epoch)\n",
    "    if epoch == unfreeze_activation_epoch:\n",
    "        print(\"\\nğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\")\n",
    "        for layer in net.module.features if isinstance(net, torch.nn.DataParallel) else net.features:\n",
    "            if isinstance(layer, FFTGate):   \n",
    "                layer.gamma1.requires_grad = True  # âœ… Only gamma1 is trainable\n",
    "        print(\"âœ… Activation Parameters Unfrozen! ğŸš€\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 2: Gradual Warm-up for Activation Learning Rates (AFTER Unfreezing)\n",
    "    warmup_start = unfreeze_activation_epoch  # ğŸ”¹ Start warm-up when unfreezing happens\n",
    "    warmup_end = unfreeze_activation_epoch + WARMUP_ACTIVATION_EPOCHS  # ğŸ”¹ End warm-up period\n",
    "\n",
    "    # âœ… Adjust learning rates **only** during the warm-up phase\n",
    "    if warmup_start <= epoch < warmup_end:\n",
    "        warmup_factor = (epoch - warmup_start + 1) / WARMUP_ACTIVATION_EPOCHS  \n",
    "\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            for param_group in act_scheduler.optimizer.param_groups:\n",
    "                if \"initial_lr\" not in param_group:\n",
    "                    param_group[\"initial_lr\"] = param_group[\"lr\"]  # ğŸ”¹ Store initial LR\n",
    "                param_group[\"lr\"] = param_group[\"initial_lr\"] * warmup_factor  # ğŸ”¹ Scale LR\n",
    "\n",
    "        # âœ… Debugging output to track warm-up process\n",
    "        print(f\"ğŸ”¥ Warm-up Epoch {epoch}: Scaling LR by {warmup_factor:.3f}\")\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            print(f\"  ğŸ”¹ {name} LR: {act_scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    activation_history = []  # ğŸ”´ Initialize empty history at start of epoch (outside batch loop)\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Training Loop\n",
    "    with tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch}\") as progress:\n",
    "        for batch_idx, (inputs, targets) in progress:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # zero_grad activation parameter\n",
    "            for opt in activation_optimizers.values():\n",
    "                opt.zero_grad()\n",
    "\n",
    "\n",
    "            # âœ… Forward Pass\n",
    "            outputs = net(inputs, epoch=epoch, train_accuracy=train_accuracy, targets=targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            \n",
    "            feature_activations = features(inputs)  # Feature activations\n",
    "\n",
    "\n",
    "            # âœ… Collect Activation History | âœ… Per-layer mean activations\n",
    "            batch_means = [layer.saved_output.mean().item() for layer in activation_layers]\n",
    "            activation_history.extend(batch_means)\n",
    "\n",
    "            # âœ… Apply Decay strategy to history for each activation layer\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    if isinstance(layer, FFTGate):\n",
    "                        layer.decay_spectral_history(epoch, num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Compute Training Accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            train_accuracy = 100. * correct / total if total > 0 else 0.0  # Compute training accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Call Regularization Function for the Activation Parameter\n",
    "            if epoch > 0:\n",
    "                gamma1_reg = apply_dynamic_regularization(\n",
    "                    inputs, feature_activations, epoch,\n",
    "                    prev_params, layer_index_map, batch_idx\n",
    "                )\n",
    "                loss += gamma1_reg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… ğŸ¯ Adaptive Gradient Clipping of gamma1  \n",
    "            for layer in features:\n",
    "                if isinstance(layer, FFTGate):  # âœ… Ensure layer has gamma1 before clipping\n",
    "                    torch.nn.utils.clip_grad_norm_([layer.gamma1], max_norm=0.7)\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Apply Optimizer Step for Model Parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # âœ… Apply Optimizer Steps for Activation Parameters (Only if Unfrozen)\n",
    "            if epoch >= unfreeze_activation_epoch:\n",
    "                for opt in activation_optimizers.values():\n",
    "                    opt.step()\n",
    "\n",
    "\n",
    "            # âœ… Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Clamping of gamma1 (Applied AFTER Optimizer Step)\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    layer.gamma1.clamp_(0.1, 6.0)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Update progress bar\n",
    "            progress.set_postfix(Train_loss=round(train_loss / (batch_idx + 1), 3),\n",
    "                                 Train_acc=train_accuracy)  \n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step the main optimizer scheduler (ONLY for model parameters)\n",
    "    main_scheduler.step()\n",
    "\n",
    "    # âœ… Step the activation parameter schedulers (ONLY for activation parameters) | Epoch-wise stepping\n",
    "    if epoch >= unfreeze_activation_epoch:\n",
    "        for name, act_scheduler in activation_schedulers.items():  \n",
    "            act_scheduler.step()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… ONLY update prev_params here AFTER all updates | âœ… Update prev_params AFTER training epoch\n",
    "    for idx, layer in enumerate(activation_layers):      \n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Logging Activation Parameters & Gradients\n",
    "    last_batch_grads = {\"Gamma1 Grad\": []}\n",
    "    current_params = {\"Gamma1\": []}\n",
    "\n",
    "    for layer in features:\n",
    "        if isinstance(layer, FFTGate):  \n",
    "            # âœ… Convert gradients to scalar floats and format to 5 decimal places (removes device='cuda:0' and tensor(...))\n",
    "            last_batch_grads[\"Gamma1 Grad\"].append(f\"{layer.gamma1.grad.item():.5f}\" if layer.gamma1.grad is not None else \"None\")\n",
    "\n",
    "            # âœ… Collect current parameter values (already scalar), formatted to 5 decimal places\n",
    "            current_params[\"Gamma1\"].append(f\"{layer.gamma1.item():.5f}\")\n",
    "\n",
    "    # âœ… Build log message (showing params and gradients for ALL layers)\n",
    "    log_msg = (\n",
    "        f\"Epoch {epoch}: M_Optimizer LR => {optimizer.param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1 LR => {activation_optimizers['gamma1'].param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1: {current_params['Gamma1']} | \"\n",
    "        f\"Gamma1 Grad: {last_batch_grads['Gamma1 Grad']}\"\n",
    "    )\n",
    "\n",
    "    log_history.append(log_msg)\n",
    "    print(log_msg)  # âœ… Prints only once per epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (Clear old logs)\n",
    "    if epoch == 0:  # âœ… Only clear at the start of training\n",
    "        with open(save_paths[\"log_history\"], \"w\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Save logs once per epoch (Append new logs)\n",
    "    if log_history:\n",
    "        with open(save_paths[\"log_history\"], \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_history) + \"\\n\")         # âœ… Ensure each entry is on a new line\n",
    "        print(f\"ğŸ“œ Logs saved to {save_paths['log_history']}!\")  # âœ… Only prints once per epoch\n",
    "    else:\n",
    "        print(\"âš  No logs to save!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final training accuracy for the epoch\n",
    "    final_train_loss = train_loss / len(trainloader)\n",
    "    final_train_acc = 100. * correct / total\n",
    "\n",
    "    # âœ… Append to history\n",
    "    train_loss_history.append(final_train_loss)\n",
    "\n",
    "    # Append per-epoch training accuracy\n",
    "    train_acc_history.append(final_train_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save training results (without affecting best accuracy tracking)\n",
    "    train_results_path = f'./Results/CIFAR100_Train_{noise_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "    # âœ… Clear the log file at the start of training (Epoch 0)\n",
    "    if epoch == 0 and os.path.exists(train_results_path):\n",
    "        with open(train_results_path, 'w') as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs only once\n",
    "\n",
    "    # âœ… Append new training results for each epoch\n",
    "    with open(train_results_path, 'a') as f:\n",
    "        f.write(f\"Epoch {epoch} | Train Loss: {final_train_loss:.3f} | Train Acc: {final_train_acc:.3f}%\\n\")\n",
    "\n",
    "    if final_train_acc > best_train_acc:\n",
    "        best_train_acc = final_train_acc  # âœ… Update best training accuracy\n",
    "        print(f\"ğŸ† New Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "    # âœ… Append the best training accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):  # Only log once at the final epoch\n",
    "        with open(train_results_path, 'a') as f:\n",
    "            f.write(f\"\\nğŸ† Best Training Accuracy: {best_train_acc:.3f}%\\n\")  \n",
    "\n",
    "    # âœ… Print both Final and Best Training Accuracy\n",
    "    print(f\"ğŸ“Š Train Accuracy: {final_train_acc:.3f}% | ğŸ† Best Train Accuracy: {best_train_acc:.3f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"ğŸ“œ Training logs saved to {train_results_path}!\")\n",
    "    print(f\"ğŸ† Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"ğŸ“ Sizes â†’ ActivationHist: {len(activation_history)} | TestAccHist: {len(test_acc_history)} | TrainLossHist: {len(train_loss_history)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # return final_train_loss, final_train_acc, feature_activations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 10. DEFINE TEST LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(epoch, save_results=True):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and optionally saves the results.\n",
    "    \n",
    "    Args:\n",
    "    - epoch (int): The current epoch number.\n",
    "    - save_results (bool): Whether to save results to a file.\n",
    "\n",
    "    Returns:\n",
    "    - acc (float): Test accuracy percentage.\n",
    "    \"\"\"\n",
    "    global best_acc, val_accuracy, noise_mode  \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # âœ… Ensure activation function parameters are clamped before evaluation\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(testloader), total=len(testloader), desc=f\"Testing Epoch {epoch}\") as progress:\n",
    "            for batch_idx, (inputs, targets) in progress:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                # âœ… Pass validation accuracy to activation function\n",
    "                val_accuracy = 100. * correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "                # âœ… Update progress bar with loss & accuracy\n",
    "                progress.set_postfix(Test_loss=round(test_loss / (batch_idx + 1), 3),\n",
    "                                     Test_acc=round(val_accuracy, 3))\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final test accuracy\n",
    "    final_test_loss = test_loss / len(testloader)\n",
    "    final_test_acc = 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Ensure \"Results\" folder exists (just like training logs)\n",
    "    results_dir = os.path.join(PROJECT_PATH, \"Results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # âœ… Define log file path for test results\n",
    "    test_results_path = os.path.join(results_dir, f'CIFAR100_Test_{noise_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt')\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (clear old logs)\n",
    "    if epoch == 0:\n",
    "        with open(test_results_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Append new test results for each epoch (same style as training)\n",
    "    with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Epoch {epoch} | Test Loss: {final_test_loss:.3f} | Test Acc: {final_test_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save checkpoint if accuracy improves (does NOT interfere with logging)\n",
    "    if final_test_acc > best_acc:\n",
    "        print('ğŸ† Saving best model...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': final_test_acc,  # âœ… Ensures the best test accuracy is saved in checkpoint\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Ensure checkpoint directory exists\n",
    "        checkpoint_dir = \"checkpoint\"\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "        # âœ… Format learning rate properly before saving filename\n",
    "        lr_str = str(lr).replace('.', '_')\n",
    "        checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr_str}_{net1}_{optimizer1}.t7'\n",
    "        torch.save(state, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "        best_acc = final_test_acc  # âœ… Update best accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Append the best test accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):\n",
    "        with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\nğŸ† Best Test Accuracy: {best_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Print both Final and Best Test Accuracy (always executed)\n",
    "    print(f\"ğŸ“Š Test Accuracy: {final_test_acc:.3f}% | ğŸ† Best Test Accuracy: {best_acc:.3f}%\")\n",
    "    print(f\"ğŸ“œ Test logs saved to {test_results_path}!\")\n",
    "\n",
    "\n",
    "    global recent_test_acc\n",
    "    recent_test_acc = final_test_acc  # Capture latest test accuracy for next train() call | Store latest test accuracy\n",
    "\n",
    "    test_acc_history.append(final_test_acc)\n",
    "\n",
    "    return final_test_acc  # âœ… Return the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b24abd-3c74-4dd0-98fd-cdc0f3fd8730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:23<00:00, 32.81it/s, Train_acc=3.32, Train_loss=4.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000'] | Gamma1 Grad: ['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 3.318% (Updated)\n",
      "ğŸ“Š Train Accuracy: 3.318% | ğŸ† Best Train Accuracy: 3.318%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 3.318% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.23it/s, Test_acc=5.57, Test_loss=4.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 5.570% | ğŸ† Best Test Accuracy: 5.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "\n",
      "ğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\n",
      "âœ… Activation Parameters Unfrozen! ğŸš€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=1 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1865879399984 | Mean=1.50000 | Layer 1: ID=1865879400784 | Mean=1.50000 | Layer 2: ID=1865879401744 | Mean=1.50000 | Layer 3: ID=1865879402704 | Mean=1.50000 | Layer 4: ID=1865879403664 | Mean=1.50000 | Layer 5: ID=1865878144400 | Mean=1.50000 | Layer 6: ID=1865846647408 | Mean=1.50000 | Layer 7: ID=1865846639728 | Mean=1.50000 | Layer 8: ID=1865846638848 | Mean=1.50000 | Layer 9: ID=1865846651888 | Mean=1.50000 | Layer 10: ID=1865846648848 | Mean=1.50000 | Layer 11: ID=1865846648208 | Mean=1.50000 | Layer 12: ID=1865846647008 | Mean=1.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.92it/s, Train_acc=6.78, Train_loss=4.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.22795', '2.23494', '2.23014', '2.22774', '2.22733', '2.22671', '2.22577', '2.22691', '2.22785', '2.22699', '2.22252', '2.21943', '2.20776'] | Gamma1 Grad: ['0.00933', '-0.00427', '0.00018', '-0.00439', '-0.00850', '-0.00989', '-0.00877', '-0.00363', '-0.01255', '-0.01158', '-0.00627', '-0.02307', '0.00432']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 6.780% (Updated)\n",
      "ğŸ“Š Train Accuracy: 6.780% | ğŸ† Best Train Accuracy: 6.780%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 6.780% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.25it/s, Test_acc=6.95, Test_loss=3.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 6.950% | ğŸ† Best Test Accuracy: 6.950%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=2 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1865879399984 | Mean=2.22795 | Layer 1: ID=1865879400784 | Mean=2.23494 | Layer 2: ID=1865879401744 | Mean=2.23014 | Layer 3: ID=1865879402704 | Mean=2.22774 | Layer 4: ID=1865879403664 | Mean=2.22733 | Layer 5: ID=1865878144400 | Mean=2.22671 | Layer 6: ID=1865846647408 | Mean=2.22577 | Layer 7: ID=1865846639728 | Mean=2.22691 | Layer 8: ID=1865846638848 | Mean=2.22785 | Layer 9: ID=1865846651888 | Mean=2.22699 | Layer 10: ID=1865846648848 | Mean=2.22252 | Layer 11: ID=1865846648208 | Mean=2.21943 | Layer 12: ID=1865846647008 | Mean=2.20776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.70it/s, Train_acc=10.3, Train_loss=3.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.28884', '2.29982', '2.29246', '2.28344', '2.28527', '2.28075', '2.28175', '2.28316', '2.28382', '2.28266', '2.28037', '2.28211', '2.25888'] | Gamma1 Grad: ['-0.02591', '-0.00145', '0.00414', '-0.00905', '-0.00516', '-0.00106', '-0.00231', '0.00298', '-0.00554', '-0.00431', '-0.00276', '0.01448', '-0.01988']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 10.312% (Updated)\n",
      "ğŸ“Š Train Accuracy: 10.312% | ğŸ† Best Train Accuracy: 10.312%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 10.312% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.45it/s, Test_acc=13.1, Test_loss=3.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 13.100% | ğŸ† Best Test Accuracy: 13.100%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=3 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1865879399984 | Mean=2.28884 | Layer 1: ID=1865879400784 | Mean=2.29982 | Layer 2: ID=1865879401744 | Mean=2.29246 | Layer 3: ID=1865879402704 | Mean=2.28344 | Layer 4: ID=1865879403664 | Mean=2.28527 | Layer 5: ID=1865878144400 | Mean=2.28075 | Layer 6: ID=1865846647408 | Mean=2.28175 | Layer 7: ID=1865846639728 | Mean=2.28316 | Layer 8: ID=1865846638848 | Mean=2.28382 | Layer 9: ID=1865846651888 | Mean=2.28266 | Layer 10: ID=1865846648848 | Mean=2.28037 | Layer 11: ID=1865846648208 | Mean=2.28211 | Layer 12: ID=1865846647008 | Mean=2.25888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.57it/s, Train_acc=14.7, Train_loss=3.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.30452', '2.30929', '2.30599', '2.29050', '2.29394', '2.29187', '2.28966', '2.28794', '2.28910', '2.28720', '2.28508', '2.27090', '2.26998'] | Gamma1 Grad: ['-0.00124', '0.01234', '0.00805', '0.00987', '-0.00235', '0.00098', '-0.00544', '0.00027', '-0.01430', '-0.01423', '-0.00561', '-0.03088', '-0.04595']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 14.658% (Updated)\n",
      "ğŸ“Š Train Accuracy: 14.658% | ğŸ† Best Train Accuracy: 14.658%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 14.658% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.43it/s, Test_acc=18.6, Test_loss=3.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 18.550% | ğŸ† Best Test Accuracy: 18.550%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=4 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1865879399984 | Mean=2.30452 | Layer 1: ID=1865879400784 | Mean=2.30929 | Layer 2: ID=1865879401744 | Mean=2.30599 | Layer 3: ID=1865879402704 | Mean=2.29050 | Layer 4: ID=1865879403664 | Mean=2.29394 | Layer 5: ID=1865878144400 | Mean=2.29187 | Layer 6: ID=1865846647408 | Mean=2.28966 | Layer 7: ID=1865846639728 | Mean=2.28794 | Layer 8: ID=1865846638848 | Mean=2.28910 | Layer 9: ID=1865846651888 | Mean=2.28720 | Layer 10: ID=1865846648848 | Mean=2.28508 | Layer 11: ID=1865846648208 | Mean=2.27090 | Layer 12: ID=1865846647008 | Mean=2.26998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.72it/s, Train_acc=19.7, Train_loss=3.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.30142', '2.31566', '2.30612', '2.29783', '2.29718', '2.29525', '2.29362', '2.29456', '2.29035', '2.29254', '2.29215', '2.27742', '2.27782'] | Gamma1 Grad: ['0.02010', '-0.01584', '0.00732', '-0.00592', '0.00057', '-0.00173', '-0.00359', '0.00631', '-0.00354', '-0.00451', '0.00100', '0.00979', '0.01989']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 19.664% (Updated)\n",
      "ğŸ“Š Train Accuracy: 19.664% | ğŸ† Best Train Accuracy: 19.664%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 19.664% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.18it/s, Test_acc=22.3, Test_loss=2.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 22.340% | ğŸ† Best Test Accuracy: 22.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.41it/s, Train_acc=24, Train_loss=2.91]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.31396', '2.32951', '2.31623', '2.30199', '2.30410', '2.29932', '2.29610', '2.29563', '2.29588', '2.29123', '2.28973', '2.28172', '2.28559'] | Gamma1 Grad: ['-0.00715', '0.03196', '0.02884', '-0.00434', '-0.00035', '0.00328', '0.01173', '0.00527', '-0.00270', '0.01462', '0.00813', '-0.01765', '-0.01875']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 23.988% (Updated)\n",
      "ğŸ“Š Train Accuracy: 23.988% | ğŸ† Best Train Accuracy: 23.988%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 23.988% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.49it/s, Test_acc=27.1, Test_loss=2.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 27.050% | ğŸ† Best Test Accuracy: 27.050%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.89it/s, Train_acc=27.7, Train_loss=2.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.31912', '2.31693', '2.30322', '2.29344', '2.30172', '2.29663', '2.29033', '2.29164', '2.29101', '2.29411', '2.29270', '2.28287', '2.28815'] | Gamma1 Grad: ['-0.02130', '-0.01486', '0.00909', '-0.01530', '-0.00769', '0.00322', '0.00263', '-0.00912', '0.00154', '0.00471', '0.00523', '0.01145', '0.00806']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 27.738% (Updated)\n",
      "ğŸ“Š Train Accuracy: 27.738% | ğŸ† Best Train Accuracy: 27.738%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 27.738% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.25it/s, Test_acc=28.8, Test_loss=2.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 28.790% | ğŸ† Best Test Accuracy: 28.790%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.83it/s, Train_acc=31.8, Train_loss=2.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.31611', '2.30953', '2.30621', '2.30120', '2.29471', '2.29868', '2.29516', '2.29585', '2.29454', '2.29309', '2.29558', '2.28610', '2.28851'] | Gamma1 Grad: ['-0.00238', '0.00565', '-0.00667', '0.00570', '0.00136', '0.00472', '0.00199', '-0.00137', '-0.00201', '-0.00452', '0.00170', '-0.02368', '0.10247']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 31.802% (Updated)\n",
      "ğŸ“Š Train Accuracy: 31.802% | ğŸ† Best Train Accuracy: 31.802%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 31.802% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.71it/s, Test_acc=32.7, Test_loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 32.680% | ğŸ† Best Test Accuracy: 32.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.18it/s, Train_acc=35.6, Train_loss=2.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.31906', '2.32422', '2.32015', '2.29657', '2.29969', '2.30031', '2.29222', '2.29704', '2.29778', '2.29779', '2.29597', '2.28510', '2.29539'] | Gamma1 Grad: ['-0.00784', '-0.01541', '0.00626', '0.02285', '0.00338', '-0.01002', '0.00616', '-0.00156', '0.01095', '0.00003', '-0.00092', '0.02153', '0.07887']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 35.616% (Updated)\n",
      "ğŸ“Š Train Accuracy: 35.616% | ğŸ† Best Train Accuracy: 35.616%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 35.616% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.91it/s, Test_acc=36.3, Test_loss=2.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 36.320% | ğŸ† Best Test Accuracy: 36.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.95it/s, Train_acc=38.9, Train_loss=2.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.31945', '2.31567', '2.31731', '2.29417', '2.30094', '2.29687', '2.28601', '2.29714', '2.29911', '2.29887', '2.29669', '2.27922', '2.29607'] | Gamma1 Grad: ['-0.01222', '-0.01300', '-0.01587', '-0.02295', '0.00700', '-0.00674', '0.00743', '0.00600', '-0.00267', '-0.01260', '-0.00180', '0.00868', '-0.00895']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 38.878% (Updated)\n",
      "ğŸ“Š Train Accuracy: 38.878% | ğŸ† Best Train Accuracy: 38.878%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 38.878% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.92it/s, Test_acc=42, Test_loss=2.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 42.010% | ğŸ† Best Test Accuracy: 42.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.89it/s, Train_acc=42.6, Train_loss=2.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.31593', '2.31946', '2.31958', '2.29723', '2.30439', '2.30470', '2.29356', '2.30038', '2.29461', '2.29507', '2.29851', '2.28756', '2.29678'] | Gamma1 Grad: ['-0.02378', '-0.03103', '0.00215', '-0.01518', '0.00255', '0.00728', '0.00429', '-0.00024', '0.00662', '-0.00005', '0.00565', '0.00593', '0.00921']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 42.584% (Updated)\n",
      "ğŸ“Š Train Accuracy: 42.584% | ğŸ† Best Train Accuracy: 42.584%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 42.584% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 10 | TrainLossHist: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.46it/s, Test_acc=41.9, Test_loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 41.910% | ğŸ† Best Test Accuracy: 42.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.32it/s, Train_acc=45.4, Train_loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.31190', '2.31094', '2.32117', '2.29791', '2.30358', '2.30844', '2.28715', '2.29200', '2.29662', '2.29234', '2.29288', '2.28071', '2.30138'] | Gamma1 Grad: ['-0.00967', '0.01198', '0.01322', '-0.00865', '-0.00627', '-0.01812', '0.00522', '-0.00039', '0.00516', '0.00798', '-0.00181', '-0.00470', '0.05069']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 45.368% (Updated)\n",
      "ğŸ“Š Train Accuracy: 45.368% | ğŸ† Best Train Accuracy: 45.368%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 45.368% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 78.78it/s, Test_acc=46.3, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 46.340% | ğŸ† Best Test Accuracy: 46.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.70it/s, Train_acc=48, Train_loss=1.87]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.30581', '2.31613', '2.31299', '2.29396', '2.31024', '2.30826', '2.28510', '2.29722', '2.28825', '2.28890', '2.29048', '2.27453', '2.29948'] | Gamma1 Grad: ['-0.00411', '0.01445', '-0.00104', '0.00955', '0.01228', '-0.01129', '-0.00113', '-0.00042', '-0.00452', '0.00152', '-0.00302', '0.00117', '0.04521']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 47.972% (Updated)\n",
      "ğŸ“Š Train Accuracy: 47.972% | ğŸ† Best Train Accuracy: 47.972%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 47.972% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.40it/s, Test_acc=49.2, Test_loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 49.210% | ğŸ† Best Test Accuracy: 49.210%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.24it/s, Train_acc=50.4, Train_loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.32401', '2.33179', '2.30877', '2.29862', '2.30028', '2.31004', '2.29433', '2.31388', '2.30652', '2.29924', '2.28384', '2.29501', '2.30425'] | Gamma1 Grad: ['0.01317', '0.04420', '-0.05501', '0.03276', '-0.02279', '-0.00831', '0.00486', '-0.00015', '0.00730', '0.00692', '0.00479', '0.02136', '0.04560']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 50.362% (Updated)\n",
      "ğŸ“Š Train Accuracy: 50.362% | ğŸ† Best Train Accuracy: 50.362%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 50.362% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.39it/s, Test_acc=50.8, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 50.750% | ğŸ† Best Test Accuracy: 50.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.78it/s, Train_acc=52.7, Train_loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.31004', '2.32952', '2.31872', '2.28208', '2.31185', '2.29832', '2.28511', '2.29961', '2.29408', '2.28563', '2.28174', '2.30272', '2.29541'] | Gamma1 Grad: ['0.02175', '-0.00278', '0.00516', '-0.05175', '0.00059', '-0.00302', '-0.01966', '-0.00989', '0.00287', '0.00153', '0.00040', '0.01602', '-0.02398']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 52.748% (Updated)\n",
      "ğŸ“Š Train Accuracy: 52.748% | ğŸ† Best Train Accuracy: 52.748%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 52.748% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.94it/s, Test_acc=50.7, Test_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 50.700% | ğŸ† Best Test Accuracy: 50.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.52it/s, Train_acc=54.8, Train_loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['2.30914', '2.30790', '2.30298', '2.30349', '2.30264', '2.31261', '2.29526', '2.29976', '2.30129', '2.29923', '2.29926', '2.28454', '2.29439'] | Gamma1 Grad: ['0.04509', '-0.01031', '0.02336', '-0.00153', '-0.01677', '0.01134', '-0.00453', '0.01892', '0.01282', '0.02207', '0.01272', '0.00082', '0.07534']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 54.808% (Updated)\n",
      "ğŸ“Š Train Accuracy: 54.808% | ğŸ† Best Train Accuracy: 54.808%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 54.808% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.77it/s, Test_acc=52.9, Test_loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 52.930% | ğŸ† Best Test Accuracy: 52.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.57it/s, Train_acc=56.3, Train_loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.30564', '2.31861', '2.31022', '2.30022', '2.30269', '2.31707', '2.29494', '2.30311', '2.28614', '2.28806', '2.29570', '2.28963', '2.30124'] | Gamma1 Grad: ['0.06745', '-0.00660', '0.01990', '-0.00870', '-0.00069', '0.01989', '0.01924', '0.00671', '0.00539', '0.00427', '0.00122', '0.02216', '-0.01129']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 56.290% (Updated)\n",
      "ğŸ“Š Train Accuracy: 56.290% | ğŸ† Best Train Accuracy: 56.290%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 56.290% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.71it/s, Test_acc=53.4, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 53.350% | ğŸ† Best Test Accuracy: 53.350%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.95it/s, Train_acc=58.3, Train_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['2.31847', '2.31544', '2.29697', '2.30197', '2.29769', '2.31385', '2.29566', '2.30607', '2.29777', '2.29105', '2.29502', '2.28436', '2.31730'] | Gamma1 Grad: ['-0.01297', '0.00964', '0.00964', '-0.01345', '-0.00278', '-0.01047', '-0.00141', '-0.00219', '-0.00052', '-0.00223', '-0.00436', '-0.00077', '0.03937']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 58.320% (Updated)\n",
      "ğŸ“Š Train Accuracy: 58.320% | ğŸ† Best Train Accuracy: 58.320%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 58.320% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.69it/s, Test_acc=54.9, Test_loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 54.930% | ğŸ† Best Test Accuracy: 54.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.16it/s, Train_acc=59.9, Train_loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.31037', '2.32956', '2.28599', '2.30087', '2.31624', '2.31329', '2.29524', '2.30096', '2.29609', '2.29888', '2.29496', '2.28913', '2.31099'] | Gamma1 Grad: ['0.01783', '-0.04290', '0.01935', '0.02963', '-0.01311', '0.00796', '-0.00879', '-0.00192', '-0.00699', '-0.00139', '-0.00142', '-0.00250', '0.03150']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 59.864% (Updated)\n",
      "ğŸ“Š Train Accuracy: 59.864% | ğŸ† Best Train Accuracy: 59.864%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 59.864% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.15it/s, Test_acc=55.6, Test_loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 55.560% | ğŸ† Best Test Accuracy: 55.560%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.83it/s, Train_acc=61.4, Train_loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['2.31032', '2.32892', '2.30398', '2.28295', '2.31196', '2.29479', '2.29541', '2.30208', '2.29956', '2.29943', '2.29464', '2.28675', '2.30517'] | Gamma1 Grad: ['0.00095', '0.03513', '-0.03057', '-0.00982', '0.00083', '-0.00720', '0.00224', '0.00244', '-0.00100', '0.00435', '0.00468', '-0.01442', '-0.02521']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 61.408% (Updated)\n",
      "ğŸ“Š Train Accuracy: 61.408% | ğŸ† Best Train Accuracy: 61.408%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 61.408% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.59it/s, Test_acc=57.4, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 57.410% | ğŸ† Best Test Accuracy: 57.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.98it/s, Train_acc=62.9, Train_loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.31049', '2.31326', '2.28951', '2.29833', '2.30943', '2.30552', '2.29914', '2.29825', '2.29199', '2.29254', '2.28868', '2.28510', '2.30239'] | Gamma1 Grad: ['0.01844', '-0.03893', '0.02050', '0.00215', '-0.04958', '-0.04229', '-0.00461', '0.00830', '0.00922', '0.01524', '0.00358', '-0.03336', '-0.01744']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 62.900% (Updated)\n",
      "ğŸ“Š Train Accuracy: 62.900% | ğŸ† Best Train Accuracy: 62.900%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 62.900% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 20 | TrainLossHist: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.50it/s, Test_acc=57.2, Test_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 57.210% | ğŸ† Best Test Accuracy: 57.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.68it/s, Train_acc=64.1, Train_loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['2.30037', '2.31205', '2.30213', '2.29877', '2.30422', '2.30516', '2.29559', '2.29973', '2.29533', '2.29436', '2.29850', '2.29097', '2.30310'] | Gamma1 Grad: ['0.02825', '0.04476', '0.06778', '-0.00691', '0.02241', '0.01790', '-0.01412', '-0.00069', '-0.00634', '0.00171', '0.00599', '0.01950', '-0.01695']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 64.092% (Updated)\n",
      "ğŸ“Š Train Accuracy: 64.092% | ğŸ† Best Train Accuracy: 64.092%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 64.092% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.54it/s, Test_acc=58.4, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 58.440% | ğŸ† Best Test Accuracy: 58.440%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.35it/s, Train_acc=65.6, Train_loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.31166', '2.30365', '2.30151', '2.29231', '2.30498', '2.29780', '2.29297', '2.30700', '2.29725', '2.29379', '2.29177', '2.28818', '2.30336'] | Gamma1 Grad: ['0.02584', '-0.02835', '0.04616', '-0.01460', '-0.01118', '0.02200', '0.00065', '-0.00839', '-0.01570', '-0.01098', '-0.00790', '0.01017', '0.02394']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 65.620% (Updated)\n",
      "ğŸ“Š Train Accuracy: 65.620% | ğŸ† Best Train Accuracy: 65.620%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 65.620% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.14it/s, Test_acc=59.2, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 59.240% | ğŸ† Best Test Accuracy: 59.240%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.57it/s, Train_acc=66.7, Train_loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.30211', '2.30726', '2.30204', '2.28808', '2.31075', '2.30821', '2.29629', '2.30564', '2.29625', '2.29486', '2.29474', '2.28691', '2.30840'] | Gamma1 Grad: ['-0.00633', '0.01475', '-0.09762', '-0.00574', '0.00910', '-0.02231', '0.01183', '-0.01242', '-0.00331', '-0.00061', '-0.01316', '0.00772', '0.04727']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 66.666% (Updated)\n",
      "ğŸ“Š Train Accuracy: 66.666% | ğŸ† Best Train Accuracy: 66.666%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 66.666% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.66it/s, Test_acc=60.5, Test_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.450% | ğŸ† Best Test Accuracy: 60.450%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.43it/s, Train_acc=67.8, Train_loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.30420', '2.30468', '2.30533', '2.29444', '2.30506', '2.30430', '2.29695', '2.29759', '2.30131', '2.29623', '2.29641', '2.29246', '2.30512'] | Gamma1 Grad: ['-0.02510', '0.03043', '-0.01714', '0.04024', '-0.00212', '-0.02404', '0.02546', '-0.00962', '0.00565', '0.00147', '0.00186', '0.00397', '-0.02210']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 67.752% (Updated)\n",
      "ğŸ“Š Train Accuracy: 67.752% | ğŸ† Best Train Accuracy: 67.752%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 67.752% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.25it/s, Test_acc=59.3, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 59.340% | ğŸ† Best Test Accuracy: 60.450%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.35it/s, Train_acc=69.1, Train_loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['2.31103', '2.31088', '2.30369', '2.30801', '2.30529', '2.30251', '2.29268', '2.29987', '2.29262', '2.28783', '2.28688', '2.29439', '2.30652'] | Gamma1 Grad: ['-0.05887', '-0.01094', '0.00660', '-0.02780', '-0.02295', '-0.01470', '-0.00120', '0.00144', '-0.00401', '0.01066', '0.00203', '-0.00090', '-0.03916']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 69.116% (Updated)\n",
      "ğŸ“Š Train Accuracy: 69.116% | ğŸ† Best Train Accuracy: 69.116%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 69.116% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.49it/s, Test_acc=59.9, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 59.930% | ğŸ† Best Test Accuracy: 60.450%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.60it/s, Train_acc=70.1, Train_loss=1.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.30619', '2.31288', '2.31550', '2.30280', '2.30522', '2.32320', '2.30702', '2.30778', '2.30328', '2.29067', '2.28958', '2.28458', '2.29510'] | Gamma1 Grad: ['-0.02997', '-0.05745', '0.04713', '-0.02904', '-0.06871', '0.02420', '-0.01388', '-0.00333', '-0.00009', '-0.00318', '-0.00721', '0.01732', '-0.00159']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 70.148% (Updated)\n",
      "ğŸ“Š Train Accuracy: 70.148% | ğŸ† Best Train Accuracy: 70.148%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 70.148% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.73it/s, Test_acc=60.9, Test_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.870% | ğŸ† Best Test Accuracy: 60.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.02it/s, Train_acc=71.4, Train_loss=0.978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['2.29980', '2.30539', '2.29408', '2.30093', '2.28873', '2.31132', '2.30126', '2.31227', '2.29830', '2.29575', '2.29603', '2.29509', '2.29895'] | Gamma1 Grad: ['0.03326', '-0.00073', '-0.00284', '-0.00221', '-0.05428', '0.01585', '0.00727', '0.00091', '-0.01219', '0.01133', '0.01576', '0.00395', '0.05335']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 71.434% (Updated)\n",
      "ğŸ“Š Train Accuracy: 71.434% | ğŸ† Best Train Accuracy: 71.434%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 71.434% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.89it/s, Test_acc=61.1, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.150% | ğŸ† Best Test Accuracy: 61.150%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.52it/s, Train_acc=72, Train_loss=0.942]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.30225', '2.31114', '2.30496', '2.30849', '2.31341', '2.31424', '2.29846', '2.30955', '2.29880', '2.28700', '2.29377', '2.28857', '2.31441'] | Gamma1 Grad: ['0.01762', '-0.01127', '0.02500', '-0.03139', '-0.01136', '-0.01503', '0.01458', '0.00787', '0.01495', '0.00391', '-0.00074', '0.03163', '0.00202']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 72.036% (Updated)\n",
      "ğŸ“Š Train Accuracy: 72.036% | ğŸ† Best Train Accuracy: 72.036%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 72.036% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.59it/s, Test_acc=61.1, Test_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 61.120% | ğŸ† Best Test Accuracy: 61.150%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.52it/s, Train_acc=73.5, Train_loss=0.901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['2.29399', '2.30939', '2.29871', '2.29137', '2.30815', '2.30572', '2.29890', '2.30738', '2.29868', '2.29348', '2.29563', '2.28682', '2.30013'] | Gamma1 Grad: ['0.01718', '0.04184', '-0.01097', '0.04712', '0.00798', '-0.03138', '-0.00123', '-0.00932', '-0.00205', '-0.00969', '0.00961', '-0.01879', '0.06159']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 73.504% (Updated)\n",
      "ğŸ“Š Train Accuracy: 73.504% | ğŸ† Best Train Accuracy: 73.504%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 73.504% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.69it/s, Test_acc=61.4, Test_loss=1.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.430% | ğŸ† Best Test Accuracy: 61.430%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.11it/s, Train_acc=74.4, Train_loss=0.867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.30053', '2.30639', '2.30145', '2.30143', '2.29704', '2.30827', '2.30422', '2.31090', '2.29751', '2.29049', '2.29120', '2.28593', '2.30441'] | Gamma1 Grad: ['-0.01115', '0.00721', '-0.05039', '0.00081', '0.02189', '-0.00127', '-0.00800', '0.00167', '-0.00108', '-0.01838', '-0.00371', '0.03136', '-0.04273']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 74.418% (Updated)\n",
      "ğŸ“Š Train Accuracy: 74.418% | ğŸ† Best Train Accuracy: 74.418%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 74.418% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 30 | TrainLossHist: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.45it/s, Test_acc=60.7, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 60.720% | ğŸ† Best Test Accuracy: 61.430%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.20it/s, Train_acc=75.5, Train_loss=0.829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.30787', '2.30620', '2.31357', '2.30855', '2.30167', '2.30574', '2.30234', '2.31594', '2.29918', '2.29436', '2.29986', '2.28933', '2.30054'] | Gamma1 Grad: ['0.03543', '0.02835', '-0.00693', '0.00629', '-0.00009', '-0.00480', '-0.01227', '-0.00535', '0.00508', '0.01494', '0.01862', '-0.00722', '-0.06868']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 75.500% (Updated)\n",
      "ğŸ“Š Train Accuracy: 75.500% | ğŸ† Best Train Accuracy: 75.500%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 75.500% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.20it/s, Test_acc=61.9, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.870% | ğŸ† Best Test Accuracy: 61.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.87it/s, Train_acc=76.5, Train_loss=0.796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.30563', '2.32110', '2.29620', '2.31170', '2.32114', '2.29578', '2.29463', '2.30017', '2.30099', '2.29373', '2.29658', '2.28690', '2.30512'] | Gamma1 Grad: ['-0.01921', '-0.01659', '0.00871', '-0.04182', '0.00728', '0.00099', '-0.01096', '0.00318', '-0.01110', '-0.00031', '-0.00601', '-0.00687', '-0.04899']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 76.450% (Updated)\n",
      "ğŸ“Š Train Accuracy: 76.450% | ğŸ† Best Train Accuracy: 76.450%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 76.450% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.20it/s, Test_acc=61.8, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 61.800% | ğŸ† Best Test Accuracy: 61.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.73it/s, Train_acc=77.1, Train_loss=0.771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.30116', '2.30638', '2.31348', '2.28157', '2.29849', '2.29889', '2.29647', '2.30649', '2.30909', '2.29734', '2.30143', '2.29431', '2.30533'] | Gamma1 Grad: ['-0.02952', '0.00581', '-0.02508', '0.00063', '-0.01702', '0.00286', '-0.02662', '-0.00173', '-0.01450', '-0.00526', '-0.00049', '-0.01027', '0.01532']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 77.074% (Updated)\n",
      "ğŸ“Š Train Accuracy: 77.074% | ğŸ† Best Train Accuracy: 77.074%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 77.074% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.61it/s, Test_acc=62.6, Test_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.600% | ğŸ† Best Test Accuracy: 62.600%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.69it/s, Train_acc=77.9, Train_loss=0.739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.29684', '2.30900', '2.29522', '2.29152', '2.29214', '2.29255', '2.28540', '2.29955', '2.30722', '2.29928', '2.30120', '2.28447', '2.29603'] | Gamma1 Grad: ['0.00435', '-0.02679', '0.01786', '-0.02411', '0.02450', '-0.00993', '-0.01590', '0.00811', '-0.01688', '-0.00088', '0.00763', '0.00373', '0.03015']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 77.908% (Updated)\n",
      "ğŸ“Š Train Accuracy: 77.908% | ğŸ† Best Train Accuracy: 77.908%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 77.908% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.23it/s, Test_acc=62.6, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.650% | ğŸ† Best Test Accuracy: 62.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.52it/s, Train_acc=78.8, Train_loss=0.712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00144 | Gamma1: ['2.30346', '2.30798', '2.30574', '2.29176', '2.31671', '2.31965', '2.31154', '2.31481', '2.28413', '2.29299', '2.29218', '2.28023', '2.30122'] | Gamma1 Grad: ['0.02114', '-0.03370', '0.06468', '0.00472', '0.04061', '0.04749', '0.02221', '-0.01823', '0.00328', '-0.00817', '0.00797', '-0.01596', '0.01042']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 78.766% (Updated)\n",
      "ğŸ“Š Train Accuracy: 78.766% | ğŸ† Best Train Accuracy: 78.766%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 78.766% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.86it/s, Test_acc=62.8, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.840% | ğŸ† Best Test Accuracy: 62.840%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.50it/s, Train_acc=79.5, Train_loss=0.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.29638', '2.29516', '2.31436', '2.30472', '2.29736', '2.31207', '2.28910', '2.31738', '2.30263', '2.30244', '2.30044', '2.29578', '2.29071'] | Gamma1 Grad: ['0.01429', '-0.00684', '0.03460', '-0.00764', '0.07634', '0.01890', '-0.00724', '0.00722', '-0.01386', '-0.00519', '-0.00602', '-0.03155', '0.03015']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 79.488% (Updated)\n",
      "ğŸ“Š Train Accuracy: 79.488% | ğŸ† Best Train Accuracy: 79.488%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 79.488% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.28it/s, Test_acc=63.7, Test_loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.680% | ğŸ† Best Test Accuracy: 63.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.00it/s, Train_acc=80.2, Train_loss=0.659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00139 | Gamma1: ['2.29286', '2.29701', '2.29723', '2.28168', '2.27767', '2.32766', '2.29768', '2.29706', '2.30966', '2.28996', '2.29128', '2.29787', '2.31462'] | Gamma1 Grad: ['0.09329', '-0.00571', '-0.06483', '-0.01963', '0.04862', '0.02063', '-0.02712', '-0.00979', '-0.00873', '0.00123', '0.00064', '0.00862', '0.01211']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 80.200% (Updated)\n",
      "ğŸ“Š Train Accuracy: 80.200% | ğŸ† Best Train Accuracy: 80.200%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 80.200% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.35it/s, Test_acc=63, Test_loss=1.53]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.000% | ğŸ† Best Test Accuracy: 63.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.85it/s, Train_acc=80.8, Train_loss=0.639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.30101', '2.29198', '2.29097', '2.30455', '2.30658', '2.31612', '2.30655', '2.30876', '2.29968', '2.29188', '2.28613', '2.28475', '2.30401'] | Gamma1 Grad: ['0.01968', '0.02063', '-0.03940', '0.08943', '0.00307', '-0.07245', '0.01661', '-0.01017', '-0.01626', '-0.00643', '-0.00875', '0.01506', '0.07420']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 80.750% (Updated)\n",
      "ğŸ“Š Train Accuracy: 80.750% | ğŸ† Best Train Accuracy: 80.750%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 80.750% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.02it/s, Test_acc=63.1, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.140% | ğŸ† Best Test Accuracy: 63.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.35it/s, Train_acc=81.9, Train_loss=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00133 | Gamma1: ['2.30842', '2.30202', '2.31289', '2.28890', '2.29348', '2.30453', '2.29817', '2.31163', '2.29049', '2.29596', '2.30102', '2.28868', '2.30029'] | Gamma1 Grad: ['-0.01481', '0.04445', '0.03337', '-0.00448', '0.08127', '0.01119', '0.00341', '0.01130', '-0.00389', '0.00800', '-0.00556', '-0.02109', '0.02605']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 81.942% (Updated)\n",
      "ğŸ“Š Train Accuracy: 81.942% | ğŸ† Best Train Accuracy: 81.942%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 81.942% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.10it/s, Test_acc=63.8, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.780% | ğŸ† Best Test Accuracy: 63.780%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.02it/s, Train_acc=82.3, Train_loss=0.59] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['2.32785', '2.36822', '2.31433', '2.28462', '2.31788', '2.33005', '2.35005', '2.35369', '2.32211', '2.28662', '2.30023', '2.27841', '2.32194'] | Gamma1 Grad: ['0.00032', '0.01784', '-0.06867', '-0.03452', '0.03407', '0.00536', '-0.00858', '-0.01249', '-0.00228', '0.00971', '0.00581', '-0.03398', '-0.03253']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.316% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.316% | ğŸ† Best Train Accuracy: 82.316%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.316% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 40 | TrainLossHist: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.70it/s, Test_acc=62.9, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.940% | ğŸ† Best Test Accuracy: 63.780%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.06it/s, Train_acc=82.8, Train_loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00125 | Gamma1: ['2.34143', '2.33211', '2.33722', '2.28635', '2.32009', '2.33485', '2.33890', '2.35634', '2.32191', '2.29827', '2.29917', '2.28185', '2.31432'] | Gamma1 Grad: ['-0.01402', '0.02721', '0.03543', '0.04717', '-0.15081', '0.03516', '-0.00721', '-0.03300', '0.02834', '0.00754', '-0.00015', '0.01789', '-0.01119']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.790% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.790% | ğŸ† Best Train Accuracy: 82.790%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.790% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.79it/s, Test_acc=64, Test_loss=1.52]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.010% | ğŸ† Best Test Accuracy: 64.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.06it/s, Train_acc=83.4, Train_loss=0.551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.32835', '2.33760', '2.33140', '2.28733', '2.29475', '2.34208', '2.36179', '2.35933', '2.33239', '2.30886', '2.29830', '2.28068', '2.33531'] | Gamma1 Grad: ['0.03216', '0.03165', '0.01330', '0.01951', '0.00737', '-0.01866', '-0.00582', '0.02822', '-0.00638', '-0.01089', '0.00210', '-0.05803', '0.00725']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.432% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.432% | ğŸ† Best Train Accuracy: 83.432%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.432% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.35it/s, Test_acc=62.9, Test_loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.930% | ğŸ† Best Test Accuracy: 64.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.69it/s, Train_acc=84.1, Train_loss=0.529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00115 | Gamma1: ['2.32324', '2.35429', '2.32464', '2.27976', '2.31951', '2.33318', '2.36206', '2.35228', '2.33344', '2.30167', '2.31275', '2.27694', '2.33379'] | Gamma1 Grad: ['0.00835', '0.01879', '0.05146', '0.01075', '0.13559', '0.01229', '-0.01995', '0.02683', '-0.02680', '0.00476', '-0.00351', '0.00234', '0.01268']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 84.080% (Updated)\n",
      "ğŸ“Š Train Accuracy: 84.080% | ğŸ† Best Train Accuracy: 84.080%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 84.080% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.62it/s, Test_acc=64.1, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.110% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.42it/s, Train_acc=85.1, Train_loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['2.34228', '2.32350', '2.34506', '2.30113', '2.31506', '2.33301', '2.35903', '2.36016', '2.34560', '2.30351', '2.30616', '2.28567', '2.31515'] | Gamma1 Grad: ['0.07265', '-0.08264', '0.01510', '0.03667', '-0.09264', '-0.02275', '-0.03931', '0.04520', '0.01565', '0.01342', '0.00915', '-0.01865', '-0.06844']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.072% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.072% | ğŸ† Best Train Accuracy: 85.072%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.072% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.44it/s, Test_acc=63.5, Test_loss=1.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.460% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.97it/s, Train_acc=85.2, Train_loss=0.494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00105 | Gamma1: ['2.33963', '2.32586', '2.33009', '2.33215', '2.27518', '2.36003', '2.36726', '2.36048', '2.32935', '2.31248', '2.31396', '2.28825', '2.32400'] | Gamma1 Grad: ['0.03402', '-0.06075', '0.03854', '-0.02643', '0.01486', '-0.02041', '0.01935', '-0.05366', '0.02361', '0.00272', '-0.01101', '0.00831', '0.03021']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.226% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.226% | ğŸ† Best Train Accuracy: 85.226%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.226% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.03it/s, Test_acc=63.9, Test_loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.920% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.57it/s, Train_acc=86, Train_loss=0.471]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.33753', '2.34579', '2.31915', '2.29751', '2.29250', '2.32082', '2.35770', '2.36222', '2.32687', '2.31010', '2.30820', '2.30021', '2.31168'] | Gamma1 Grad: ['-0.05758', '-0.01232', '0.03409', '0.02060', '-0.05752', '0.01996', '-0.02736', '0.01137', '-0.01410', '0.00081', '0.00924', '-0.00918', '0.01042']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.992% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.992% | ğŸ† Best Train Accuracy: 85.992%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.992% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.61it/s, Test_acc=64, Test_loss=1.62]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.030% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.64it/s, Train_acc=86.2, Train_loss=0.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00094 | Gamma1: ['2.32553', '2.33847', '2.29180', '2.28050', '2.29037', '2.32413', '2.36038', '2.35702', '2.33670', '2.30453', '2.30507', '2.27260', '2.30436'] | Gamma1 Grad: ['-0.06606', '0.01653', '0.03017', '-0.05834', '-0.03008', '0.00904', '-0.01204', '0.01683', '-0.00205', '0.00052', '0.00378', '0.02822', '-0.00414']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.244% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.244% | ğŸ† Best Train Accuracy: 86.244%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.244% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.55it/s, Test_acc=64.1, Test_loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.060% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.99it/s, Train_acc=86.4, Train_loss=0.451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['2.32138', '2.33499', '2.30376', '2.26477', '2.26786', '2.33342', '2.36116', '2.37256', '2.33531', '2.30154', '2.29788', '2.28516', '2.31655'] | Gamma1 Grad: ['-0.05498', '-0.02548', '0.02084', '-0.00101', '0.06548', '0.03397', '-0.02062', '-0.01467', '0.02277', '0.00347', '0.01572', '-0.10981', '0.11022']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.440% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.440% | ğŸ† Best Train Accuracy: 86.440%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.440% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.46it/s, Test_acc=63.9, Test_loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.910% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.16it/s, Train_acc=87.1, Train_loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00083 | Gamma1: ['2.29691', '2.32208', '2.30020', '2.27401', '2.27825', '2.32554', '2.34648', '2.37954', '2.34359', '2.30782', '2.30356', '2.29514', '2.30897'] | Gamma1 Grad: ['-0.01040', '-0.06467', '-0.04596', '0.03871', '-0.05584', '0.01186', '0.00643', '0.02114', '0.02310', '0.00209', '0.01064', '0.00412', '0.00670']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.122% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.122% | ğŸ† Best Train Accuracy: 87.122%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.122% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.84it/s, Test_acc=63.3, Test_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.310% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.02it/s, Train_acc=87.6, Train_loss=0.413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.31461', '2.32789', '2.31209', '2.28250', '2.28071', '2.31760', '2.34165', '2.35604', '2.32118', '2.30860', '2.30018', '2.30198', '2.30937'] | Gamma1 Grad: ['0.02545', '-0.02947', '-0.02771', '0.08208', '-0.04242', '0.05901', '-0.03606', '-0.03510', '0.00545', '-0.00028', '0.01006', '0.00831', '-0.05324']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.636% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.636% | ğŸ† Best Train Accuracy: 87.636%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.636% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 50 | TrainLossHist: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.65it/s, Test_acc=64.1, Test_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.080% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.13it/s, Train_acc=87.9, Train_loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00072 | Gamma1: ['2.32663', '2.32864', '2.30062', '2.29923', '2.27513', '2.32794', '2.35331', '2.35596', '2.30585', '2.30892', '2.29592', '2.28718', '2.30641'] | Gamma1 Grad: ['-0.03192', '-0.04269', '0.02709', '0.00187', '0.00798', '0.01857', '-0.00516', '-0.00023', '0.01589', '-0.01309', '-0.00053', '0.00593', '-0.04175']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.910% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.910% | ğŸ† Best Train Accuracy: 87.910%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.910% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.71it/s, Test_acc=64.3, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.300% | ğŸ† Best Test Accuracy: 64.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.68it/s, Train_acc=88.3, Train_loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['2.29634', '2.32743', '2.29585', '2.29789', '2.26978', '2.32551', '2.34226', '2.34813', '2.30228', '2.29604', '2.29895', '2.28281', '2.29632'] | Gamma1 Grad: ['-0.02660', '-0.02216', '-0.01027', '-0.00503', '0.00267', '-0.04602', '0.00739', '0.02053', '0.00018', '0.00162', '0.00294', '0.01653', '0.00480']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.286% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.286% | ğŸ† Best Train Accuracy: 88.286%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.286% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.29it/s, Test_acc=64.2, Test_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.210% | ğŸ† Best Test Accuracy: 64.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.27it/s, Train_acc=88.6, Train_loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00061 | Gamma1: ['2.31240', '2.31674', '2.31604', '2.28683', '2.28493', '2.30685', '2.33367', '2.34113', '2.32368', '2.29190', '2.29850', '2.27682', '2.28451'] | Gamma1 Grad: ['-0.02222', '-0.01150', '0.08557', '0.01777', '-0.00301', '-0.02631', '0.00151', '0.02533', '0.01461', '0.00057', '-0.00301', '0.00573', '-0.02653']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.590% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.590% | ğŸ† Best Train Accuracy: 88.590%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.590% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.29it/s, Test_acc=64, Test_loss=1.68]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.050% | ğŸ† Best Test Accuracy: 64.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.62it/s, Train_acc=89, Train_loss=0.371]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.31577', '2.33190', '2.29870', '2.28745', '2.26112', '2.31182', '2.31171', '2.35149', '2.32976', '2.30513', '2.30688', '2.29171', '2.30499'] | Gamma1 Grad: ['-0.05887', '-0.03329', '-0.09713', '0.01856', '0.04816', '-0.02257', '-0.00326', '0.02730', '-0.00841', '-0.01670', '-0.00007', '-0.01630', '-0.00733']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.954% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.954% | ğŸ† Best Train Accuracy: 88.954%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.954% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.60it/s, Test_acc=64.5, Test_loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.520% | ğŸ† Best Test Accuracy: 64.520%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.43it/s, Train_acc=89.5, Train_loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00050 | Gamma1: ['2.31751', '2.33232', '2.30012', '2.30114', '2.25457', '2.31276', '2.33954', '2.35472', '2.32510', '2.31072', '2.31443', '2.27154', '2.30376'] | Gamma1 Grad: ['0.02584', '-0.05522', '-0.00816', '-0.01114', '0.06369', '-0.00408', '0.00621', '0.08233', '-0.01952', '-0.01376', '0.00753', '-0.00769', '0.04694']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.510% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.510% | ğŸ† Best Train Accuracy: 89.510%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.510% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.90it/s, Test_acc=64.7, Test_loss=1.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.740% | ğŸ† Best Test Accuracy: 64.740%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.52it/s, Train_acc=89.8, Train_loss=0.345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.31412', '2.31191', '2.30164', '2.27282', '2.27666', '2.32437', '2.33954', '2.34987', '2.32004', '2.29939', '2.30497', '2.28879', '2.28856'] | Gamma1 Grad: ['-0.03206', '-0.02980', '0.07105', '-0.06343', '0.06642', '-0.08576', '-0.00836', '-0.01040', '-0.01128', '0.00174', '-0.00192', '-0.00293', '0.08919']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.798% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.798% | ğŸ† Best Train Accuracy: 89.798%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.798% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.65it/s, Test_acc=64.3, Test_loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.310% | ğŸ† Best Test Accuracy: 64.740%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.73it/s, Train_acc=90, Train_loss=0.341]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00040 | Gamma1: ['2.30827', '2.32922', '2.29781', '2.27285', '2.25605', '2.29529', '2.33771', '2.33566', '2.32158', '2.30728', '2.29315', '2.28417', '2.29862'] | Gamma1 Grad: ['-0.01862', '0.00254', '-0.00558', '-0.04941', '-0.00642', '0.02671', '-0.00839', '0.00180', '-0.00462', '0.00326', '0.01949', '-0.00011', '0.03511']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.018% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.018% | ğŸ† Best Train Accuracy: 90.018%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.018% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.96it/s, Test_acc=64.7, Test_loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.710% | ğŸ† Best Test Accuracy: 64.740%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.07it/s, Train_acc=90.5, Train_loss=0.324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.32330', '2.35502', '2.27941', '2.28458', '2.26532', '2.28424', '2.34468', '2.34790', '2.31294', '2.30009', '2.27546', '2.27826', '2.30824'] | Gamma1 Grad: ['-0.00353', '0.02084', '0.03487', '0.04673', '0.08303', '0.04752', '-0.08589', '0.05354', '0.01901', '0.00303', '0.00096', '0.01191', '0.00540']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.450% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.450% | ğŸ† Best Train Accuracy: 90.450%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.450% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.50it/s, Test_acc=64.7, Test_loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.650% | ğŸ† Best Test Accuracy: 64.740%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.48it/s, Train_acc=90.5, Train_loss=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00030 | Gamma1: ['2.32017', '2.33495', '2.27126', '2.30242', '2.26533', '2.27025', '2.33169', '2.33808', '2.32075', '2.30328', '2.28637', '2.28977', '2.30088'] | Gamma1 Grad: ['-0.02856', '0.00343', '-0.03381', '0.04590', '0.06816', '-0.07149', '-0.03488', '0.01188', '-0.00098', '0.00184', '-0.00303', '-0.01587', '-0.00711']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.500% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.500% | ğŸ† Best Train Accuracy: 90.500%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.500% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.72it/s, Test_acc=64.8, Test_loss=1.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.760% | ğŸ† Best Test Accuracy: 64.760%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.71it/s, Train_acc=90.8, Train_loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['2.32366', '2.33767', '2.30877', '2.29204', '2.27655', '2.30698', '2.33319', '2.37093', '2.32922', '2.29688', '2.29179', '2.27849', '2.29681'] | Gamma1 Grad: ['-0.01442', '0.05586', '0.02540', '0.00354', '-0.01882', '0.02335', '-0.02084', '-0.00941', '0.00179', '-0.00065', '-0.00403', '0.01688', '-0.00585']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.760% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.760% | ğŸ† Best Train Accuracy: 90.760%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.760% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 60 | TrainLossHist: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.81it/s, Test_acc=65, Test_loss=1.76]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.990% | ğŸ† Best Test Accuracy: 64.990%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.07it/s, Train_acc=91.3, Train_loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00022 | Gamma1: ['2.31214', '2.35786', '2.31387', '2.28299', '2.26038', '2.29458', '2.35422', '2.37564', '2.34422', '2.29315', '2.29212', '2.26714', '2.29525'] | Gamma1 Grad: ['-0.01269', '0.00394', '-0.00183', '0.03078', '-0.00840', '-0.02128', '-0.01373', '0.01746', '-0.00119', '0.00304', '-0.00203', '-0.00742', '-0.00881']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.346% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.346% | ğŸ† Best Train Accuracy: 91.346%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.346% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.26it/s, Test_acc=64.3, Test_loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.260% | ğŸ† Best Test Accuracy: 64.990%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.07it/s, Train_acc=91.4, Train_loss=0.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.30733', '2.34992', '2.29734', '2.27257', '2.26677', '2.28863', '2.35431', '2.37036', '2.33978', '2.28708', '2.28506', '2.26921', '2.30442'] | Gamma1 Grad: ['-0.01233', '-0.04625', '0.00490', '-0.01208', '-0.07465', '-0.04166', '-0.04191', '-0.04098', '0.01887', '-0.01018', '-0.01309', '-0.02099', '-0.02728']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.352% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.352% | ğŸ† Best Train Accuracy: 91.352%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.352% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.48it/s, Test_acc=64.5, Test_loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.520% | ğŸ† Best Test Accuracy: 64.990%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.02it/s, Train_acc=91.5, Train_loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00016 | Gamma1: ['2.31735', '2.35643', '2.30623', '2.27931', '2.24118', '2.29573', '2.36295', '2.37825', '2.33202', '2.29931', '2.28663', '2.27454', '2.30269'] | Gamma1 Grad: ['0.04595', '0.02136', '0.04314', '-0.07204', '-0.01258', '0.12603', '-0.10559', '0.02289', '-0.00337', '-0.01586', '-0.00793', '0.00457', '-0.04660']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.500% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.500% | ğŸ† Best Train Accuracy: 91.500%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.500% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.69it/s, Test_acc=64.1, Test_loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.100% | ğŸ† Best Test Accuracy: 64.990%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.81it/s, Train_acc=92, Train_loss=0.279]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['2.33156', '2.34647', '2.30499', '2.27922', '2.24472', '2.29535', '2.35059', '2.36269', '2.34327', '2.30890', '2.29122', '2.29006', '2.30228'] | Gamma1 Grad: ['0.02406', '-0.03732', '0.02188', '-0.00989', '-0.02080', '-0.02722', '-0.01480', '-0.02490', '0.00775', '-0.00429', '-0.01828', '-0.00061', '-0.04195']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.982% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.982% | ğŸ† Best Train Accuracy: 91.982%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.982% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.19it/s, Test_acc=65.5, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 65.490% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.29it/s, Train_acc=91.8, Train_loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00011 | Gamma1: ['2.32227', '2.35519', '2.30596', '2.27060', '2.25629', '2.29180', '2.34879', '2.36609', '2.33792', '2.31537', '2.29154', '2.28598', '2.28050'] | Gamma1 Grad: ['0.02535', '0.03320', '0.04427', '-0.05032', '0.08717', '-0.01636', '0.02757', '0.00803', '0.00167', '-0.00517', '-0.00789', '-0.00270', '-0.00459']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 91.802% | ğŸ† Best Train Accuracy: 91.982%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.982% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.53it/s, Test_acc=64.5, Test_loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.530% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.49it/s, Train_acc=92.1, Train_loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.33725', '2.35227', '2.30462', '2.26285', '2.24935', '2.29609', '2.34369', '2.36037', '2.35278', '2.31986', '2.29442', '2.28924', '2.28428'] | Gamma1 Grad: ['-0.00679', '0.00444', '-0.01497', '0.04543', '0.09348', '0.03477', '0.00858', '-0.05157', '-0.00519', '-0.00297', '-0.01038', '-0.00255', '0.00693']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.142% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.142% | ğŸ† Best Train Accuracy: 92.142%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.142% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.33it/s, Test_acc=64.5, Test_loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.510% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.56it/s, Train_acc=92.3, Train_loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.33509', '2.35730', '2.31022', '2.27188', '2.24438', '2.29830', '2.34515', '2.36023', '2.34842', '2.31014', '2.30200', '2.28848', '2.28487'] | Gamma1 Grad: ['0.04067', '-0.04464', '-0.08311', '0.12738', '0.08544', '-0.01643', '0.03726', '-0.03640', '-0.00258', '0.00163', '0.01288', '0.03459', '0.02534']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.330% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.330% | ğŸ† Best Train Accuracy: 92.330%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.330% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.64it/s, Test_acc=64.5, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.530% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.07it/s, Train_acc=92.6, Train_loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['2.32476', '2.37653', '2.32494', '2.26011', '2.26288', '2.28588', '2.33861', '2.36846', '2.33326', '2.31425', '2.30226', '2.26875', '2.29782'] | Gamma1 Grad: ['0.02506', '0.01404', '0.02631', '0.00105', '0.02664', '0.02758', '-0.03338', '-0.02213', '0.00465', '-0.00441', '-0.01185', '0.02557', '0.00507']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.638% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.638% | ğŸ† Best Train Accuracy: 92.638%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.638% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.00it/s, Test_acc=64.8, Test_loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.750% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.86it/s, Train_acc=92.9, Train_loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['2.34470', '2.34598', '2.30611', '2.26163', '2.22834', '2.27653', '2.34500', '2.36305', '2.32884', '2.30885', '2.30069', '2.28683', '2.29580'] | Gamma1 Grad: ['0.00015', '-0.00236', '-0.00453', '0.00918', '0.02882', '-0.06653', '0.00531', '0.01801', '-0.01023', '0.00373', '-0.00027', '0.02741', '0.00986']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.864% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.864% | ğŸ† Best Train Accuracy: 92.864%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.864% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.11it/s, Test_acc=64.6, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.560% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.35it/s, Train_acc=92.8, Train_loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.34491', '2.35330', '2.29788', '2.25754', '2.24493', '2.28630', '2.34341', '2.37523', '2.33476', '2.30952', '2.29531', '2.28659', '2.30631'] | Gamma1 Grad: ['0.00690', '0.02675', '0.07089', '0.03884', '0.02693', '-0.06396', '-0.00995', '0.00214', '-0.00440', '-0.00139', '0.01072', '0.00359', '0.02672']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 92.820% | ğŸ† Best Train Accuracy: 92.864%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.864% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 70 | TrainLossHist: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.75it/s, Test_acc=64.9, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.910% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.00it/s, Train_acc=92.9, Train_loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.30158', '2.34085', '2.31673', '2.22275', '2.25339', '2.29761', '2.36201', '2.37384', '2.34600', '2.29596', '2.29699', '2.27266', '2.31627'] | Gamma1 Grad: ['-0.01611', '-0.00490', '-0.03706', '-0.04397', '0.00430', '-0.05069', '-0.01577', '0.02991', '0.01163', '0.00649', '0.00292', '0.01206', '0.04904']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.872% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.872% | ğŸ† Best Train Accuracy: 92.872%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.872% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.17it/s, Test_acc=64.5, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.500% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.15it/s, Train_acc=93.3, Train_loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.30754', '2.35840', '2.28665', '2.25950', '2.25864', '2.28301', '2.35129', '2.36239', '2.33389', '2.32413', '2.29037', '2.24530', '2.27806'] | Gamma1 Grad: ['0.00360', '-0.03032', '-0.01815', '0.03634', '0.05853', '-0.07363', '0.02225', '0.00911', '-0.00599', '0.00885', '0.00577', '0.00212', '0.03993']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.294% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.294% | ğŸ† Best Train Accuracy: 93.294%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.294% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.72it/s, Test_acc=64.5, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.490% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.06it/s, Train_acc=93.5, Train_loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.33180', '2.36185', '2.33629', '2.27415', '2.20723', '2.24903', '2.35001', '2.39918', '2.37053', '2.28660', '2.28174', '2.26277', '2.27985'] | Gamma1 Grad: ['-0.01330', '-0.01115', '-0.12844', '-0.00463', '-0.07342', '0.01261', '0.00433', '-0.02433', '-0.01914', '0.02834', '0.01346', '-0.01434', '0.05523']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.450% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.450% | ğŸ† Best Train Accuracy: 93.450%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.450% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.38it/s, Test_acc=64.6, Test_loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.620% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.85it/s, Train_acc=93.4, Train_loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.32367', '2.32031', '2.29142', '2.23873', '2.23693', '2.24120', '2.34276', '2.36137', '2.38328', '2.29961', '2.28479', '2.27354', '2.29268'] | Gamma1 Grad: ['0.00826', '-0.02992', '0.01153', '-0.00433', '0.03433', '0.01449', '-0.00702', '0.03408', '-0.01344', '0.00834', '0.00429', '-0.01289', '0.01553']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.356% | ğŸ† Best Train Accuracy: 93.450%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.450% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.84it/s, Test_acc=64.6, Test_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.560% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.51it/s, Train_acc=93.6, Train_loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.31600', '2.32091', '2.33108', '2.26665', '2.24009', '2.29954', '2.33570', '2.36745', '2.37213', '2.30012', '2.29805', '2.29709', '2.29143'] | Gamma1 Grad: ['-0.01948', '-0.07034', '-0.02129', '-0.00576', '-0.09727', '0.11991', '-0.03286', '0.00330', '0.00091', '0.00802', '0.00761', '0.02960', '0.00225']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.586% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.586% | ğŸ† Best Train Accuracy: 93.586%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.586% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.10it/s, Test_acc=65.4, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.440% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.92it/s, Train_acc=93.9, Train_loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.29724', '2.31624', '2.29456', '2.25566', '2.21804', '2.27077', '2.32711', '2.36295', '2.34935', '2.30096', '2.31573', '2.28788', '2.27930'] | Gamma1 Grad: ['0.00628', '-0.00914', '-0.05219', '-0.01884', '0.04314', '-0.01112', '-0.04400', '-0.04191', '0.00298', '0.00413', '-0.01200', '-0.00588', '0.04107']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.870% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.870% | ğŸ† Best Train Accuracy: 93.870%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.870% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.32it/s, Test_acc=65.1, Test_loss=1.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.100% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.58it/s, Train_acc=93.9, Train_loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00147 | Gamma1: ['2.31859', '2.32062', '2.31444', '2.25967', '2.25297', '2.28367', '2.32668', '2.37469', '2.35359', '2.29521', '2.31363', '2.28376', '2.26234'] | Gamma1 Grad: ['0.01588', '0.02136', '0.01555', '0.00249', '0.01681', '-0.00469', '0.01443', '0.00413', '0.00479', '-0.00441', '-0.00310', '0.00243', '-0.02372']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.860% | ğŸ† Best Train Accuracy: 93.870%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.870% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.62it/s, Test_acc=65, Test_loss=1.9]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.050% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.85it/s, Train_acc=94.1, Train_loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.29015', '2.31321', '2.28226', '2.26435', '2.25808', '2.27001', '2.34248', '2.36226', '2.33710', '2.29311', '2.28534', '2.29041', '2.27112'] | Gamma1 Grad: ['-0.05418', '-0.03252', '0.03159', '-0.04010', '0.06712', '0.00437', '0.06750', '-0.04967', '-0.01202', '-0.00226', '-0.00246', '0.01268', '0.02824']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.136% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.136% | ğŸ† Best Train Accuracy: 94.136%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.136% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.25it/s, Test_acc=65.2, Test_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.180% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.76it/s, Train_acc=94.1, Train_loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00146 | Gamma1: ['2.28864', '2.34702', '2.27613', '2.25611', '2.23600', '2.26070', '2.32402', '2.35987', '2.34741', '2.31080', '2.29102', '2.28262', '2.27828'] | Gamma1 Grad: ['0.00407', '-0.04449', '-0.00898', '-0.00371', '-0.00990', '0.04544', '0.04084', '0.06139', '-0.00200', '-0.00023', '0.00239', '0.02104', '0.03063']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 94.056% | ğŸ† Best Train Accuracy: 94.136%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.136% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.41it/s, Test_acc=65, Test_loss=1.93]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.040% | ğŸ† Best Test Accuracy: 65.490%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.02it/s, Train_acc=96.8, Train_loss=0.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00144 | Gamma1: ['2.30417', '2.33730', '2.32331', '2.27407', '2.23992', '2.27654', '2.33301', '2.34218', '2.32210', '2.32017', '2.31394', '2.29006', '2.29490'] | Gamma1 Grad: ['0.02130', '-0.02906', '-0.01559', '0.08107', '-0.15338', '-0.07458', '-0.06283', '0.08159', '-0.01882', '0.00918', '0.03426', '-0.02631', '0.02667']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 96.808% (Updated)\n",
      "ğŸ“Š Train Accuracy: 96.808% | ğŸ† Best Train Accuracy: 96.808%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 96.808% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 80 | TrainLossHist: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.62it/s, Test_acc=67.3, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.320% | ğŸ† Best Test Accuracy: 67.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.00it/s, Train_acc=97.9, Train_loss=0.099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00143 | Gamma1: ['2.29318', '2.32288', '2.30220', '2.25534', '2.22940', '2.26256', '2.31541', '2.32809', '2.30688', '2.32213', '2.31284', '2.29080', '2.31447'] | Gamma1 Grad: ['0.01926', '0.02245', '0.00859', '0.00849', '0.09185', '0.06715', '0.01333', '0.07012', '0.00820', '0.01277', '0.02619', '-0.00276', '0.02235']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.862% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.862% | ğŸ† Best Train Accuracy: 97.862%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.862% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.47it/s, Test_acc=67.7, Test_loss=1.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.670% | ğŸ† Best Test Accuracy: 67.670%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.01it/s, Train_acc=98.2, Train_loss=0.089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00142 | Gamma1: ['2.28119', '2.31274', '2.27584', '2.25936', '2.23172', '2.27584', '2.29514', '2.31179', '2.31119', '2.32547', '2.32815', '2.28607', '2.32998'] | Gamma1 Grad: ['0.02765', '-0.01345', '0.04247', '0.02523', '0.05589', '-0.04861', '-0.00964', '0.02994', '-0.00484', '-0.00105', '0.00303', '-0.01361', '0.02210']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.194% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.194% | ğŸ† Best Train Accuracy: 98.194%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.194% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.21it/s, Test_acc=67.6, Test_loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.610% | ğŸ† Best Test Accuracy: 67.670%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.58it/s, Train_acc=98.5, Train_loss=0.081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00141 | Gamma1: ['2.30821', '2.32738', '2.27453', '2.25159', '2.24162', '2.27800', '2.29268', '2.33222', '2.31709', '2.31645', '2.32683', '2.29655', '2.32058'] | Gamma1 Grad: ['0.00061', '0.00166', '0.00116', '0.00059', '-0.00017', '-0.00092', '0.00065', '0.00025', '0.00096', '0.00054', '0.00032', '0.00089', '0.00066']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.456% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.456% | ğŸ† Best Train Accuracy: 98.456%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.456% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.45it/s, Test_acc=68, Test_loss=1.86]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.960% | ğŸ† Best Test Accuracy: 67.960%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.32it/s, Train_acc=98.5, Train_loss=0.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00139 | Gamma1: ['2.29653', '2.29735', '2.28279', '2.25533', '2.26327', '2.28843', '2.29984', '2.34263', '2.31473', '2.30925', '2.32772', '2.29313', '2.31998'] | Gamma1 Grad: ['-0.03017', '-0.04093', '0.00411', '-0.05420', '-0.03694', '0.02634', '0.00070', '0.01017', '-0.00816', '-0.00401', '-0.00796', '0.01886', '0.01083']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.496% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.496% | ğŸ† Best Train Accuracy: 98.496%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.496% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.16it/s, Test_acc=67.8, Test_loss=1.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.780% | ğŸ† Best Test Accuracy: 67.960%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.53it/s, Train_acc=98.7, Train_loss=0.073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00138 | Gamma1: ['2.29629', '2.31237', '2.30293', '2.26293', '2.27367', '2.27347', '2.29392', '2.33224', '2.29920', '2.31921', '2.31427', '2.30652', '2.31232'] | Gamma1 Grad: ['0.00402', '0.01473', '-0.00407', '0.00817', '-0.00549', '0.02349', '0.00068', '0.04487', '0.00070', '-0.00348', '-0.00296', '0.00333', '0.00814']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.690% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.690% | ğŸ† Best Train Accuracy: 98.690%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.690% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.65it/s, Test_acc=67.8, Test_loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.780% | ğŸ† Best Test Accuracy: 67.960%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.83it/s, Train_acc=98.7, Train_loss=0.073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00136 | Gamma1: ['2.29597', '2.30463', '2.28530', '2.26616', '2.25899', '2.29926', '2.29496', '2.31231', '2.31856', '2.30819', '2.30959', '2.30617', '2.32475'] | Gamma1 Grad: ['0.00196', '-0.02300', '-0.01494', '0.07482', '-0.01243', '-0.01846', '0.06347', '-0.03870', '0.01099', '0.00201', '0.00260', '0.00153', '-0.03130']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.692% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.692% | ğŸ† Best Train Accuracy: 98.692%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.692% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.32it/s, Test_acc=68.1, Test_loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 68.090% | ğŸ† Best Test Accuracy: 68.090%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.58it/s, Train_acc=98.8, Train_loss=0.069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00134 | Gamma1: ['2.29749', '2.29824', '2.29086', '2.25065', '2.28368', '2.25169', '2.30439', '2.29832', '2.30494', '2.31311', '2.30796', '2.30765', '2.32627'] | Gamma1 Grad: ['-0.00396', '0.01960', '0.02867', '0.01714', '0.03665', '0.03476', '0.02597', '0.04135', '0.00010', '-0.00549', '0.00463', '0.00493', '-0.00515']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.822% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.822% | ğŸ† Best Train Accuracy: 98.822%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.822% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.30it/s, Test_acc=67.6, Test_loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.610% | ğŸ† Best Test Accuracy: 68.090%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.65it/s, Train_acc=98.8, Train_loss=0.069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00133 | Gamma1: ['2.31277', '2.30088', '2.27273', '2.24195', '2.26656', '2.26968', '2.30316', '2.31479', '2.30409', '2.29926', '2.30618', '2.30778', '2.30957'] | Gamma1 Grad: ['0.00243', '0.01055', '0.09197', '0.05522', '0.03080', '0.01869', '-0.05741', '-0.01487', '-0.00035', '0.00191', '-0.00356', '0.00773', '0.00529']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.780% | ğŸ† Best Train Accuracy: 98.822%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.822% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.20it/s, Test_acc=67.8, Test_loss=2.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.790% | ğŸ† Best Test Accuracy: 68.090%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.00it/s, Train_acc=98.9, Train_loss=0.064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00131 | Gamma1: ['2.30001', '2.32115', '2.27406', '2.27995', '2.24621', '2.26364', '2.27927', '2.30403', '2.30582', '2.30667', '2.31938', '2.30323', '2.31542'] | Gamma1 Grad: ['0.01114', '-0.03204', '0.03298', '0.06191', '-0.01659', '-0.01916', '-0.03970', '-0.06876', '0.01022', '0.00406', '-0.01640', '0.00615', '-0.05698']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.938% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.938% | ğŸ† Best Train Accuracy: 98.938%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.938% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.21it/s, Test_acc=67.6, Test_loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.610% | ğŸ† Best Test Accuracy: 68.090%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.42it/s, Train_acc=98.9, Train_loss=0.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00129 | Gamma1: ['2.30298', '2.30319', '2.30070', '2.24049', '2.22439', '2.25520', '2.28519', '2.30523', '2.30465', '2.30208', '2.30872', '2.31190', '2.31946'] | Gamma1 Grad: ['-0.00476', '0.02282', '-0.05452', '-0.01850', '-0.01474', '-0.00368', '-0.01045', '-0.01895', '-0.00214', '-0.00457', '-0.00258', '0.00157', '0.00756']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.926% | ğŸ† Best Train Accuracy: 98.938%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.938% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 90 | TrainLossHist: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 76.86it/s, Test_acc=67.9, Test_loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.930% | ğŸ† Best Test Accuracy: 68.090%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.81it/s, Train_acc=99, Train_loss=0.064]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00127 | Gamma1: ['2.32079', '2.29848', '2.29037', '2.22758', '2.25248', '2.27621', '2.26872', '2.30403', '2.31013', '2.30578', '2.30346', '2.29636', '2.33455'] | Gamma1 Grad: ['0.00487', '0.06229', '0.04961', '-0.03168', '0.00725', '-0.03334', '0.03590', '-0.03195', '-0.00069', '-0.00227', '-0.01148', '0.01006', '-0.00281']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.014% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.014% | ğŸ† Best Train Accuracy: 99.014%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.014% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.69it/s, Test_acc=68, Test_loss=2.08]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.970% | ğŸ† Best Test Accuracy: 68.090%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.92it/s, Train_acc=99.1, Train_loss=0.061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00125 | Gamma1: ['2.29979', '2.29146', '2.27706', '2.24211', '2.26579', '2.27077', '2.28876', '2.29653', '2.30853', '2.29273', '2.30392', '2.29817', '2.33912'] | Gamma1 Grad: ['0.04875', '0.01486', '-0.05827', '-0.05899', '0.07268', '-0.09401', '-0.02572', '0.01715', '0.00840', '0.00709', '-0.01680', '0.04766', '-0.00214']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.072% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.072% | ğŸ† Best Train Accuracy: 99.072%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.072% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.56it/s, Test_acc=67.9, Test_loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.880% | ğŸ† Best Test Accuracy: 68.090%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.98it/s, Train_acc=99.1, Train_loss=0.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00122 | Gamma1: ['2.28346', '2.30092', '2.24557', '2.23393', '2.26330', '2.28493', '2.29929', '2.31113', '2.31600', '2.30860', '2.31409', '2.31241', '2.33084'] | Gamma1 Grad: ['-0.03758', '-0.01850', '-0.01265', '0.06659', '0.10705', '-0.05792', '-0.06607', '-0.03520', '-0.00517', '-0.00411', '-0.01412', '-0.00230', '0.02676']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.072% | ğŸ† Best Train Accuracy: 99.072%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.072% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.62it/s, Test_acc=68.1, Test_loss=2.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 68.140% | ğŸ† Best Test Accuracy: 68.140%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.13it/s, Train_acc=99, Train_loss=0.062]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00120 | Gamma1: ['2.29538', '2.30510', '2.29396', '2.26521', '2.24280', '2.26975', '2.28708', '2.29943', '2.30316', '2.30772', '2.30761', '2.31010', '2.32018'] | Gamma1 Grad: ['-0.00107', '-0.00427', '-0.00251', '0.01107', '0.00438', '-0.00476', '0.00134', '-0.00367', '0.00080', '0.00018', '0.00029', '0.00051', '0.00021']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.992% | ğŸ† Best Train Accuracy: 99.072%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.072% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.93it/s, Test_acc=67.8, Test_loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.800% | ğŸ† Best Test Accuracy: 68.140%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.69it/s, Train_acc=99.1, Train_loss=0.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00118 | Gamma1: ['2.28257', '2.27507', '2.26927', '2.25259', '2.24753', '2.27269', '2.30122', '2.32403', '2.30905', '2.29697', '2.30692', '2.30120', '2.32017'] | Gamma1 Grad: ['-0.05094', '-0.05527', '-0.03142', '-0.05033', '-0.13768', '0.00482', '0.00010', '0.02938', '0.00218', '-0.01104', '-0.01706', '0.00332', '-0.03107']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.090% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.090% | ğŸ† Best Train Accuracy: 99.090%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.090% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.99it/s, Test_acc=68.3, Test_loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 68.320% | ğŸ† Best Test Accuracy: 68.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.66it/s, Train_acc=99.1, Train_loss=0.058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00115 | Gamma1: ['2.30329', '2.30398', '2.27765', '2.26050', '2.24023', '2.28104', '2.30242', '2.30451', '2.29644', '2.29871', '2.32207', '2.30656', '2.34179'] | Gamma1 Grad: ['-0.01444', '-0.02589', '0.00379', '0.01592', '-0.00612', '-0.00430', '-0.01277', '0.01065', '0.00696', '0.00069', '0.00624', '-0.00313', '-0.00910']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.140% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.140% | ğŸ† Best Train Accuracy: 99.140%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.140% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.19it/s, Test_acc=68.3, Test_loss=2.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.260% | ğŸ† Best Test Accuracy: 68.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.72it/s, Train_acc=99.2, Train_loss=0.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00113 | Gamma1: ['2.30469', '2.31778', '2.28175', '2.24543', '2.22182', '2.29124', '2.29664', '2.30647', '2.30359', '2.29582', '2.30856', '2.31237', '2.33535'] | Gamma1 Grad: ['-0.00171', '0.00127', '-0.00810', '-0.00858', '-0.00516', '0.00126', '0.01918', '-0.01383', '-0.00073', '-0.00060', '-0.00394', '0.00262', '-0.00559']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.208% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.208% | ğŸ† Best Train Accuracy: 99.208%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.208% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.07it/s, Test_acc=68.4, Test_loss=2.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 68.440% | ğŸ† Best Test Accuracy: 68.440%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.52it/s, Train_acc=99.2, Train_loss=0.055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00110 | Gamma1: ['2.30836', '2.29961', '2.30278', '2.24619', '2.25773', '2.27141', '2.28614', '2.30147', '2.30764', '2.29936', '2.30376', '2.30602', '2.32306'] | Gamma1 Grad: ['0.02509', '0.00817', '0.04508', '0.00496', '-0.00464', '-0.04557', '0.01040', '-0.02786', '-0.00194', '-0.00445', '-0.00172', '0.00094', '-0.00317']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.238% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.238% | ğŸ† Best Train Accuracy: 99.238%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.238% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.28it/s, Test_acc=67.9, Test_loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.920% | ğŸ† Best Test Accuracy: 68.440%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.14it/s, Train_acc=99.2, Train_loss=0.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00108 | Gamma1: ['2.28731', '2.29999', '2.27418', '2.23760', '2.24964', '2.27236', '2.28374', '2.31769', '2.30555', '2.30132', '2.29724', '2.29810', '2.33327'] | Gamma1 Grad: ['0.01989', '0.02974', '-0.00104', '0.03572', '0.01457', '-0.01299', '0.00521', '-0.05680', '-0.00152', '0.00274', '0.00150', '0.00429', '-0.02246']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.224% | ğŸ† Best Train Accuracy: 99.238%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.238% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.40it/s, Test_acc=67.7, Test_loss=2.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.710% | ğŸ† Best Test Accuracy: 68.440%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveNoise-No_AdaptiveNoise\\Results\\CIFAR100_Test_no_noise_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "Best Test Accuracy:  68.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 11. TRAIN MODEL WITH SHEDULAR | XXX ----------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Set Seed for Reproducibility BEFORE training starts\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)  \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "# âœ… Training Loop\n",
    "num_epochs = 100 # Example: Set the total number of epochs\n",
    "for epoch in range(start_epoch, num_epochs):   # Runs training for 100 epochs\n",
    "\n",
    "    train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler, WARMUP_ACTIVATION_EPOCHS) # âœ… Pass required arguments\n",
    "\n",
    "    test(epoch)  # âœ… Test the model\n",
    "    tqdm.write(\"\")  # âœ… Clear leftover progress bar from test()\n",
    "\n",
    "\n",
    "print(\"Best Test Accuracy: \", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9b45e-d27d-430d-af41-c68854a7c029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'âœ… Annotated comparison plots with BEST accuracy markers saved to ./Results/Plots'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 550x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 13.  ADAPTIVE REGULARIZATION VS NOADAPTIVE REGULARIZATION | XXX ------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.patheffects as path_effects\n",
    "import os\n",
    "\n",
    "def read_test_log(file_path):\n",
    "    test_loss_history = []\n",
    "    test_acc_history = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if \"Test Loss\" in line and \"Test Acc\" in line:\n",
    "                try:\n",
    "                    loss = float(line.split(\"Test Loss:\")[1].split(\"|\")[0].strip())\n",
    "                    acc = float(line.split(\"Test Acc:\")[1].split(\"%\")[0].strip())\n",
    "                    test_loss_history.append(loss)\n",
    "                    test_acc_history.append(acc)\n",
    "                except:\n",
    "                    continue\n",
    "    return test_loss_history, test_acc_history\n",
    "\n",
    "def plot_train_test_metrics(save_dir=\"./Results/Plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    NoisePenalty_test_log_path = f'./Results_AdaptiveNoise/CIFAR100_Test_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "    noNoisePenalty_test_log_path = f'./Results/CIFAR100_Test_{noise_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "\n",
    "    NoisePenalty_test_loss, NoisePenalty_test_acc = read_test_log(NoisePenalty_test_log_path)\n",
    "    noNoisePenalty_test_loss, noNoisePenalty_test_acc = read_test_log(noNoisePenalty_test_log_path)\n",
    "\n",
    "    num_epochs = min(len(NoisePenalty_test_loss), len(noNoisePenalty_test_loss))\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    COLOR_SCALE = ['#00295B', '#CF0A66']  # NoisePenalty, noNoisePenalty\n",
    "    rcParams.update({\n",
    "        \"font.size\": 11,\n",
    "        \"axes.titlesize\": 11,\n",
    "        \"axes.labelsize\": 13,\n",
    "        \"xtick.labelsize\": 11,\n",
    "        \"ytick.labelsize\": 11,\n",
    "        \"axes.labelweight\": \"bold\",\n",
    "        \"xtick.color\": \"black\",\n",
    "        \"ytick.color\": \"black\",\n",
    "    })\n",
    "\n",
    "    # Custom settings\n",
    "    custom_yticks_test_loss = [1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
    "    custom_yticks_test_acc = [10, 20, 30, 40, 50, 60, 70]\n",
    "    custom_xticks = [0, 20, 40, 60, 80, 100]\n",
    "    custom_yaxis_test_loss = [1.2, 4.2]\n",
    "    custom_yaxis_test_acc = [35, 72]\n",
    "    custom_xaxis = [0, 105]\n",
    "\n",
    "    # Offsets\n",
    "    y_offset_loss_tp = 0.2\n",
    "    y_offset_loss_ntp = 0.07\n",
    "    x_offset_loss_tp = 3.5\n",
    "    x_offset_loss_ntp = 3.5\n",
    "\n",
    "    y_offset_acc_tp = 1\n",
    "    y_offset_acc_ntp = 3.2\n",
    "    x_offset_acc_tp = 8.5\n",
    "    x_offset_acc_ntp = 6.5\n",
    "\n",
    "    # ğŸ”· Plot Test Loss\n",
    "    fig1, ax1 = plt.subplots(figsize=(5.5, 3.5))\n",
    "    ax1.plot(epochs, NoisePenalty_test_loss[:num_epochs], label=\"Adaptive Noise Penalty\", color=COLOR_SCALE[0], linewidth=2)\n",
    "    ax1.plot(epochs, noNoisePenalty_test_loss[:num_epochs], label=\"No Adaptive Noise Penalty\", color=COLOR_SCALE[1], linestyle='--', linewidth=2)\n",
    "    ax1.set_xlabel(\"Epoch\", fontweight='bold')\n",
    "    ax1.set_ylabel(\"Test Loss\", fontweight='bold')\n",
    "    ax1.set_xticks(custom_xticks)\n",
    "    ax1.set_yticks(custom_yticks_test_loss)\n",
    "    ax1.set_xlim(custom_xaxis)\n",
    "    ax1.set_ylim(custom_yaxis_test_loss)\n",
    "    ax1.tick_params(axis='x', width=1.5)\n",
    "    ax1.tick_params(axis='y', width=1.5)\n",
    "    for label in ax1.get_xticklabels() + ax1.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    leg1 = ax1.legend(fontsize='small', loc=\"upper right\")\n",
    "    for text in leg1.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Add final loss markers\n",
    "    ax1.plot(epochs[-1], NoisePenalty_test_loss[-1], marker='o', color=COLOR_SCALE[0], markersize=4)\n",
    "    ax1.text(epochs[-1] - x_offset_loss_tp, NoisePenalty_test_loss[-1] - y_offset_loss_tp,\n",
    "             f\"{NoisePenalty_test_loss[-1]:.2f}\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "    ax1.plot(epochs[-1], noNoisePenalty_test_loss[-1], marker='o', color=COLOR_SCALE[1], markersize=4)\n",
    "    ax1.text(epochs[-1] - x_offset_loss_ntp, noNoisePenalty_test_loss[-1] + y_offset_loss_ntp,\n",
    "             f\"{noNoisePenalty_test_loss[-1]:.2f}\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"compare_test_loss_noisepenalty_vs_no_noisepenalty.svg\"),\n",
    "                format='svg', transparent=True, bbox_inches='tight')\n",
    "    plt.close(fig1)\n",
    "\n",
    "    # ğŸ”¶ Plot Test Accuracy\n",
    "    best_epoch_tp = NoisePenalty_test_acc.index(max(NoisePenalty_test_acc)) + 1\n",
    "    best_acc_tp = max(NoisePenalty_test_acc)\n",
    "    best_epoch_ntp = noNoisePenalty_test_acc.index(max(noNoisePenalty_test_acc)) + 1\n",
    "    best_acc_ntp = max(noNoisePenalty_test_acc)\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(5.5, 3.5))\n",
    "    ax2.plot(epochs, NoisePenalty_test_acc[:num_epochs], label=\"Adaptive Noise Penalty\", color=COLOR_SCALE[0], linewidth=2)\n",
    "    ax2.plot(epochs, noNoisePenalty_test_acc[:num_epochs], label=\"No Adaptive Noise Penalty\", color=COLOR_SCALE[1], linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel(\"Epoch\", fontweight='bold')\n",
    "    ax2.set_ylabel(\"Test Accuracy (%)\", fontweight='bold')\n",
    "    ax2.set_xticks(custom_xticks)\n",
    "    ax2.set_yticks(custom_yticks_test_acc)\n",
    "    ax2.set_xlim(custom_xaxis)\n",
    "    ax2.set_ylim(custom_yaxis_test_acc)\n",
    "    ax2.tick_params(axis='x', width=1.5)\n",
    "    ax2.tick_params(axis='y', width=1.5)\n",
    "    for label in ax2.get_xticklabels() + ax2.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    leg2 = ax2.legend(fontsize='small', loc=\"lower right\")\n",
    "    for text in leg2.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Markers for best accuracy\n",
    "    ax2.plot(best_epoch_tp, best_acc_tp - 0.21, marker='o', color=COLOR_SCALE[0], markersize=5.5, markeredgecolor='black', markeredgewidth=1)\n",
    "    ax2.text(best_epoch_tp - x_offset_acc_tp, best_acc_tp + y_offset_acc_tp,\n",
    "             f\"{best_acc_tp:.2f}%\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "    ax2.plot(best_epoch_ntp, best_acc_ntp - 0.4, marker='o', color=COLOR_SCALE[1], markersize=5.5, markeredgecolor='black', markeredgewidth=1)\n",
    "    ax2.text(best_epoch_ntp - x_offset_acc_ntp, best_acc_ntp - y_offset_acc_ntp,\n",
    "             f\"{best_acc_ntp:.2f}%\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"compare_test_accuracy_noisepenalty_vs_no_noisepenalty.svg\"),\n",
    "                format='svg', transparent=True, bbox_inches='tight')\n",
    "    # plt.close(fig2)\n",
    "\n",
    "    return f\"âœ… Annotated comparison plots with BEST accuracy markers saved to {save_dir}\"\n",
    "\n",
    "# ğŸ”· Call the function\n",
    "plot_train_test_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
