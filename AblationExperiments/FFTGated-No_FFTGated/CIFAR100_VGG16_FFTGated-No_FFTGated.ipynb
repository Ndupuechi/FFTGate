{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to pytorch_env (Python 3.10.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55662c16-24b1-4450-ac59-1f85826ef260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Current working directory: C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\n",
      "âœ… sys.path updated:\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\python310.zip\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\DLLs\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\n",
      "   ğŸ“‚ \n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\Pythonwin\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\models\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\activation\n",
      "âœ… FFTGate imported successfully!\n",
      "âœ… FFTGate instance created successfully!\n",
      "âœ… FFTGate_VGG imported successfully!\n",
      "CIFAR100 Training Script Initialized...\n",
      "Using device: cuda\n",
      "Parsed learning rate: 0.001 (type: <class 'float'>)\n",
      "Formatted learning rate for filenames: 0_001\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Length of training dataset: 50000\n",
      "Length of testing dataset: 10000\n",
      "Number of classes in CIFAR-100: 100\n",
      "==> Building model..\n",
      "âœ… Found 13 FFTGate layers.\n",
      "âœ… Collected 13 trainable activation parameters.\n",
      "   ğŸ”¹ Layer 0: FFTGate()\n",
      "   ğŸ”¹ Layer 1: FFTGate()\n",
      "   ğŸ”¹ Layer 2: FFTGate()\n",
      "   ğŸ”¹ Layer 3: FFTGate()\n",
      "   ğŸ”¹ Layer 4: FFTGate()\n",
      "   ğŸ”¹ Layer 5: FFTGate()\n",
      "   ğŸ”¹ Layer 6: FFTGate()\n",
      "   ğŸ”¹ Layer 7: FFTGate()\n",
      "   ğŸ”¹ Layer 8: FFTGate()\n",
      "   ğŸ”¹ Layer 9: FFTGate()\n",
      "   ğŸ”¹ Layer 10: FFTGate()\n",
      "   ğŸ”¹ Layer 11: FFTGate()\n",
      "   ğŸ”¹ Layer 12: FFTGate()\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 1.A. IMPORTS LIBRARIES | XXX -----------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\"\"\"Train CIFAR100 with PyTorch.\"\"\"\n",
    "\n",
    "# Python 2/3 compatibility\n",
    "# from __future__ import print_function\n",
    "\n",
    "\n",
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# PyTorch and related modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# torchvision for datasets and transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch_optimizer as torch_opt  # Use 'torch_opt' for torch_optimizer\n",
    "from timm.scheduler import CosineLRScheduler \n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Define currect working directory to ensure on right directory\n",
    "VGG16_PATH = r\"C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\"\n",
    "if os.getcwd() != VGG16_PATH:\n",
    "    os.chdir(VGG16_PATH)\n",
    "print(f\"âœ… Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# âœ… Define absolute paths\n",
    "PROJECT_PATH = VGG16_PATH\n",
    "MODELS_PATH = os.path.join(VGG16_PATH, \"models\")\n",
    "ACTIVATION_PATH = os.path.join(VGG16_PATH, \"activation\")\n",
    "# PAU_PATH = os.path.join(VGG16_PATH, \"pau\")\n",
    "\n",
    "# âœ… Ensure necessary paths are in sys.path\n",
    "for path in [PROJECT_PATH, MODELS_PATH, ACTIVATION_PATH]:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "# âœ… Print updated sys.path for debugging\n",
    "print(\"âœ… sys.path updated:\")\n",
    "for path in sys.path:\n",
    "    print(\"   ğŸ“‚\", path)\n",
    "\n",
    "# âœ… Import FFTGate (Check if the module exists)\n",
    "try:\n",
    "    from activation.FFTGate import FFTGate  # type: ignore\n",
    "    print(\"âœ… FFTGate imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ Import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'Activation4.py' exists inside: {ACTIVATION_PATH}\")\n",
    "\n",
    "# âœ… Test if FFTGate is callable\n",
    "try:\n",
    "    activation_test = FFTGate()\n",
    "    print(\"âœ… FFTGate instance created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error while initializing FFTGate: {e}\")\n",
    "\n",
    "# âœ… Now import FFTGate_VGG (Ensure module exists inside models/)\n",
    "try:\n",
    "    from models.FFTGate_VGG import FFTGate_VGG  # type: ignore\n",
    "    print(\"âœ… FFTGate_VGG imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ FFTGate_VGG import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'FFTGate_VGG.py' exists inside: {MODELS_PATH}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 1.B. SEEDING FOR REPRODUCIBILITY | XXX -------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def set_seed_torch(seed):\n",
    "    torch.manual_seed(seed)                          \n",
    "\n",
    "\n",
    "\n",
    "def set_seed_main(seed):\n",
    "    random.seed(seed)                                ## Python's random module\n",
    "    np.random.seed(seed)                             ## NumPy's random module\n",
    "    torch.cuda.manual_seed(seed)                     ## PyTorch's random module for CUDA\n",
    "    torch.cuda.manual_seed_all(seed)                 ## Seed for all CUDA devices\n",
    "    torch.backends.cudnn.deterministic = True        ## Ensure deterministic behavior for CuDNN\n",
    "    torch.backends.cudnn.benchmark = False           ## Disable CuDNN's autotuning for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)   \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (Optional) Import Optimizers - Uncomment as needed\n",
    "# from Opt import opt\n",
    "# from diffGrad import diffGrad\n",
    "# from diffRGrad import diffRGrad, SdiffRGrad, BetaDiffRGrad, Beta12DiffRGrad, BetaDFCDiffRGrad\n",
    "# from RADAM import Radam, BetaRadam\n",
    "# from BetaAdam import BetaAdam, BetaAdam1, BetaAdam2, BetaAdam3, BetaAdam4, BetaAdam5, BetaAdam6, BetaAdam7, BetaAdam4A\n",
    "# from AdamRM import AdamRM, AdamRM1, AdamRM2, AdamRM3, AdamRM4, AdamRM5\n",
    "# from sadam import sadam\n",
    "# from SdiffGrad import SdiffGrad\n",
    "# from SRADAM import SRADAM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 2. DEFINE MODEL Lr | XXX ---------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Main Execution (Placeholder)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"CIFAR100 Training Script Initialized...\")\n",
    "    # Add your training pipeline here\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Argument parser to get user inputs\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR100 Training')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "\n",
    "args, unknown = parser.parse_known_args()  # Avoids Jupyter argument issues\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Ensure lr is correctly parsed\n",
    "lr = args.lr  # Get learning rate from argparse\n",
    "lr_str = str(lr).replace('.', '_')  # Convert to string and replace '.' for filenames\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Parsed learning rate: {lr} (type: {type(lr)})\")\n",
    "print(f\"Formatted learning rate for filenames: {lr_str}\")\n",
    "\n",
    "# Initialize training variables\n",
    "best_acc = 0  # Best test accuracy\n",
    "start_epoch = 0  # Start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 3. LOAD DATASET | XXX ------------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "bs = 64 #set batch size\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Length of train and test datasets\n",
    "len_train = len(trainset)\n",
    "len_test = len(testset)\n",
    "print(f\"Length of training dataset: {len_train}\")\n",
    "print(f\"Length of testing dataset: {len_test}\")\n",
    "\n",
    "# âœ… Print number of classes\n",
    "num_classes_Print = len(trainset.classes)\n",
    "print(f\"Number of classes in CIFAR-100: {num_classes_Print}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 4. DYNAMIC REGULARIZATION| XXX ---------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "def apply_dynamic_regularization(inputs, feature_activations, epoch,\n",
    "                                  prev_params, layer_index_map, batch_idx):\n",
    "\n",
    "\n",
    "    global activation_layers  # âœ… Reference already-collected layers\n",
    "\n",
    "    # âœ… Print gamma1 stats early in training for monitoring\n",
    "    if batch_idx == 0 and epoch <= 4:\n",
    "        print(f\"\\nğŸš¨ ENTERED apply_dynamic_regularization | Epoch={epoch} | Batch={batch_idx}\", flush=True)\n",
    "\n",
    "        # ğŸ§  Print gamma1 details\n",
    "        all_layer_info = []\n",
    "        for idx, layer in enumerate(activation_layers):\n",
    "            param = getattr(layer, \"gamma1\")\n",
    "            all_layer_info.append(f\"Layer {idx}: ID={id(param)} | Mean={param.mean().item():.5f}\")\n",
    "        print(\"ğŸ§  GAMMA1 INFO:\", \" | \".join(all_layer_info), flush=True)\n",
    "\n",
    "    # âœ… Initialize gamma1 regularization accumulator\n",
    "    gamma1_reg = 0.0\n",
    "\n",
    "    # âœ… Compute batch std and define regularization strength\n",
    "    batch_std = torch.std(inputs) + 1e-6\n",
    "    regularization_strength = 0.05 if epoch < 40 else (0.01 if epoch < 60 else 0.005)\n",
    "\n",
    "    # âœ… Track layers where noise is injected (informative)\n",
    "    noisy_layers = []\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        if idx not in layer_index_map:\n",
    "            continue\n",
    "\n",
    "        prev_layer_params = prev_params[layer_index_map[idx]]\n",
    "        param_name = \"gamma1\"\n",
    "        param = getattr(layer, param_name)\n",
    "        prev_param = prev_layer_params[param_name]\n",
    "\n",
    "        # âœ… Target based on input stats\n",
    "        target = compute_target(param_name, batch_std)\n",
    "\n",
    "        # âœ… Adaptive Target Regularization\n",
    "        gamma1_reg += regularization_strength * (param - target).pow(2).mean() * 1.2\n",
    "\n",
    "        # âœ… Adaptive Cohesion Regularization\n",
    "        cohesion = (param - prev_param).pow(2)\n",
    "        gamma1_reg += 0.005 * cohesion.mean()\n",
    "\n",
    "        # âœ… Adaptive Noise Regularization\n",
    "        epoch_AddNoise = 50\n",
    "        if epoch > epoch_AddNoise:\n",
    "            param_variation = torch.abs(param - prev_param).mean()\n",
    "            if param_variation < 0.015:\n",
    "                noise = (0.001 + 0.0004 * batch_std.item()) * torch.randn_like(param)\n",
    "                penalty = (param - (prev_param + noise)).pow(2).sum()\n",
    "                gamma1_reg += 0.00015 * penalty\n",
    "                noisy_layers.append(f\"{idx} (Î”={param_variation.item():.5f})\") # Collect index and variation\n",
    "\n",
    "    # âœ… Print noise injection summary\n",
    "    if batch_idx == 0 and epoch <= (epoch_AddNoise + 4) and noisy_layers:\n",
    "        print(f\"ğŸ”¥ Stable Noise Injected | Epoch {epoch} | Batch {batch_idx} | Layers: \" + \", \".join(noisy_layers), flush=True)\n",
    "    mags = feature_activations.abs().mean(dim=(0, 2, 3))\n",
    "    m = mags / mags.sum()\n",
    "    gamma1_reg += 0.005 * (-(m * torch.log(m + 1e-6)).sum())\n",
    "\n",
    "    return gamma1_reg\n",
    "\n",
    "\n",
    "def compute_target(param_name, batch_std):\n",
    "    if param_name == \"gamma1\":\n",
    "        return 2.0 + 0.2 * batch_std.item()  \n",
    "\n",
    "    raise ValueError(f\"Unknown param {param_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 5. INITIALIZE MODEL | XXX --------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "#net = Elliott_VGG('VGG16'); net1 = 'Elliott_VGG16'\n",
    "#net = GELU_MobileNet(); net1 = 'GELU_MobileNet'\n",
    "#net = GELU_SENet18(); net1 = 'GELU_SENet18'\n",
    "#net = PDELU_ResNet50(); net1 = 'PDELU_ResNet50'\n",
    "# net = Sigmoid_GoogLeNet(); net1 = 'Sigmoid_GoogLeNet'\n",
    "#net = GELU_DenseNet121(); net1 = 'GELU_DenseNet121'\n",
    "# net = ReLU_VGG('VGG16'); net1 = 'ReLU_VGG16'\n",
    "net = FFTGate_VGG('VGG16'); net1 = 'FFTGate_VGG16'\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9); optimizer1 = 'SGDM5'\n",
    "#optimizer = optim.Adagrad(net.parameters()); optimizer1 = 'AdaGrad'\n",
    "#optimizer = optim.Adadelta(net.parameters()); optimizer1 = 'AdaDelta'\n",
    "#optimizer = optim.RMSprop(net.parameters()); optimizer1 = 'RMSprop'\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr); optimizer1 = 'Adam'\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr, amsgrad=True); optimizer1 = 'amsgrad'\n",
    "#optimizer = diffGrad(net.parameters(), lr=args.lr); optimizer1 = 'diffGrad'\n",
    "#optimizer = Radam(net.parameters(), lr=args.lr); optimizer1 = 'Radam'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 6. INITIALIZE ACTIVATION PARAMETERS, OPTIMIZERS & SCHEDULERS | XXX ---------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# âœ… Step 1: Collect Activation Parameters from ALL Layers (Ensure Compatibility with DataParallel)\n",
    "if isinstance(net, torch.nn.DataParallel):\n",
    "    features = net.module.features\n",
    "else:\n",
    "    features = net.features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 2: Recursively search for FFTGate layers\n",
    "activation_params = []\n",
    "activation_layers = []\n",
    "\n",
    "for layer in features:\n",
    "    if isinstance(layer, FFTGate):  \n",
    "        activation_layers.append(layer)\n",
    "        activation_params.append(layer.gamma1)  # âœ… Only gamma1 is trainable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 3: Define Unfreeze Epoch\n",
    "unfreeze_activation_epoch = 1  # âœ… Change this value if needed\n",
    "# unfreeze_activation_epoch = 10  # âœ… Delay unfreezing until epoch 10\n",
    "\n",
    "\n",
    "# âœ… Define the warm-up epoch value\n",
    "# WARMUP_ACTIVATION_EPOCHS = 5  # The number of epochs for warm-up\n",
    "WARMUP_ACTIVATION_EPOCHS = 0  # The number of epochs for warm-up\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initially Freeze Activation Parameters\n",
    "for param in activation_params:\n",
    "    param.requires_grad = False  # ğŸš« Keep frozen before the unfreeze epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initialize Activation Optimizers (Using AdamW for Better Weight Decay)\n",
    "activation_optimizers = {\n",
    "    \"gamma1\": torch.optim.AdamW(activation_params, lr=0.0015, weight_decay=1e-6)  # ğŸ”º Reduce LR from 0.005 â†’ 0.0025\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 5: Initialize Activation Schedulers with Warm Restarts (Per Parameter Type)\n",
    "activation_schedulers = {\n",
    "    \"gamma1\": CosineAnnealingWarmRestarts(\n",
    "        activation_optimizers[\"gamma1\"],\n",
    "        T_0=10,      # Shorter cycle to explore aggressively\n",
    "        T_mult=2,    # Increase cycle length gradually\n",
    "        eta_min=5e-5  # âœ… recommended safer modification\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 6: Print collected activation layers and parameters\n",
    "if activation_layers and activation_params:\n",
    "    print(f\"âœ… Found {len(activation_layers)} FFTGate layers.\")\n",
    "    print(f\"âœ… Collected {len(activation_params)} trainable activation parameters.\")\n",
    "    \n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        print(f\"   ğŸ”¹ Layer {idx}: {layer}\")\n",
    "\n",
    "elif activation_layers and not activation_params:\n",
    "    print(f\"âš  Warning: Found {len(activation_layers)} FFTGate layers, but no trainable parameters were collected.\")\n",
    "\n",
    "elif activation_params and not activation_layers:\n",
    "    print(f\"âš  Warning: Collected {len(activation_params)} activation parameters, but no FFTGate layers were recorded.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš  Warning: No FFTGate layers or activation parameters found! Skipping activation optimizer.\")\n",
    "    activation_optimizers = None\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 7. INITIALIZE MAIN OPTIMIZER SCHEDULER | XXX -------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Step 6: Define MultiStepLR for Main Optimizer\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "main_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 8. MODEL CHECK POINT | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists('checkpoint'):\n",
    "    os.makedirs('checkpoint')\n",
    "\n",
    "if not os.path.exists('Results'):\n",
    "    os.makedirs('Results')\n",
    "\n",
    "# Construct checkpoint path\n",
    "checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr}_{net1}_{optimizer1}.t7'\n",
    "\n",
    "# Resume checkpoint only if file exists\n",
    "if args.resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 9. DEFINE TRAIN LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Used for naming files \n",
    "gate_mode = \"no_FFT\"  # Options: \"FFT\", \"no_FFT\", \"disable\"\n",
    "\n",
    "# Training\n",
    "\n",
    "def train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler , WARMUP_ACTIVATION_EPOCHS):\n",
    "    global train_loss_history, best_train_acc, prev_params, recent_test_acc, gamma1_history, activation_layers, test_acc_history, train_acc_history, gate_mode  # ğŸŸ¢ğŸŸ¢ğŸŸ¢\n",
    "\n",
    "    if epoch == 0:\n",
    "        train_loss_history = []\n",
    "        train_acc_history = []\n",
    "        best_train_acc = 0.0\n",
    "        recent_test_acc = 0.0\n",
    "        gamma1_history = {}         # âœ… Initialize history\n",
    "        test_acc_history = []       # âœ… test accuracy history\n",
    "\n",
    "\n",
    "\n",
    "    prev_params = {}\n",
    "    layer_index_map = {idx: idx for idx in range(len(activation_layers))}  \n",
    "\n",
    "    # âœ… Cache previous gamma1 values from activation layers\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_accuracy = 0.0\n",
    "\n",
    "    # âœ… Initialize log history\n",
    "    log_history = []\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Define path to store Training log\n",
    "    save_paths = {\n",
    "       \n",
    "        \"log_history\": f\"C:\\\\Users\\\\emeka\\\\Research\\\\ModelCUDA\\\\Big_Data_Journal\\\\Comparison\\\\Code\\\\Paper\\\\github2\\\\AblationExperiments\\\\FFTGated-No_FFTGated\\\\Results\\\\FFTGate\\\\FFTGate_training_logs.txt\"  # âœ… Training log_history \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 1: Unfreeze Activation Parameters (Only Once Per Epoch)\n",
    "    if epoch == unfreeze_activation_epoch:\n",
    "        print(\"\\nğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\")\n",
    "        for layer in net.module.features if isinstance(net, torch.nn.DataParallel) else net.features:\n",
    "            if isinstance(layer, FFTGate):   \n",
    "                layer.gamma1.requires_grad = True  # âœ… Only gamma1 is trainable\n",
    "        print(\"âœ… Activation Parameters Unfrozen! ğŸš€\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 2: Gradual Warm-up for Activation Learning Rates (AFTER Unfreezing)\n",
    "    warmup_start = unfreeze_activation_epoch  # ğŸ”¹ Start warm-up when unfreezing happens\n",
    "    warmup_end = unfreeze_activation_epoch + WARMUP_ACTIVATION_EPOCHS  # ğŸ”¹ End warm-up period\n",
    "\n",
    "    # âœ… Adjust learning rates **only** during the warm-up phase\n",
    "    if warmup_start <= epoch < warmup_end:\n",
    "        warmup_factor = (epoch - warmup_start + 1) / WARMUP_ACTIVATION_EPOCHS  \n",
    "\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            for param_group in act_scheduler.optimizer.param_groups:\n",
    "                if \"initial_lr\" not in param_group:\n",
    "                    param_group[\"initial_lr\"] = param_group[\"lr\"]  # ğŸ”¹ Store initial LR\n",
    "                param_group[\"lr\"] = param_group[\"initial_lr\"] * warmup_factor  # ğŸ”¹ Scale LR\n",
    "\n",
    "        # âœ… Debugging output to track warm-up process\n",
    "        print(f\"ğŸ”¥ Warm-up Epoch {epoch}: Scaling LR by {warmup_factor:.3f}\")\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            print(f\"  ğŸ”¹ {name} LR: {act_scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    activation_history = []  # ğŸ”´ Initialize empty history at start of epoch (outside batch loop)\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Training Loop\n",
    "    with tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch}\") as progress:\n",
    "        for batch_idx, (inputs, targets) in progress:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # zero_grad activation parameter\n",
    "            for opt in activation_optimizers.values():\n",
    "                opt.zero_grad()\n",
    "\n",
    "\n",
    "            # âœ… Forward Pass\n",
    "            outputs = net(inputs, epoch=epoch, train_accuracy=train_accuracy, targets=targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            \n",
    "            feature_activations = features(inputs)  # Feature activations\n",
    "\n",
    "\n",
    "            # âœ… Collect Activation History | âœ… Per-layer mean activations\n",
    "            batch_means = [layer.saved_output.mean().item() for layer in activation_layers]\n",
    "            activation_history.extend(batch_means)\n",
    "\n",
    "            # âœ… Apply Decay strategy to history for each activation layer\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    if isinstance(layer, FFTGate):\n",
    "                        layer.decay_spectral_history(epoch, num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Compute Training Accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            train_accuracy = 100. * correct / total if total > 0 else 0.0  # Compute training accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Call Regularization Function for the Activation Parameter\n",
    "            if epoch > 0:\n",
    "                gamma1_reg = apply_dynamic_regularization(\n",
    "                    inputs, feature_activations, epoch,\n",
    "                    prev_params, layer_index_map, batch_idx\n",
    "                )\n",
    "                loss += gamma1_reg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… ğŸ¯ Adaptive Gradient Clipping of gamma1  \n",
    "            for layer in features:\n",
    "                if isinstance(layer, FFTGate):  # âœ… Ensure layer has gamma1 before clipping\n",
    "                    torch.nn.utils.clip_grad_norm_([layer.gamma1], max_norm=0.7)\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Apply Optimizer Step for Model Parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # âœ… Apply Optimizer Steps for Activation Parameters (Only if Unfrozen)\n",
    "            if epoch >= unfreeze_activation_epoch:\n",
    "                for opt in activation_optimizers.values():\n",
    "                    opt.step()\n",
    "\n",
    "\n",
    "            # âœ… Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Clamping of gamma1 (Applied AFTER Optimizer Step)\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    layer.gamma1.clamp_(0.1, 6.0)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Update progress bar\n",
    "            progress.set_postfix(Train_loss=round(train_loss / (batch_idx + 1), 3),\n",
    "                                 Train_acc=train_accuracy)  \n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step the main optimizer scheduler (ONLY for model parameters)\n",
    "    main_scheduler.step()\n",
    "\n",
    "    # âœ… Step the activation parameter schedulers (ONLY for activation parameters) | Epoch-wise stepping\n",
    "    if epoch >= unfreeze_activation_epoch:\n",
    "        for name, act_scheduler in activation_schedulers.items():  \n",
    "            act_scheduler.step()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… ONLY update prev_params here AFTER all updates | âœ… Update prev_params AFTER training epoch\n",
    "    for idx, layer in enumerate(activation_layers):      \n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Logging Activation Parameters & Gradients\n",
    "    last_batch_grads = {\"Gamma1 Grad\": []}\n",
    "    current_params = {\"Gamma1\": []}\n",
    "\n",
    "    for layer in features:\n",
    "        if isinstance(layer, FFTGate):  \n",
    "            # âœ… Convert gradients to scalar floats and format to 5 decimal places (removes device='cuda:0' and tensor(...))\n",
    "            last_batch_grads[\"Gamma1 Grad\"].append(f\"{layer.gamma1.grad.item():.5f}\" if layer.gamma1.grad is not None else \"None\")\n",
    "\n",
    "            # âœ… Collect current parameter values (already scalar), formatted to 5 decimal places\n",
    "            current_params[\"Gamma1\"].append(f\"{layer.gamma1.item():.5f}\")\n",
    "\n",
    "    # âœ… Build log message (showing params and gradients for ALL layers)\n",
    "    log_msg = (\n",
    "        f\"Epoch {epoch}: M_Optimizer LR => {optimizer.param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1 LR => {activation_optimizers['gamma1'].param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1: {current_params['Gamma1']} | \"\n",
    "        f\"Gamma1 Grad: {last_batch_grads['Gamma1 Grad']}\"\n",
    "    )\n",
    "\n",
    "    log_history.append(log_msg)\n",
    "    print(log_msg)  # âœ… Prints only once per epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (Clear old logs)\n",
    "    if epoch == 0:  # âœ… Only clear at the start of training\n",
    "        with open(save_paths[\"log_history\"], \"w\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Save logs once per epoch (Append new logs)\n",
    "    if log_history:\n",
    "        with open(save_paths[\"log_history\"], \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_history) + \"\\n\")         # âœ… Ensure each entry is on a new line\n",
    "        print(f\"ğŸ“œ Logs saved to {save_paths['log_history']}!\")  # âœ… Only prints once per epoch\n",
    "    else:\n",
    "        print(\"âš  No logs to save!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final training accuracy for the epoch\n",
    "    final_train_loss = train_loss / len(trainloader)\n",
    "    final_train_acc = 100. * correct / total\n",
    "\n",
    "    # âœ… Append to history\n",
    "    train_loss_history.append(final_train_loss)\n",
    "\n",
    "    # Append per-epoch training accuracy\n",
    "    train_acc_history.append(final_train_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save training results (without affecting best accuracy tracking)\n",
    "    train_results_path = f'./Results/CIFAR100_Train_{gate_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "    # âœ… Clear the log file at the start of training (Epoch 0)\n",
    "    if epoch == 0 and os.path.exists(train_results_path):\n",
    "        with open(train_results_path, 'w') as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs only once\n",
    "\n",
    "    # âœ… Append new training results for each epoch\n",
    "    with open(train_results_path, 'a') as f:\n",
    "        f.write(f\"Epoch {epoch} | Train Loss: {final_train_loss:.3f} | Train Acc: {final_train_acc:.3f}%\\n\")\n",
    "\n",
    "    if final_train_acc > best_train_acc:\n",
    "        best_train_acc = final_train_acc  # âœ… Update best training accuracy\n",
    "        print(f\"ğŸ† New Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "    # âœ… Append the best training accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):  # Only log once at the final epoch\n",
    "        with open(train_results_path, 'a') as f:\n",
    "            f.write(f\"\\nğŸ† Best Training Accuracy: {best_train_acc:.3f}%\\n\")  \n",
    "\n",
    "    # âœ… Print both Final and Best Training Accuracy\n",
    "    print(f\"ğŸ“Š Train Accuracy: {final_train_acc:.3f}% | ğŸ† Best Train Accuracy: {best_train_acc:.3f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"ğŸ“œ Training logs saved to {train_results_path}!\")\n",
    "    print(f\"ğŸ† Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"ğŸ“ Sizes â†’ ActivationHist: {len(activation_history)} | TestAccHist: {len(test_acc_history)} | TrainLossHist: {len(train_loss_history)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # return final_train_loss, final_train_acc, feature_activations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 10. DEFINE TEST LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(epoch, save_results=True):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and optionally saves the results.\n",
    "    \n",
    "    Args:\n",
    "    - epoch (int): The current epoch number.\n",
    "    - save_results (bool): Whether to save results to a file.\n",
    "\n",
    "    Returns:\n",
    "    - acc (float): Test accuracy percentage.\n",
    "    \"\"\"\n",
    "    global best_acc, val_accuracy, gate_mode  \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # âœ… Ensure activation function parameters are clamped before evaluation\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(testloader), total=len(testloader), desc=f\"Testing Epoch {epoch}\") as progress:\n",
    "            for batch_idx, (inputs, targets) in progress:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                # âœ… Pass validation accuracy to activation function\n",
    "                val_accuracy = 100. * correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "                # âœ… Update progress bar with loss & accuracy\n",
    "                progress.set_postfix(Test_loss=round(test_loss / (batch_idx + 1), 3),\n",
    "                                     Test_acc=round(val_accuracy, 3))\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final test accuracy\n",
    "    final_test_loss = test_loss / len(testloader)\n",
    "    final_test_acc = 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Ensure \"Results\" folder exists (just like training logs)\n",
    "    results_dir = os.path.join(PROJECT_PATH, \"Results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # âœ… Define log file path for test results\n",
    "    test_results_path = os.path.join(results_dir, f'CIFAR100_Test_{gate_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt')\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (clear old logs)\n",
    "    if epoch == 0:\n",
    "        with open(test_results_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Append new test results for each epoch (same style as training)\n",
    "    with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Epoch {epoch} | Test Loss: {final_test_loss:.3f} | Test Acc: {final_test_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save checkpoint if accuracy improves (does NOT interfere with logging)\n",
    "    if final_test_acc > best_acc:\n",
    "        print('ğŸ† Saving best model...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': final_test_acc,  # âœ… Ensures the best test accuracy is saved in checkpoint\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Ensure checkpoint directory exists\n",
    "        checkpoint_dir = \"checkpoint\"\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "        # âœ… Format learning rate properly before saving filename\n",
    "        lr_str = str(lr).replace('.', '_')\n",
    "        checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr_str}_{net1}_{optimizer1}.t7'\n",
    "        torch.save(state, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "        best_acc = final_test_acc  # âœ… Update best accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Append the best test accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):\n",
    "        with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\nğŸ† Best Test Accuracy: {best_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Print both Final and Best Test Accuracy (always executed)\n",
    "    print(f\"ğŸ“Š Test Accuracy: {final_test_acc:.3f}% | ğŸ† Best Test Accuracy: {best_acc:.3f}%\")\n",
    "    print(f\"ğŸ“œ Test logs saved to {test_results_path}!\")\n",
    "\n",
    "\n",
    "    global recent_test_acc\n",
    "    recent_test_acc = final_test_acc  # Capture latest test accuracy for next train() call | Store latest test accuracy\n",
    "\n",
    "    test_acc_history.append(final_test_acc)\n",
    "\n",
    "    return final_test_acc  # âœ… Return the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4121f-375f-4b9f-84ce-feb80a798598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:23<00:00, 32.62it/s, Train_acc=3.34, Train_loss=4.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000'] | Gamma1 Grad: ['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 3.344% (Updated)\n",
      "ğŸ“Š Train Accuracy: 3.344% | ğŸ† Best Train Accuracy: 3.344%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 3.344% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.86it/s, Test_acc=4.84, Test_loss=4.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 4.840% | ğŸ† Best Test Accuracy: 4.840%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "\n",
      "ğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\n",
      "âœ… Activation Parameters Unfrozen! ğŸš€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=1 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1706590335472 | Mean=1.50000 | Layer 1: ID=1706558360688 | Mean=1.50000 | Layer 2: ID=1706558364368 | Mean=1.50000 | Layer 3: ID=1706558363488 | Mean=1.50000 | Layer 4: ID=1706558362528 | Mean=1.50000 | Layer 5: ID=1706558361408 | Mean=1.50000 | Layer 6: ID=1706558364608 | Mean=1.50000 | Layer 7: ID=1707190420320 | Mean=1.50000 | Layer 8: ID=1707190421280 | Mean=1.50000 | Layer 9: ID=1707190422240 | Mean=1.50000 | Layer 10: ID=1707190423200 | Mean=1.50000 | Layer 11: ID=1707190424080 | Mean=1.50000 | Layer 12: ID=1707190424960 | Mean=1.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.16it/s, Train_acc=6.4, Train_loss=4.09] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.22877', '2.23382', '2.23213', '2.22852', '2.22703', '2.22764', '2.22539', '2.22260', '2.22283', '2.22699', '2.22155', '2.22091', '2.21137'] | Gamma1 Grad: ['0.00442', '-0.00031', '-0.00601', '-0.00122', '-0.00444', '-0.00393', '-0.00143', '-0.00565', '-0.01676', '-0.00943', '-0.01106', '-0.01930', '-0.02150']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 6.400% (Updated)\n",
      "ğŸ“Š Train Accuracy: 6.400% | ğŸ† Best Train Accuracy: 6.400%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 6.400% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.92it/s, Test_acc=8.05, Test_loss=3.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 8.050% | ğŸ† Best Test Accuracy: 8.050%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=2 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1706590335472 | Mean=2.22877 | Layer 1: ID=1706558360688 | Mean=2.23382 | Layer 2: ID=1706558364368 | Mean=2.23213 | Layer 3: ID=1706558363488 | Mean=2.22852 | Layer 4: ID=1706558362528 | Mean=2.22703 | Layer 5: ID=1706558361408 | Mean=2.22764 | Layer 6: ID=1706558364608 | Mean=2.22539 | Layer 7: ID=1707190420320 | Mean=2.22260 | Layer 8: ID=1707190421280 | Mean=2.22283 | Layer 9: ID=1707190422240 | Mean=2.22699 | Layer 10: ID=1707190423200 | Mean=2.22155 | Layer 11: ID=1707190424080 | Mean=2.22091 | Layer 12: ID=1707190424960 | Mean=2.21137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.96it/s, Train_acc=10.1, Train_loss=3.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.29320', '2.30092', '2.29607', '2.28499', '2.28320', '2.28398', '2.28155', '2.28344', '2.27965', '2.28457', '2.28107', '2.27690', '2.27087'] | Gamma1 Grad: ['-0.01173', '-0.01529', '-0.00073', '-0.01383', '-0.01073', '-0.00367', '-0.01074', '-0.00466', '-0.00627', '-0.00891', '-0.00441', '0.01163', '-0.01404']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 10.106% (Updated)\n",
      "ğŸ“Š Train Accuracy: 10.106% | ğŸ† Best Train Accuracy: 10.106%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 10.106% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.10it/s, Test_acc=11.1, Test_loss=3.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 11.110% | ğŸ† Best Test Accuracy: 11.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=3 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1706590335472 | Mean=2.29320 | Layer 1: ID=1706558360688 | Mean=2.30092 | Layer 2: ID=1706558364368 | Mean=2.29607 | Layer 3: ID=1706558363488 | Mean=2.28499 | Layer 4: ID=1706558362528 | Mean=2.28320 | Layer 5: ID=1706558361408 | Mean=2.28398 | Layer 6: ID=1706558364608 | Mean=2.28155 | Layer 7: ID=1707190420320 | Mean=2.28344 | Layer 8: ID=1707190421280 | Mean=2.27965 | Layer 9: ID=1707190422240 | Mean=2.28457 | Layer 10: ID=1707190423200 | Mean=2.28107 | Layer 11: ID=1707190424080 | Mean=2.27690 | Layer 12: ID=1707190424960 | Mean=2.27087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.26it/s, Train_acc=14.1, Train_loss=3.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.30761', '2.31503', '2.31461', '2.29196', '2.29241', '2.29127', '2.28944', '2.29018', '2.28880', '2.28579', '2.28810', '2.27643', '2.27499'] | Gamma1 Grad: ['-0.02470', '-0.00262', '-0.01795', '0.00095', '-0.00718', '-0.01034', '-0.00493', '-0.00758', '-0.00279', '-0.01305', '-0.00201', '0.00554', '-0.03750']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 14.114% (Updated)\n",
      "ğŸ“Š Train Accuracy: 14.114% | ğŸ† Best Train Accuracy: 14.114%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 14.114% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.08it/s, Test_acc=16.8, Test_loss=3.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 16.810% | ğŸ† Best Test Accuracy: 16.810%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=4 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1706590335472 | Mean=2.30761 | Layer 1: ID=1706558360688 | Mean=2.31503 | Layer 2: ID=1706558364368 | Mean=2.31461 | Layer 3: ID=1706558363488 | Mean=2.29196 | Layer 4: ID=1706558362528 | Mean=2.29241 | Layer 5: ID=1706558361408 | Mean=2.29127 | Layer 6: ID=1706558364608 | Mean=2.28944 | Layer 7: ID=1707190420320 | Mean=2.29018 | Layer 8: ID=1707190421280 | Mean=2.28880 | Layer 9: ID=1707190422240 | Mean=2.28579 | Layer 10: ID=1707190423200 | Mean=2.28810 | Layer 11: ID=1707190424080 | Mean=2.27643 | Layer 12: ID=1707190424960 | Mean=2.27499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.55it/s, Train_acc=19.2, Train_loss=3.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.30896', '2.31813', '2.30743', '2.29942', '2.29603', '2.29561', '2.29270', '2.29644', '2.29247', '2.29027', '2.28864', '2.27553', '2.28516'] | Gamma1 Grad: ['0.00192', '-0.00707', '0.00721', '0.00287', '-0.00142', '0.00004', '0.00448', '0.00080', '0.00184', '0.00738', '-0.00110', '-0.03998', '0.02662']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 19.222% (Updated)\n",
      "ğŸ“Š Train Accuracy: 19.222% | ğŸ† Best Train Accuracy: 19.222%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 19.222% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.67it/s, Test_acc=22.5, Test_loss=2.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 22.530% | ğŸ† Best Test Accuracy: 22.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.24it/s, Train_acc=24.5, Train_loss=2.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.31167', '2.31957', '2.31564', '2.29846', '2.30505', '2.29974', '2.29377', '2.29588', '2.28806', '2.29550', '2.28758', '2.28642', '2.27936'] | Gamma1 Grad: ['0.00696', '0.02171', '0.01513', '-0.00012', '0.00105', '0.00495', '0.00697', '0.00279', '0.00228', '-0.00478', '-0.00473', '0.01472', '-0.00225']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 24.486% (Updated)\n",
      "ğŸ“Š Train Accuracy: 24.486% | ğŸ† Best Train Accuracy: 24.486%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 24.486% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.89it/s, Test_acc=28.3, Test_loss=2.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 28.270% | ğŸ† Best Test Accuracy: 28.270%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.92it/s, Train_acc=29, Train_loss=2.67]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.31508', '2.32317', '2.31695', '2.29950', '2.30213', '2.30099', '2.28620', '2.29653', '2.29807', '2.28988', '2.28725', '2.28044', '2.28505'] | Gamma1 Grad: ['-0.01144', '0.01072', '-0.00742', '-0.01138', '0.00491', '0.01069', '-0.00212', '-0.00420', '0.00324', '0.00780', '-0.00252', '-0.00909', '0.00882']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 28.998% (Updated)\n",
      "ğŸ“Š Train Accuracy: 28.998% | ğŸ† Best Train Accuracy: 28.998%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 28.998% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.95it/s, Test_acc=30.7, Test_loss=2.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 30.700% | ğŸ† Best Test Accuracy: 30.700%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.89it/s, Train_acc=33.2, Train_loss=2.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.31498', '2.31607', '2.31566', '2.29386', '2.29918', '2.29948', '2.29108', '2.29317', '2.28997', '2.29180', '2.28833', '2.28624', '2.29180'] | Gamma1 Grad: ['0.00919', '0.01570', '-0.00514', '-0.01033', '0.01654', '0.00360', '-0.01058', '-0.00949', '-0.02399', '-0.00957', '-0.00002', '-0.00090', '0.12610']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 33.166% (Updated)\n",
      "ğŸ“Š Train Accuracy: 33.166% | ğŸ† Best Train Accuracy: 33.166%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 33.166% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.57it/s, Test_acc=34.8, Test_loss=2.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 34.840% | ğŸ† Best Test Accuracy: 34.840%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.39it/s, Train_acc=37, Train_loss=2.31]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.31022', '2.32869', '2.31153', '2.29181', '2.30458', '2.29838', '2.29108', '2.30158', '2.29821', '2.29700', '2.29426', '2.29021', '2.29550'] | Gamma1 Grad: ['0.00438', '-0.02609', '-0.01039', '0.00291', '0.01131', '0.01196', '0.00581', '0.00208', '0.01564', '0.00617', '-0.00617', '0.02199', '0.03064']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 37.026% (Updated)\n",
      "ğŸ“Š Train Accuracy: 37.026% | ğŸ† Best Train Accuracy: 37.026%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 37.026% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.23it/s, Test_acc=38.6, Test_loss=2.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 38.610% | ğŸ† Best Test Accuracy: 38.610%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.81it/s, Train_acc=40.4, Train_loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.31566', '2.31471', '2.31294', '2.29928', '2.29572', '2.30051', '2.28819', '2.30080', '2.29587', '2.29657', '2.29141', '2.27693', '2.29508'] | Gamma1 Grad: ['-0.04175', '-0.04704', '-0.03737', '-0.01007', '-0.00688', '0.02193', '0.00762', '0.01339', '0.00923', '0.00490', '-0.00008', '-0.02298', '-0.01488']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 40.368% (Updated)\n",
      "ğŸ“Š Train Accuracy: 40.368% | ğŸ† Best Train Accuracy: 40.368%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 40.368% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.14it/s, Test_acc=41.7, Test_loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 41.700% | ğŸ† Best Test Accuracy: 41.700%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.14it/s, Train_acc=43.6, Train_loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.31809', '2.32098', '2.31772', '2.29889', '2.30761', '2.30122', '2.29015', '2.29590', '2.29794', '2.29514', '2.29578', '2.28582', '2.29722'] | Gamma1 Grad: ['-0.01337', '-0.00727', '0.00701', '-0.00371', '0.02967', '-0.00682', '0.01550', '0.00237', '0.00791', '0.00018', '0.00081', '-0.00705', '0.01443']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 43.586% (Updated)\n",
      "ğŸ“Š Train Accuracy: 43.586% | ğŸ† Best Train Accuracy: 43.586%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 43.586% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 10 | TrainLossHist: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.88it/s, Test_acc=44.1, Test_loss=2.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 44.150% | ğŸ† Best Test Accuracy: 44.150%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.73it/s, Train_acc=46.1, Train_loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.31535', '2.31008', '2.32053', '2.29365', '2.30869', '2.30200', '2.29387', '2.29996', '2.29809', '2.29872', '2.29450', '2.28994', '2.29690'] | Gamma1 Grad: ['0.00820', '0.02291', '-0.01461', '0.02158', '0.00235', '-0.00313', '0.00409', '-0.00401', '0.02313', '0.01083', '0.00454', '-0.01280', '0.02064']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 46.110% (Updated)\n",
      "ğŸ“Š Train Accuracy: 46.110% | ğŸ† Best Train Accuracy: 46.110%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 46.110% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 78.73it/s, Test_acc=46.9, Test_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 46.900% | ğŸ† Best Test Accuracy: 46.900%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.79it/s, Train_acc=48.7, Train_loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.30407', '2.32064', '2.30559', '2.29297', '2.31286', '2.30362', '2.28884', '2.28635', '2.29119', '2.28017', '2.28468', '2.29578', '2.29920'] | Gamma1 Grad: ['-0.00384', '0.00882', '0.02202', '-0.00644', '0.00819', '-0.00439', '0.00262', '-0.00731', '0.00233', '0.00864', '-0.00152', '-0.00260', '0.10529']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 48.716% (Updated)\n",
      "ğŸ“Š Train Accuracy: 48.716% | ğŸ† Best Train Accuracy: 48.716%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 48.716% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.74it/s, Test_acc=48.2, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 48.190% | ğŸ† Best Test Accuracy: 48.190%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.56it/s, Train_acc=50.8, Train_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.32410', '2.32293', '2.31034', '2.29536', '2.30655', '2.29437', '2.30599', '2.30473', '2.28360', '2.29426', '2.28544', '2.28838', '2.31053'] | Gamma1 Grad: ['0.01494', '-0.00837', '-0.01471', '0.00552', '-0.02092', '-0.01073', '-0.00119', '-0.00703', '-0.00235', '0.00779', '-0.00896', '-0.01000', '0.04530']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 50.822% (Updated)\n",
      "ğŸ“Š Train Accuracy: 50.822% | ğŸ† Best Train Accuracy: 50.822%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 50.822% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.48it/s, Test_acc=51.2, Test_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 51.200% | ğŸ† Best Test Accuracy: 51.200%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.10it/s, Train_acc=53.2, Train_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.31527', '2.32061', '2.31157', '2.29230', '2.31047', '2.30462', '2.29330', '2.31132', '2.27970', '2.29465', '2.29701', '2.28503', '2.29096'] | Gamma1 Grad: ['0.01208', '-0.00884', '0.02105', '-0.00675', '-0.01744', '0.00113', '0.00426', '-0.00672', '0.01520', '-0.00118', '0.00099', '0.02754', '-0.03027']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 53.206% (Updated)\n",
      "ğŸ“Š Train Accuracy: 53.206% | ğŸ† Best Train Accuracy: 53.206%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 53.206% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.37it/s, Test_acc=51.6, Test_loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 51.580% | ğŸ† Best Test Accuracy: 51.580%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.97it/s, Train_acc=55, Train_loss=1.59]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['2.29775', '2.30002', '2.29914', '2.31162', '2.29994', '2.30861', '2.29755', '2.31056', '2.31011', '2.29397', '2.30375', '2.28505', '2.29822'] | Gamma1 Grad: ['0.03626', '0.02230', '0.04040', '0.02568', '-0.06498', '-0.00087', '-0.01722', '-0.00759', '0.01631', '-0.00106', '-0.00031', '-0.05171', '0.05531']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 55.050% (Updated)\n",
      "ğŸ“Š Train Accuracy: 55.050% | ğŸ† Best Train Accuracy: 55.050%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 55.050% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.92it/s, Test_acc=51.4, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 51.390% | ğŸ† Best Test Accuracy: 51.580%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.99it/s, Train_acc=56.9, Train_loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.31081', '2.31299', '2.30185', '2.28863', '2.29562', '2.30863', '2.28945', '2.29356', '2.29471', '2.28345', '2.29191', '2.27354', '2.29966'] | Gamma1 Grad: ['0.02188', '-0.00524', '0.05281', '0.02713', '-0.01936', '0.02876', '0.00056', '-0.02003', '0.02719', '0.00350', '0.00038', '0.02431', '0.00373']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 56.904% (Updated)\n",
      "ğŸ“Š Train Accuracy: 56.904% | ğŸ† Best Train Accuracy: 56.904%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 56.904% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.15it/s, Test_acc=54, Test_loss=1.66]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 54.010% | ğŸ† Best Test Accuracy: 54.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.00it/s, Train_acc=58.7, Train_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['2.31932', '2.31875', '2.29767', '2.30314', '2.31674', '2.31613', '2.29178', '2.30679', '2.29660', '2.28990', '2.28156', '2.28827', '2.31070'] | Gamma1 Grad: ['-0.00203', '0.03661', '-0.00815', '-0.00925', '0.01773', '-0.01003', '-0.00306', '0.00018', '0.00144', '-0.00779', '-0.00290', '0.01063', '0.01282']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 58.664% (Updated)\n",
      "ğŸ“Š Train Accuracy: 58.664% | ğŸ† Best Train Accuracy: 58.664%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 58.664% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.99it/s, Test_acc=54.1, Test_loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 54.150% | ğŸ† Best Test Accuracy: 54.150%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.22it/s, Train_acc=60.3, Train_loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.31258', '2.31803', '2.30115', '2.29991', '2.30655', '2.32124', '2.30689', '2.30871', '2.29767', '2.29539', '2.29078', '2.29414', '2.30943'] | Gamma1 Grad: ['0.00203', '-0.00627', '-0.02548', '-0.01025', '-0.03078', '0.01931', '-0.01621', '-0.00527', '-0.02892', '-0.01247', '0.00820', '-0.01083', '0.04274']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 60.318% (Updated)\n",
      "ğŸ“Š Train Accuracy: 60.318% | ğŸ† Best Train Accuracy: 60.318%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 60.318% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.09it/s, Test_acc=56.3, Test_loss=1.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 56.330% | ğŸ† Best Test Accuracy: 56.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.20it/s, Train_acc=61.7, Train_loss=1.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['2.31569', '2.32302', '2.29661', '2.30004', '2.30470', '2.31113', '2.30210', '2.30001', '2.29899', '2.29230', '2.29409', '2.29644', '2.30474'] | Gamma1 Grad: ['-0.02667', '0.01245', '-0.00081', '0.00796', '0.00105', '-0.00987', '-0.00521', '0.00579', '-0.00608', '-0.00616', '0.00751', '-0.01921', '-0.03911']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 61.722% (Updated)\n",
      "ğŸ“Š Train Accuracy: 61.722% | ğŸ† Best Train Accuracy: 61.722%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 61.722% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.03it/s, Test_acc=56.7, Test_loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 56.670% | ğŸ† Best Test Accuracy: 56.670%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.62it/s, Train_acc=63.1, Train_loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.30786', '2.31187', '2.30417', '2.28776', '2.28895', '2.31373', '2.28999', '2.29183', '2.29287', '2.28434', '2.29186', '2.28658', '2.31402'] | Gamma1 Grad: ['-0.06440', '-0.03173', '-0.07868', '0.00197', '-0.04235', '0.01584', '0.03076', '0.00192', '0.01964', '0.00345', '0.00881', '-0.04458', '-0.00018']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 63.062% (Updated)\n",
      "ğŸ“Š Train Accuracy: 63.062% | ğŸ† Best Train Accuracy: 63.062%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 63.062% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 20 | TrainLossHist: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.42it/s, Test_acc=57.7, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 57.710% | ğŸ† Best Test Accuracy: 57.710%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.52it/s, Train_acc=64.7, Train_loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['2.31042', '2.31675', '2.30996', '2.29392', '2.29375', '2.31928', '2.30552', '2.30609', '2.30017', '2.28981', '2.29047', '2.29086', '2.29900'] | Gamma1 Grad: ['-0.02719', '0.02569', '0.00794', '-0.02657', '0.01992', '0.01331', '-0.01984', '-0.00506', '-0.00491', '0.00428', '-0.00056', '0.00042', '-0.02695']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 64.724% (Updated)\n",
      "ğŸ“Š Train Accuracy: 64.724% | ğŸ† Best Train Accuracy: 64.724%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 64.724% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 93.18it/s, Test_acc=57.7, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 57.710% | ğŸ† Best Test Accuracy: 57.710%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.54it/s, Train_acc=65.9, Train_loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.31575', '2.31610', '2.30371', '2.30202', '2.29583', '2.32363', '2.30733', '2.30175', '2.29294', '2.29400', '2.28871', '2.28730', '2.30984'] | Gamma1 Grad: ['-0.02321', '0.01022', '0.03109', '-0.00536', '0.01303', '0.00824', '0.01354', '0.00689', '0.01409', '0.00248', '0.00024', '0.01451', '-0.02497']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 65.908% (Updated)\n",
      "ğŸ“Š Train Accuracy: 65.908% | ğŸ† Best Train Accuracy: 65.908%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 65.908% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.60it/s, Test_acc=59.3, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 59.260% | ğŸ† Best Test Accuracy: 59.260%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.48it/s, Train_acc=67.2, Train_loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.30508', '2.30164', '2.29607', '2.29432', '2.29948', '2.30491', '2.30158', '2.30887', '2.29441', '2.29518', '2.29694', '2.29000', '2.31092'] | Gamma1 Grad: ['-0.03196', '0.01455', '-0.04484', '-0.00025', '-0.03860', '0.02745', '-0.00929', '0.00759', '0.01144', '-0.01413', '-0.01134', '0.02373', '0.05744']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 67.194% (Updated)\n",
      "ğŸ“Š Train Accuracy: 67.194% | ğŸ† Best Train Accuracy: 67.194%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 67.194% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.05it/s, Test_acc=59.1, Test_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 59.080% | ğŸ† Best Test Accuracy: 59.260%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.72it/s, Train_acc=68, Train_loss=1.09]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.30573', '2.29875', '2.30393', '2.29033', '2.31577', '2.30695', '2.30069', '2.30334', '2.30289', '2.29678', '2.29136', '2.29737', '2.30733'] | Gamma1 Grad: ['-0.02773', '0.03910', '0.01885', '0.03611', '-0.02893', '-0.01952', '0.00572', '-0.00201', '-0.00881', '0.01671', '0.00783', '0.00020', '-0.04427']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 67.996% (Updated)\n",
      "ğŸ“Š Train Accuracy: 67.996% | ğŸ† Best Train Accuracy: 67.996%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 67.996% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.40it/s, Test_acc=59.3, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 59.260% | ğŸ† Best Test Accuracy: 59.260%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.04it/s, Train_acc=69.3, Train_loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['2.30718', '2.29869', '2.30204', '2.28848', '2.29947', '2.31334', '2.30978', '2.30423', '2.29957', '2.29402', '2.29256', '2.29000', '2.31315'] | Gamma1 Grad: ['-0.02446', '-0.01319', '0.00027', '-0.02855', '0.01689', '-0.00952', '0.00595', '0.00833', '-0.00239', '-0.00367', '-0.00525', '0.01162', '-0.02228']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 69.346% (Updated)\n",
      "ğŸ“Š Train Accuracy: 69.346% | ğŸ† Best Train Accuracy: 69.346%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 69.346% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.15it/s, Test_acc=59.8, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 59.780% | ğŸ† Best Test Accuracy: 59.780%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.75it/s, Train_acc=70.8, Train_loss=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.30309', '2.31285', '2.30964', '2.29989', '2.30650', '2.31822', '2.31572', '2.30363', '2.30773', '2.28877', '2.29814', '2.29080', '2.30100'] | Gamma1 Grad: ['-0.01041', '-0.07157', '0.03981', '-0.07144', '0.03320', '-0.01182', '0.00737', '0.02518', '-0.01402', '0.00517', '-0.01987', '0.01016', '-0.00222']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 70.816% (Updated)\n",
      "ğŸ“Š Train Accuracy: 70.816% | ğŸ† Best Train Accuracy: 70.816%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 70.816% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.03it/s, Test_acc=59.8, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 59.810% | ğŸ† Best Test Accuracy: 59.810%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.61it/s, Train_acc=71.7, Train_loss=0.963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['2.30051', '2.31606', '2.29880', '2.30090', '2.30251', '2.31270', '2.30165', '2.30809', '2.30023', '2.29901', '2.29256', '2.28280', '2.29936'] | Gamma1 Grad: ['-0.04575', '0.01591', '-0.06942', '0.01374', '-0.08315', '0.00896', '-0.01899', '-0.01877', '0.05307', '0.00863', '0.01763', '-0.00657', '0.13379']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 71.650% (Updated)\n",
      "ğŸ“Š Train Accuracy: 71.650% | ğŸ† Best Train Accuracy: 71.650%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 71.650% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.22it/s, Test_acc=61, Test_loss=1.49]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.960% | ğŸ† Best Test Accuracy: 60.960%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.98it/s, Train_acc=72.9, Train_loss=0.926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.30556', '2.31292', '2.30365', '2.29208', '2.29914', '2.31002', '2.31012', '2.31253', '2.29217', '2.29744', '2.29464', '2.29188', '2.30555'] | Gamma1 Grad: ['-0.02336', '0.03035', '0.02815', '-0.03653', '0.05443', '-0.03503', '-0.00158', '-0.00638', '0.01121', '-0.01273', '0.00133', '0.00622', '-0.06304']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 72.854% (Updated)\n",
      "ğŸ“Š Train Accuracy: 72.854% | ğŸ† Best Train Accuracy: 72.854%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 72.854% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.14it/s, Test_acc=61.1, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.130% | ğŸ† Best Test Accuracy: 61.130%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.75it/s, Train_acc=73.8, Train_loss=0.889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['2.31016', '2.30820', '2.29825', '2.29184', '2.29502', '2.30520', '2.30488', '2.30560', '2.30160', '2.29228', '2.29472', '2.29107', '2.31011'] | Gamma1 Grad: ['0.01533', '0.02881', '0.04916', '0.00920', '-0.00105', '0.03036', '0.02472', '0.00713', '0.00313', '-0.00422', '-0.00949', '-0.00576', '0.08816']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 73.778% (Updated)\n",
      "ğŸ“Š Train Accuracy: 73.778% | ğŸ† Best Train Accuracy: 73.778%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 73.778% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.54it/s, Test_acc=60.6, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 60.610% | ğŸ† Best Test Accuracy: 61.130%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.33it/s, Train_acc=74.5, Train_loss=0.865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.30490', '2.30814', '2.30368', '2.30003', '2.30043', '2.31612', '2.30773', '2.31355', '2.29725', '2.29657', '2.29578', '2.28859', '2.30802'] | Gamma1 Grad: ['0.00817', '0.02264', '0.00920', '-0.02320', '0.01774', '0.02445', '0.02159', '0.01085', '0.00855', '-0.00683', '-0.00027', '-0.01273', '-0.01526']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 74.464% (Updated)\n",
      "ğŸ“Š Train Accuracy: 74.464% | ğŸ† Best Train Accuracy: 74.464%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 74.464% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 30 | TrainLossHist: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.20it/s, Test_acc=60.7, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 60.740% | ğŸ† Best Test Accuracy: 61.130%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.44it/s, Train_acc=75.6, Train_loss=0.825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.32049', '2.31780', '2.31554', '2.30566', '2.29761', '2.30536', '2.30398', '2.30721', '2.29613', '2.29486', '2.29877', '2.29939', '2.30667'] | Gamma1 Grad: ['0.04494', '0.02016', '0.02623', '-0.00649', '0.01235', '-0.00328', '-0.00730', '-0.01351', '0.02052', '0.00198', '-0.00003', '0.03247', '-0.06993']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 75.640% (Updated)\n",
      "ğŸ“Š Train Accuracy: 75.640% | ğŸ† Best Train Accuracy: 75.640%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 75.640% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.28it/s, Test_acc=60.6, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 60.560% | ğŸ† Best Test Accuracy: 61.130%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.62it/s, Train_acc=76.6, Train_loss=0.793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.30813', '2.31271', '2.30729', '2.30077', '2.28975', '2.31114', '2.30035', '2.29996', '2.29930', '2.29355', '2.29389', '2.28590', '2.30326'] | Gamma1 Grad: ['-0.00148', '-0.03438', '0.00859', '-0.04080', '0.00528', '0.05615', '-0.02555', '0.01840', '-0.00148', '0.00306', '-0.00553', '-0.04073', '-0.00783']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 76.626% (Updated)\n",
      "ğŸ“Š Train Accuracy: 76.626% | ğŸ† Best Train Accuracy: 76.626%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 76.626% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.37it/s, Test_acc=61.6, Test_loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.640% | ğŸ† Best Test Accuracy: 61.640%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.64it/s, Train_acc=76.8, Train_loss=0.774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.30465', '2.31290', '2.31425', '2.29115', '2.30211', '2.29747', '2.29687', '2.31025', '2.29505', '2.30016', '2.29150', '2.29065', '2.31425'] | Gamma1 Grad: ['-0.01055', '0.00308', '0.00066', '0.00102', '0.00026', '-0.01814', '0.02389', '0.00365', '-0.01105', '0.01103', '-0.00365', '-0.00827', '-0.04519']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 76.842% (Updated)\n",
      "ğŸ“Š Train Accuracy: 76.842% | ğŸ† Best Train Accuracy: 76.842%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 76.842% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.81it/s, Test_acc=62.4, Test_loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.350% | ğŸ† Best Test Accuracy: 62.350%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.29it/s, Train_acc=78.1, Train_loss=0.739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.31175', '2.31876', '2.29083', '2.28511', '2.30022', '2.30901', '2.30092', '2.30242', '2.30845', '2.29868', '2.28827', '2.29867', '2.29330'] | Gamma1 Grad: ['0.07630', '0.02733', '0.01170', '-0.02188', '-0.03262', '-0.01399', '-0.00435', '0.01424', '0.00960', '-0.00776', '0.00672', '-0.01475', '0.06522']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 78.144% (Updated)\n",
      "ğŸ“Š Train Accuracy: 78.144% | ğŸ† Best Train Accuracy: 78.144%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 78.144% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.44it/s, Test_acc=62.1, Test_loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.080% | ğŸ† Best Test Accuracy: 62.350%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.55it/s, Train_acc=78.8, Train_loss=0.714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00144 | Gamma1: ['2.30284', '2.30503', '2.29330', '2.29703', '2.29418', '2.31734', '2.30170', '2.30608', '2.29214', '2.29224', '2.30216', '2.29081', '2.30006'] | Gamma1 Grad: ['-0.04689', '-0.01781', '0.01913', '-0.01929', '-0.08062', '0.03843', '-0.05005', '-0.03316', '-0.01885', '0.00347', '-0.00176', '0.00477', '-0.00289']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 78.774% (Updated)\n",
      "ğŸ“Š Train Accuracy: 78.774% | ğŸ† Best Train Accuracy: 78.774%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 78.774% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.45it/s, Test_acc=62.9, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.870% | ğŸ† Best Test Accuracy: 62.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.56it/s, Train_acc=79.5, Train_loss=0.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.31515', '2.29723', '2.29724', '2.28982', '2.30414', '2.31009', '2.30750', '2.29614', '2.30500', '2.30408', '2.30026', '2.29973', '2.28940'] | Gamma1 Grad: ['0.04803', '0.04748', '0.09816', '0.00548', '0.00062', '0.01342', '0.03545', '-0.03249', '0.01322', '-0.00536', '0.00447', '0.00114', '0.02916']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 79.464% (Updated)\n",
      "ğŸ“Š Train Accuracy: 79.464% | ğŸ† Best Train Accuracy: 79.464%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 79.464% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.92it/s, Test_acc=62, Test_loss=1.61]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 61.960% | ğŸ† Best Test Accuracy: 62.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.15it/s, Train_acc=80, Train_loss=0.667]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00139 | Gamma1: ['2.31436', '2.29728', '2.28842', '2.30489', '2.29967', '2.30014', '2.30298', '2.29088', '2.29737', '2.28174', '2.28052', '2.29362', '2.30880'] | Gamma1 Grad: ['-0.00343', '0.00578', '-0.03733', '-0.02059', '0.08476', '0.03713', '0.01115', '0.01894', '-0.00613', '0.00650', '-0.02883', '0.02524', '0.04715']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 79.950% (Updated)\n",
      "ğŸ“Š Train Accuracy: 79.950% | ğŸ† Best Train Accuracy: 79.950%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 79.950% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.08it/s, Test_acc=62.5, Test_loss=1.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.460% | ğŸ† Best Test Accuracy: 62.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.28it/s, Train_acc=80.8, Train_loss=0.637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.29453', '2.29513', '2.30542', '2.29308', '2.29725', '2.30645', '2.30436', '2.31470', '2.29401', '2.29077', '2.28488', '2.29132', '2.29885'] | Gamma1 Grad: ['-0.01781', '-0.00449', '-0.06520', '-0.00104', '-0.03136', '0.04518', '-0.01485', '-0.03332', '0.02054', '-0.00650', '-0.00786', '-0.00239', '0.01366']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 80.794% (Updated)\n",
      "ğŸ“Š Train Accuracy: 80.794% | ğŸ† Best Train Accuracy: 80.794%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 80.794% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.82it/s, Test_acc=63, Test_loss=1.57]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.980% | ğŸ† Best Test Accuracy: 62.980%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.61it/s, Train_acc=81.7, Train_loss=0.614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00133 | Gamma1: ['2.30473', '2.30717', '2.29355', '2.28937', '2.29181', '2.31773', '2.31749', '2.31386', '2.29906', '2.29690', '2.28101', '2.28262', '2.30961'] | Gamma1 Grad: ['-0.06198', '0.03808', '0.02772', '0.00154', '0.01899', '0.07700', '0.02467', '0.01074', '0.00098', '0.00023', '-0.00227', '0.01018', '-0.03548']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 81.728% (Updated)\n",
      "ğŸ“Š Train Accuracy: 81.728% | ğŸ† Best Train Accuracy: 81.728%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 81.728% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.70it/s, Test_acc=62.5, Test_loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.490% | ğŸ† Best Test Accuracy: 62.980%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.90it/s, Train_acc=82.2, Train_loss=0.595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['2.34117', '2.35805', '2.30417', '2.27981', '2.29807', '2.31448', '2.35321', '2.33380', '2.31810', '2.29351', '2.28419', '2.27448', '2.34114'] | Gamma1 Grad: ['0.04095', '0.02620', '0.12174', '-0.01632', '0.06049', '0.05993', '0.00134', '-0.01816', '0.03011', '0.01570', '-0.00910', '-0.02133', '-0.04844']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.154% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.154% | ğŸ† Best Train Accuracy: 82.154%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.154% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 40 | TrainLossHist: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.08it/s, Test_acc=62.4, Test_loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.380% | ğŸ† Best Test Accuracy: 62.980%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.74it/s, Train_acc=82.8, Train_loss=0.577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00125 | Gamma1: ['2.33923', '2.36121', '2.33363', '2.28297', '2.31389', '2.31547', '2.34129', '2.35175', '2.31125', '2.29694', '2.29801', '2.29180', '2.32063'] | Gamma1 Grad: ['0.02166', '0.07146', '0.12552', '-0.00871', '0.02045', '-0.03148', '0.03809', '-0.02828', '0.03569', '-0.02794', '0.00749', '-0.04767', '0.05959']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.796% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.796% | ğŸ† Best Train Accuracy: 82.796%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.796% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.09it/s, Test_acc=62.8, Test_loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.820% | ğŸ† Best Test Accuracy: 62.980%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.44it/s, Train_acc=83.3, Train_loss=0.553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.32771', '2.35460', '2.31542', '2.30899', '2.28782', '2.30955', '2.34086', '2.35767', '2.32604', '2.29434', '2.29194', '2.27130', '2.33089'] | Gamma1 Grad: ['-0.03199', '0.02717', '-0.05233', '0.02892', '0.04859', '-0.04047', '0.02644', '0.00503', '-0.00956', '-0.00737', '-0.00383', '-0.02874', '0.01886']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.286% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.286% | ğŸ† Best Train Accuracy: 83.286%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.286% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.52it/s, Test_acc=63.2, Test_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.240% | ğŸ† Best Test Accuracy: 63.240%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.56it/s, Train_acc=83.9, Train_loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00115 | Gamma1: ['2.32529', '2.35516', '2.32753', '2.29172', '2.27393', '2.29887', '2.35644', '2.34317', '2.33514', '2.30598', '2.29428', '2.29147', '2.32565'] | Gamma1 Grad: ['0.01941', '-0.01098', '0.02024', '-0.02444', '0.06838', '0.03175', '0.00688', '-0.00405', '-0.00845', '-0.00321', '-0.01063', '-0.00326', '-0.04621']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.930% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.930% | ğŸ† Best Train Accuracy: 83.930%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.930% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.68it/s, Test_acc=63.8, Test_loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.750% | ğŸ† Best Test Accuracy: 63.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.23it/s, Train_acc=84.5, Train_loss=0.518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['2.33360', '2.33705', '2.32463', '2.28211', '2.27641', '2.32110', '2.35031', '2.35190', '2.33794', '2.31135', '2.30013', '2.28443', '2.31945'] | Gamma1 Grad: ['0.00729', '-0.05581', '0.10657', '-0.07997', '0.05906', '0.02878', '0.01186', '-0.00188', '0.01794', '0.00565', '-0.01697', '-0.02228', '-0.03728']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 84.528% (Updated)\n",
      "ğŸ“Š Train Accuracy: 84.528% | ğŸ† Best Train Accuracy: 84.528%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 84.528% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.24it/s, Test_acc=63.7, Test_loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.690% | ğŸ† Best Test Accuracy: 63.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.47it/s, Train_acc=85, Train_loss=0.502]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00105 | Gamma1: ['2.31474', '2.31698', '2.29962', '2.31305', '2.26395', '2.32893', '2.36495', '2.36154', '2.31191', '2.31296', '2.29755', '2.29926', '2.31971'] | Gamma1 Grad: ['0.02806', '0.00252', '-0.00085', '0.06908', '-0.04240', '-0.05382', '0.04449', '-0.01851', '-0.01455', '0.00100', '-0.00297', '0.01387', '0.02491']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.014% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.014% | ğŸ† Best Train Accuracy: 85.014%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.014% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.81it/s, Test_acc=63, Test_loss=1.73]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.010% | ğŸ† Best Test Accuracy: 63.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.51it/s, Train_acc=85.3, Train_loss=0.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.34874', '2.34329', '2.33060', '2.28017', '2.28696', '2.33607', '2.35460', '2.36522', '2.32363', '2.29763', '2.29644', '2.29726', '2.32120'] | Gamma1 Grad: ['-0.00703', '-0.02114', '0.01851', '0.03007', '-0.04983', '-0.04919', '0.00129', '0.01967', '0.01022', '-0.00247', '-0.01504', '-0.01192', '-0.00970']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.306% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.306% | ğŸ† Best Train Accuracy: 85.306%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.306% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.62it/s, Test_acc=63.5, Test_loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.470% | ğŸ† Best Test Accuracy: 63.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.29it/s, Train_acc=85.8, Train_loss=0.467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00094 | Gamma1: ['2.32676', '2.33436', '2.31832', '2.25168', '2.28850', '2.30164', '2.35885', '2.34705', '2.31486', '2.31014', '2.30148', '2.28172', '2.33168'] | Gamma1 Grad: ['-0.02659', '0.01722', '0.01523', '0.02544', '-0.01362', '-0.00664', '0.00568', '-0.00562', '0.00078', '-0.00210', '0.00561', '-0.00109', '0.03766']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.832% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.832% | ğŸ† Best Train Accuracy: 85.832%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.832% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.59it/s, Test_acc=63, Test_loss=1.7]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.970% | ğŸ† Best Test Accuracy: 63.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.40it/s, Train_acc=86.4, Train_loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['2.30848', '2.36174', '2.30472', '2.28124', '2.28603', '2.28857', '2.36112', '2.35861', '2.31654', '2.30250', '2.30829', '2.30089', '2.31581'] | Gamma1 Grad: ['0.00394', '-0.11931', '-0.13521', '0.12442', '0.16975', '-0.08876', '0.11501', '0.02008', '-0.00619', '-0.01246', '-0.01191', '0.00332', '0.05882']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.410% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.410% | ğŸ† Best Train Accuracy: 86.410%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.410% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.36it/s, Test_acc=63.2, Test_loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.250% | ğŸ† Best Test Accuracy: 63.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.94it/s, Train_acc=86.7, Train_loss=0.445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00083 | Gamma1: ['2.32803', '2.33664', '2.29372', '2.28055', '2.26542', '2.28627', '2.37541', '2.35677', '2.32184', '2.31196', '2.29976', '2.28857', '2.30548'] | Gamma1 Grad: ['-0.00613', '-0.06029', '-0.02681', '0.03359', '-0.15164', '0.07242', '0.01812', '0.03733', '-0.00490', '-0.00587', '0.00435', '0.01100', '-0.04494']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.706% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.706% | ğŸ† Best Train Accuracy: 86.706%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.706% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.20it/s, Test_acc=63.2, Test_loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.240% | ğŸ† Best Test Accuracy: 63.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.29it/s, Train_acc=87.2, Train_loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.31009', '2.34863', '2.31558', '2.27222', '2.25430', '2.30050', '2.37485', '2.36522', '2.31039', '2.30323', '2.28544', '2.28292', '2.30861'] | Gamma1 Grad: ['-0.00018', '-0.05812', '0.01706', '0.00606', '-0.01224', '-0.04583', '-0.02026', '-0.00547', '-0.00415', '-0.00224', '-0.00755', '-0.01145', '-0.03510']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.182% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.182% | ğŸ† Best Train Accuracy: 87.182%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.182% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 50 | TrainLossHist: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.36it/s, Test_acc=63.5, Test_loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.530% | ğŸ† Best Test Accuracy: 63.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 51 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.08it/s, Train_acc=87.6, Train_loss=0.413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00072 | Gamma1: ['2.33342', '2.33150', '2.29703', '2.26367', '2.26232', '2.28109', '2.35704', '2.35027', '2.32525', '2.29811', '2.30374', '2.30234', '2.31589'] | Gamma1 Grad: ['0.03652', '0.00999', '-0.01148', '-0.01658', '0.00912', '-0.03975', '-0.01529', '-0.02292', '-0.01509', '-0.00182', '-0.01481', '-0.01933', '-0.03622']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.608% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.608% | ğŸ† Best Train Accuracy: 87.608%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.608% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.38it/s, Test_acc=63.5, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.530% | ğŸ† Best Test Accuracy: 63.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 52 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.12it/s, Train_acc=88, Train_loss=0.403]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['2.29852', '2.33411', '2.31594', '2.29442', '2.25319', '2.28778', '2.35362', '2.35507', '2.32558', '2.30455', '2.31121', '2.28918', '2.32173'] | Gamma1 Grad: ['0.03750', '0.04198', '-0.01577', '-0.02626', '-0.00678', '0.03947', '-0.00267', '-0.01251', '0.01071', '0.00183', '0.00491', '-0.01104', '0.00291']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.986% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.986% | ğŸ† Best Train Accuracy: 87.986%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.986% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.01it/s, Test_acc=63.8, Test_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.830% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 53 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.26it/s, Train_acc=88.2, Train_loss=0.394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00061 | Gamma1: ['2.31945', '2.34066', '2.30952', '2.28232', '2.25397', '2.29487', '2.34165', '2.34975', '2.32449', '2.29974', '2.29206', '2.27706', '2.32557'] | Gamma1 Grad: ['-0.00342', '-0.00680', '0.19565', '0.01325', '-0.03166', '0.03544', '-0.01534', '-0.02330', '-0.00635', '0.00893', '0.01116', '0.00966', '-0.06349']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.176% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.176% | ğŸ† Best Train Accuracy: 88.176%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.176% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.75it/s, Test_acc=63.5, Test_loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.520% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 54 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.80it/s, Train_acc=88.4, Train_loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.33798', '2.35112', '2.30520', '2.27946', '2.25233', '2.27521', '2.32720', '2.33884', '2.32285', '2.27539', '2.28272', '2.28155', '2.30604'] | Gamma1 Grad: ['-0.00642', '-0.04282', '0.10941', '-0.07703', '0.10203', '0.00752', '0.07962', '0.10416', '0.00070', '-0.02004', '-0.00864', '-0.03926', '0.04968']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.378% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.378% | ğŸ† Best Train Accuracy: 88.378%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.378% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.73it/s, Test_acc=63.1, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.080% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.17it/s, Train_acc=88.8, Train_loss=0.372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00050 | Gamma1: ['2.31951', '2.34174', '2.30536', '2.29056', '2.25195', '2.26838', '2.35054', '2.34789', '2.33662', '2.27969', '2.28545', '2.29226', '2.30340'] | Gamma1 Grad: ['-0.02096', '-0.02307', '-0.01119', '0.02035', '0.02316', '0.06200', '-0.00022', '-0.01090', '-0.03571', '-0.01038', '0.00453', '-0.00021', '0.06056']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.838% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.838% | ğŸ† Best Train Accuracy: 88.838%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.838% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.97it/s, Test_acc=63.8, Test_loss=1.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.810% | ğŸ† Best Test Accuracy: 63.830%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.90it/s, Train_acc=89.4, Train_loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.31293', '2.33317', '2.30598', '2.29442', '2.25918', '2.28596', '2.32807', '2.36431', '2.33795', '2.29396', '2.29156', '2.27665', '2.29796'] | Gamma1 Grad: ['0.02745', '0.00010', '-0.00252', '0.06018', '-0.02608', '-0.02983', '0.07507', '-0.00173', '-0.04747', '0.01289', '0.01904', '0.04129', '0.09058']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.352% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.352% | ğŸ† Best Train Accuracy: 89.352%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.352% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.46it/s, Test_acc=64.2, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.170% | ğŸ† Best Test Accuracy: 64.170%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.94it/s, Train_acc=89.4, Train_loss=0.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00040 | Gamma1: ['2.33104', '2.33744', '2.29908', '2.28917', '2.24390', '2.26494', '2.31857', '2.36423', '2.32558', '2.29387', '2.29144', '2.28779', '2.30970'] | Gamma1 Grad: ['-0.06061', '-0.07110', '-0.03597', '0.06505', '-0.00467', '0.07882', '-0.03837', '-0.02452', '0.02282', '-0.00498', '-0.01030', '-0.00335', '-0.00655']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.402% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.402% | ğŸ† Best Train Accuracy: 89.402%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.402% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.36it/s, Test_acc=64.3, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.270% | ğŸ† Best Test Accuracy: 64.270%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.71it/s, Train_acc=90, Train_loss=0.34]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.32208', '2.32381', '2.31080', '2.28870', '2.24135', '2.27956', '2.33205', '2.34236', '2.32318', '2.29428', '2.28431', '2.28367', '2.30048'] | Gamma1 Grad: ['-0.01188', '0.04525', '-0.06325', '0.01862', '-0.10686', '0.01737', '0.01334', '-0.07251', '0.01257', '-0.00269', '0.02216', '0.02402', '0.03488']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.974% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.974% | ğŸ† Best Train Accuracy: 89.974%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.974% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.15it/s, Test_acc=63.4, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.440% | ğŸ† Best Test Accuracy: 64.270%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.41it/s, Train_acc=90, Train_loss=0.342]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00030 | Gamma1: ['2.32502', '2.33230', '2.31194', '2.28217', '2.24290', '2.28157', '2.33780', '2.35811', '2.31544', '2.29804', '2.28664', '2.28597', '2.32029'] | Gamma1 Grad: ['-0.01264', '0.01269', '-0.03020', '0.16085', '-0.01422', '-0.07486', '-0.01623', '-0.02731', '-0.01991', '0.01239', '0.00596', '0.00223', '-0.04689']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.998% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.998% | ğŸ† Best Train Accuracy: 89.998%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.998% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.35it/s, Test_acc=64, Test_loss=1.8]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.020% | ğŸ† Best Test Accuracy: 64.270%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.01it/s, Train_acc=90.2, Train_loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['2.34948', '2.31912', '2.30931', '2.26949', '2.23876', '2.26781', '2.32484', '2.35377', '2.34185', '2.30121', '2.29061', '2.30086', '2.31512'] | Gamma1 Grad: ['0.00417', '-0.01051', '0.03303', '0.00863', '-0.00786', '-0.01355', '-0.00144', '-0.01134', '0.00806', '-0.00851', '0.00458', '-0.00029', '0.03621']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.154% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.154% | ğŸ† Best Train Accuracy: 90.154%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.154% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 60 | TrainLossHist: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.60it/s, Test_acc=63.9, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.860% | ğŸ† Best Test Accuracy: 64.270%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.26it/s, Train_acc=90.7, Train_loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00022 | Gamma1: ['2.33857', '2.34055', '2.28702', '2.24603', '2.23391', '2.26970', '2.37845', '2.36966', '2.35223', '2.29771', '2.29528', '2.28340', '2.31058'] | Gamma1 Grad: ['-0.00414', '-0.00074', '-0.01869', '-0.00852', '0.05319', '-0.04707', '-0.00233', '0.02550', '0.00529', '-0.00061', '-0.00647', '0.03132', '-0.00420']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.734% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.734% | ğŸ† Best Train Accuracy: 90.734%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.734% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.33it/s, Test_acc=64.3, Test_loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.340% | ğŸ† Best Test Accuracy: 64.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.13it/s, Train_acc=90.8, Train_loss=0.313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.34689', '2.34250', '2.30075', '2.25139', '2.25678', '2.28545', '2.36842', '2.37818', '2.35887', '2.29621', '2.29584', '2.27423', '2.29856'] | Gamma1 Grad: ['0.01984', '-0.03955', '-0.02869', '0.00159', '0.14788', '-0.05236', '0.02613', '-0.02359', '-0.00831', '0.01929', '-0.00103', '-0.05846', '-0.01518']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.832% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.832% | ğŸ† Best Train Accuracy: 90.832%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.832% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.79it/s, Test_acc=64.2, Test_loss=1.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.240% | ğŸ† Best Test Accuracy: 64.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.35it/s, Train_acc=90.7, Train_loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00016 | Gamma1: ['2.33990', '2.34350', '2.32178', '2.25339', '2.24122', '2.25888', '2.35595', '2.38423', '2.35898', '2.29478', '2.29964', '2.26824', '2.30398'] | Gamma1 Grad: ['-0.05173', '0.00127', '-0.01433', '-0.05521', '-0.01880', '0.08955', '-0.04075', '-0.02581', '-0.01138', '0.00972', '-0.00574', '-0.00369', '-0.01270']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 90.684% | ğŸ† Best Train Accuracy: 90.832%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.832% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.45it/s, Test_acc=63.8, Test_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.800% | ğŸ† Best Test Accuracy: 64.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.63it/s, Train_acc=91.1, Train_loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['2.36239', '2.34344', '2.30480', '2.25617', '2.24407', '2.26227', '2.37481', '2.38173', '2.34888', '2.29209', '2.28331', '2.27403', '2.29530'] | Gamma1 Grad: ['-0.03379', '-0.04780', '0.00077', '0.02907', '0.05266', '-0.02713', '0.07532', '0.00110', '-0.01085', '0.00910', '0.02147', '-0.01228', '0.01050']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.106% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.106% | ğŸ† Best Train Accuracy: 91.106%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.106% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.03it/s, Test_acc=64.9, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.870% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.64it/s, Train_acc=91.6, Train_loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00011 | Gamma1: ['2.34448', '2.34145', '2.32335', '2.25233', '2.22603', '2.23835', '2.33929', '2.37354', '2.35637', '2.28946', '2.29015', '2.28408', '2.31383'] | Gamma1 Grad: ['-0.02060', '-0.01134', '0.01301', '-0.05510', '-0.02321', '0.04956', '0.02987', '0.01277', '-0.00728', '-0.00220', '-0.00696', '-0.00144', '0.01373']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.600% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.600% | ğŸ† Best Train Accuracy: 91.600%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.600% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.35it/s, Test_acc=64.3, Test_loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.300% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.81it/s, Train_acc=91.6, Train_loss=0.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.34705', '2.34525', '2.32242', '2.26399', '2.20505', '2.24021', '2.33403', '2.36447', '2.35226', '2.29685', '2.29216', '2.28370', '2.29557'] | Gamma1 Grad: ['-0.03565', '-0.00484', '0.02399', '0.04628', '-0.00125', '0.15093', '-0.02599', '-0.04741', '0.00022', '0.00168', '0.00648', '-0.03109', '0.02247']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 91.582% | ğŸ† Best Train Accuracy: 91.600%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.600% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.51it/s, Test_acc=64.1, Test_loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.130% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.90it/s, Train_acc=91.8, Train_loss=0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.34853', '2.35157', '2.30901', '2.24620', '2.19939', '2.25282', '2.33075', '2.37826', '2.35849', '2.30701', '2.29856', '2.29668', '2.30730'] | Gamma1 Grad: ['0.00602', '-0.01909', '0.04110', '-0.02152', '-0.11516', '0.02759', '0.04663', '-0.02437', '-0.03166', '0.00946', '-0.00113', '-0.03146', '-0.02115']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.804% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.804% | ğŸ† Best Train Accuracy: 91.804%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.804% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.92it/s, Test_acc=64.3, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.270% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.64it/s, Train_acc=92.3, Train_loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['2.35617', '2.34625', '2.30484', '2.24818', '2.21115', '2.24077', '2.34615', '2.37601', '2.34002', '2.29357', '2.30842', '2.28527', '2.29965'] | Gamma1 Grad: ['-0.01673', '-0.02216', '-0.01945', '-0.00879', '0.03962', '-0.02339', '0.02227', '-0.00039', '0.01927', '-0.01043', '-0.00089', '-0.02501', '-0.01900']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.270% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.270% | ğŸ† Best Train Accuracy: 92.270%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.270% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.04it/s, Test_acc=63.9, Test_loss=2.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.900% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.98it/s, Train_acc=92, Train_loss=0.276]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['2.34374', '2.36230', '2.30274', '2.26290', '2.21722', '2.23889', '2.34185', '2.36637', '2.34707', '2.30254', '2.29897', '2.28783', '2.29481'] | Gamma1 Grad: ['0.02291', '0.01991', '-0.00269', '0.01775', '0.06837', '0.00902', '-0.05792', '-0.03055', '0.01820', '0.00697', '0.00976', '0.02784', '0.05155']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 92.004% | ğŸ† Best Train Accuracy: 92.270%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.270% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.18it/s, Test_acc=64.4, Test_loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.360% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.15it/s, Train_acc=92.5, Train_loss=0.262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.33040', '2.35568', '2.29845', '2.25524', '2.21173', '2.24485', '2.34255', '2.37894', '2.35169', '2.30371', '2.30128', '2.28317', '2.29449'] | Gamma1 Grad: ['-0.00673', '-0.00504', '0.01475', '0.02202', '0.02480', '0.03630', '-0.03202', '0.00934', '-0.01754', '-0.01312', '-0.01848', '-0.02131', '-0.05221']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.542% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.542% | ğŸ† Best Train Accuracy: 92.542%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.542% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 70 | TrainLossHist: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.51it/s, Test_acc=64.8, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.800% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.12it/s, Train_acc=92.8, Train_loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.33650', '2.35785', '2.31703', '2.23784', '2.21087', '2.21964', '2.35259', '2.37041', '2.34343', '2.31342', '2.29734', '2.28849', '2.32315'] | Gamma1 Grad: ['-0.03382', '-0.05392', '0.02174', '0.10180', '0.17945', '0.02444', '-0.00269', '0.01242', '-0.01637', '0.00081', '0.02905', '-0.00123', '0.04090']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.784% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.784% | ğŸ† Best Train Accuracy: 92.784%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.784% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.39it/s, Test_acc=64.2, Test_loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.150% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.75it/s, Train_acc=92.6, Train_loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.32705', '2.35701', '2.29162', '2.23719', '2.20097', '2.21225', '2.35788', '2.34405', '2.33679', '2.31208', '2.30366', '2.30532', '2.31172'] | Gamma1 Grad: ['-0.01483', '0.01953', '0.00344', '-0.00641', '-0.01195', '0.03780', '-0.06904', '0.03133', '-0.00951', '0.00625', '0.00422', '-0.01244', '0.00811']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 92.616% | ğŸ† Best Train Accuracy: 92.784%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.784% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.91it/s, Test_acc=64.8, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.830% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.57it/s, Train_acc=92.9, Train_loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.34055', '2.33178', '2.27949', '2.21739', '2.20429', '2.18472', '2.33770', '2.33604', '2.34406', '2.29172', '2.28754', '2.28271', '2.28282'] | Gamma1 Grad: ['0.02220', '0.04089', '-0.00863', '-0.02758', '0.06600', '-0.02789', '0.02314', '-0.01992', '-0.00068', '-0.00804', '-0.00210', '0.00270', '0.02976']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.862% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.862% | ğŸ† Best Train Accuracy: 92.862%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.862% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.52it/s, Test_acc=64, Test_loss=1.97]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.050% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.82it/s, Train_acc=93.2, Train_loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.32486', '2.31235', '2.25484', '2.26100', '2.20161', '2.21188', '2.32719', '2.34586', '2.34715', '2.30347', '2.30255', '2.28386', '2.30532'] | Gamma1 Grad: ['0.02230', '-0.01328', '0.00595', '0.02938', '-0.03463', '0.03049', '-0.00900', '-0.01465', '0.01751', '-0.00382', '0.00903', '0.02014', '-0.02451']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.156% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.156% | ğŸ† Best Train Accuracy: 93.156%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.156% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.05it/s, Test_acc=64.5, Test_loss=1.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.500% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.24it/s, Train_acc=93, Train_loss=0.247]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.33243', '2.37862', '2.28675', '2.25819', '2.22310', '2.20693', '2.36660', '2.37430', '2.31969', '2.31439', '2.28751', '2.27317', '2.30387'] | Gamma1 Grad: ['0.02394', '0.04803', '-0.03397', '-0.05781', '0.04181', '0.05367', '0.02485', '-0.07534', '-0.01711', '-0.00222', '0.00044', '-0.01912', '-0.07301']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 92.976% | ğŸ† Best Train Accuracy: 93.156%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.156% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.07it/s, Test_acc=64.7, Test_loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.680% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.70it/s, Train_acc=93.4, Train_loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.32411', '2.36590', '2.30808', '2.27412', '2.22297', '2.21886', '2.34437', '2.35856', '2.34479', '2.31267', '2.27119', '2.29319', '2.28895'] | Gamma1 Grad: ['0.02780', '0.03366', '-0.06936', '-0.18437', '0.05413', '-0.20879', '-0.02655', '-0.01266', '0.01236', '0.01206', '0.02092', '0.02600', '-0.03893']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.414% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.414% | ğŸ† Best Train Accuracy: 93.414%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.414% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.38it/s, Test_acc=64.3, Test_loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.320% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.05it/s, Train_acc=93.4, Train_loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00147 | Gamma1: ['2.32947', '2.34059', '2.30118', '2.25471', '2.18798', '2.20290', '2.31187', '2.37009', '2.32777', '2.30963', '2.28350', '2.28460', '2.28846'] | Gamma1 Grad: ['0.00628', '0.00476', '-0.00401', '-0.02345', '0.04983', '-0.00104', '0.00379', '0.01137', '-0.00029', '-0.00137', '0.00210', '0.00722', '0.02729']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.416% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.416% | ğŸ† Best Train Accuracy: 93.416%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.416% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.51it/s, Test_acc=64.1, Test_loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.100% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.76it/s, Train_acc=93.2, Train_loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.32930', '2.34869', '2.30166', '2.24094', '2.20193', '2.19221', '2.32163', '2.37681', '2.33441', '2.30188', '2.28336', '2.27995', '2.27768'] | Gamma1 Grad: ['0.00183', '-0.01371', '0.00209', '-0.03568', '-0.04127', '-0.00333', '0.01648', '0.01322', '-0.00352', '0.00452', '-0.00332', '-0.00310', '0.01067']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.234% | ğŸ† Best Train Accuracy: 93.416%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.416% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.35it/s, Test_acc=63.9, Test_loss=2.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.850% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.70it/s, Train_acc=93.9, Train_loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00146 | Gamma1: ['2.35246', '2.32160', '2.29338', '2.29473', '2.18088', '2.20979', '2.36053', '2.37027', '2.32511', '2.29780', '2.29553', '2.28830', '2.30632'] | Gamma1 Grad: ['0.05242', '-0.07424', '-0.09218', '0.11534', '0.09418', '0.02635', '0.06489', '0.04835', '-0.02299', '0.00217', '0.01198', '-0.00658', '0.04265']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.854% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.854% | ğŸ† Best Train Accuracy: 93.854%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.854% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.96it/s, Test_acc=64.5, Test_loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.450% | ğŸ† Best Test Accuracy: 64.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.01it/s, Train_acc=96.5, Train_loss=0.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00144 | Gamma1: ['2.33499', '2.32864', '2.30297', '2.24798', '2.21001', '2.20044', '2.31897', '2.35524', '2.32230', '2.32375', '2.31563', '2.28656', '2.27338'] | Gamma1 Grad: ['-0.01135', '-0.01974', '0.01239', '-0.02979', '-0.01178', '-0.00157', '0.02593', '0.02431', '-0.00710', '-0.00139', '0.00189', '-0.00732', '0.01869']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 96.486% (Updated)\n",
      "ğŸ“Š Train Accuracy: 96.486% | ğŸ† Best Train Accuracy: 96.486%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 96.486% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 80 | TrainLossHist: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.20it/s, Test_acc=66.5, Test_loss=1.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 66.480% | ğŸ† Best Test Accuracy: 66.480%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.35it/s, Train_acc=97.5, Train_loss=0.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00143 | Gamma1: ['2.33428', '2.32692', '2.31261', '2.21155', '2.20703', '2.19318', '2.31448', '2.30818', '2.30671', '2.32962', '2.32443', '2.27475', '2.25773'] | Gamma1 Grad: ['-0.00345', '0.01262', '0.01063', '0.00097', '0.06173', '-0.01821', '-0.00851', '0.04681', '-0.00371', '0.01184', '0.00197', '0.01366', '0.03480']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.468% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.468% | ğŸ† Best Train Accuracy: 97.468%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.468% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.28it/s, Test_acc=66.6, Test_loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 66.600% | ğŸ† Best Test Accuracy: 66.600%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.42it/s, Train_acc=98, Train_loss=0.098]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00142 | Gamma1: ['2.30934', '2.32389', '2.30535', '2.21426', '2.20921', '2.21711', '2.30243', '2.34092', '2.30618', '2.31484', '2.32774', '2.28517', '2.28796'] | Gamma1 Grad: ['-0.06974', '0.02232', '0.03015', '-0.01033', '0.15344', '-0.05574', '-0.00826', '0.01039', '-0.00567', '0.00827', '0.00776', '-0.01170', '0.03589']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.994% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.994% | ğŸ† Best Train Accuracy: 97.994%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.994% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.48it/s, Test_acc=66.9, Test_loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 66.900% | ğŸ† Best Test Accuracy: 66.900%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.38it/s, Train_acc=98, Train_loss=0.093]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00141 | Gamma1: ['2.32146', '2.32338', '2.31360', '2.20806', '2.21245', '2.24527', '2.32177', '2.33423', '2.32225', '2.30729', '2.31951', '2.27086', '2.28730'] | Gamma1 Grad: ['0.00479', '0.04230', '-0.00525', '-0.00101', '0.03077', '-0.01864', '0.03900', '0.01035', '-0.00678', '0.00510', '-0.00032', '-0.00000', '0.00529']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 97.988% | ğŸ† Best Train Accuracy: 97.994%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.994% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.98it/s, Test_acc=67.3, Test_loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.330% | ğŸ† Best Test Accuracy: 67.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.84it/s, Train_acc=98.2, Train_loss=0.088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00139 | Gamma1: ['2.29505', '2.29977', '2.30234', '2.25000', '2.20127', '2.24628', '2.30390', '2.31463', '2.32155', '2.31423', '2.31712', '2.29424', '2.30950'] | Gamma1 Grad: ['-0.01214', '0.00424', '0.06620', '0.09155', '0.04289', '0.06797', '0.06087', '-0.01479', '-0.00079', '-0.00283', '0.00442', '-0.00849', '0.00366']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.238% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.238% | ğŸ† Best Train Accuracy: 98.238%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.238% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.63it/s, Test_acc=67.1, Test_loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.120% | ğŸ† Best Test Accuracy: 67.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.32it/s, Train_acc=98.4, Train_loss=0.083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00138 | Gamma1: ['2.33041', '2.31472', '2.31387', '2.23855', '2.20843', '2.26316', '2.28767', '2.30444', '2.31586', '2.30584', '2.32355', '2.28080', '2.29124'] | Gamma1 Grad: ['0.00044', '-0.02454', '0.03787', '0.01538', '-0.02382', '0.01349', '0.02722', '0.01075', '-0.01553', '0.00758', '0.00994', '0.01975', '0.05498']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.356% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.356% | ğŸ† Best Train Accuracy: 98.356%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.356% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.36it/s, Test_acc=67.1, Test_loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.100% | ğŸ† Best Test Accuracy: 67.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.46it/s, Train_acc=98.5, Train_loss=0.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00136 | Gamma1: ['2.32195', '2.31119', '2.29008', '2.23608', '2.22518', '2.24438', '2.29117', '2.31137', '2.30694', '2.31649', '2.31846', '2.28819', '2.31627'] | Gamma1 Grad: ['-0.02943', '-0.04010', '0.00113', '0.16187', '-0.10894', '-0.03956', '0.00558', '0.02125', '-0.03108', '0.03364', '0.01695', '0.00879', '0.10241']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.488% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.488% | ğŸ† Best Train Accuracy: 98.488%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.488% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.61it/s, Test_acc=67.3, Test_loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.340% | ğŸ† Best Test Accuracy: 67.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.28it/s, Train_acc=98.6, Train_loss=0.077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00134 | Gamma1: ['2.31204', '2.30759', '2.30864', '2.23419', '2.21386', '2.19735', '2.31278', '2.30115', '2.30977', '2.30717', '2.31321', '2.28518', '2.29324'] | Gamma1 Grad: ['0.04784', '0.00325', '-0.00459', '-0.00794', '-0.07778', '0.00698', '-0.02019', '-0.02109', '0.01251', '0.00291', '0.00808', '-0.00375', '0.00747']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.588% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.588% | ğŸ† Best Train Accuracy: 98.588%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.588% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.64it/s, Test_acc=67.1, Test_loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.070% | ğŸ† Best Test Accuracy: 67.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.77it/s, Train_acc=98.6, Train_loss=0.074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00133 | Gamma1: ['2.30982', '2.31458', '2.29469', '2.25588', '2.22774', '2.23544', '2.28456', '2.31175', '2.31335', '2.29385', '2.30183', '2.27872', '2.30096'] | Gamma1 Grad: ['0.00240', '0.00207', '-0.00031', '-0.00411', '-0.02655', '0.00522', '-0.00897', '-0.00845', '0.00546', '0.00115', '0.00119', '0.00503', '-0.00582']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.588% | ğŸ† Best Train Accuracy: 98.588%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.588% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.89it/s, Test_acc=67.3, Test_loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.270% | ğŸ† Best Test Accuracy: 67.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.44it/s, Train_acc=98.7, Train_loss=0.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00131 | Gamma1: ['2.31031', '2.30138', '2.30030', '2.23392', '2.25108', '2.23548', '2.28383', '2.30395', '2.32497', '2.30542', '2.32977', '2.27554', '2.28972'] | Gamma1 Grad: ['0.00163', '-0.03560', '-0.01398', '0.05603', '-0.02279', '-0.04844', '-0.00876', '-0.01001', '-0.01212', '0.00548', '0.01025', '-0.01819', '0.01129']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.742% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.742% | ğŸ† Best Train Accuracy: 98.742%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.742% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.26it/s, Test_acc=67, Test_loss=2.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.960% | ğŸ† Best Test Accuracy: 67.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.37it/s, Train_acc=98.8, Train_loss=0.071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00129 | Gamma1: ['2.32180', '2.30420', '2.30250', '2.22995', '2.22675', '2.24873', '2.29140', '2.32095', '2.31854', '2.30678', '2.31433', '2.29561', '2.32905'] | Gamma1 Grad: ['0.01791', '-0.06773', '-0.04110', '-0.04439', '-0.17373', '-0.03317', '-0.11779', '-0.09724', '0.08825', '-0.01275', '-0.01339', '-0.03157', '0.01689']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.812% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.812% | ğŸ† Best Train Accuracy: 98.812%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.812% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 90 | TrainLossHist: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.83it/s, Test_acc=67, Test_loss=2.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.980% | ğŸ† Best Test Accuracy: 67.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.32it/s, Train_acc=98.7, Train_loss=0.071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00127 | Gamma1: ['2.32365', '2.31863', '2.30394', '2.22895', '2.22960', '2.26024', '2.27550', '2.32195', '2.30740', '2.30367', '2.31543', '2.30378', '2.31526'] | Gamma1 Grad: ['-0.00936', '-0.00467', '0.02166', '-0.06407', '-0.05552', '-0.07595', '-0.01093', '-0.01847', '-0.01317', '-0.00170', '0.00400', '-0.01983', '0.01126']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.738% | ğŸ† Best Train Accuracy: 98.812%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.812% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.78it/s, Test_acc=67.1, Test_loss=2.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.060% | ğŸ† Best Test Accuracy: 67.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.59it/s, Train_acc=98.8, Train_loss=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00125 | Gamma1: ['2.30658', '2.32551', '2.27149', '2.25128', '2.22924', '2.24928', '2.31069', '2.31717', '2.31256', '2.30834', '2.31218', '2.29844', '2.30099'] | Gamma1 Grad: ['-0.01543', '-0.05524', '0.08265', '-0.11562', '0.09623', '0.01825', '-0.18824', '-0.06490', '-0.01972', '-0.00271', '0.02352', '0.00316', '0.04389']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.790% | ğŸ† Best Train Accuracy: 98.812%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.812% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.59it/s, Test_acc=67, Test_loss=2.22]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.040% | ğŸ† Best Test Accuracy: 67.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.39it/s, Train_acc=99, Train_loss=0.065]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00122 | Gamma1: ['2.31317', '2.31647', '2.28347', '2.26543', '2.23113', '2.23756', '2.27434', '2.29305', '2.31454', '2.29568', '2.30738', '2.29669', '2.30091'] | Gamma1 Grad: ['-0.02247', '0.00063', '-0.00640', '-0.00118', '-0.12566', '0.01559', '0.00547', '0.00726', '0.02023', '0.00436', '-0.00508', '-0.00007', '0.02656']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.968% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.968% | ğŸ† Best Train Accuracy: 98.968%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.968% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.74it/s, Test_acc=67.1, Test_loss=2.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.080% | ğŸ† Best Test Accuracy: 67.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.48it/s, Train_acc=99, Train_loss=0.064]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00120 | Gamma1: ['2.29776', '2.31196', '2.27702', '2.26324', '2.22435', '2.24529', '2.28613', '2.31841', '2.31366', '2.29318', '2.30451', '2.29276', '2.30367'] | Gamma1 Grad: ['0.00210', '-0.00327', '0.00189', '-0.01208', '0.01262', '0.00361', '0.00202', '0.00367', '-0.00164', '0.00067', '-0.00025', '0.00275', '-0.00079']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.956% | ğŸ† Best Train Accuracy: 98.968%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.968% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.16it/s, Test_acc=67.1, Test_loss=2.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.110% | ğŸ† Best Test Accuracy: 67.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.17it/s, Train_acc=99, Train_loss=0.064]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00118 | Gamma1: ['2.29891', '2.31636', '2.27354', '2.23902', '2.22547', '2.25799', '2.25819', '2.30480', '2.30770', '2.28727', '2.30301', '2.28746', '2.29918'] | Gamma1 Grad: ['0.02423', '0.00801', '-0.03469', '-0.01647', '-0.03442', '0.03597', '-0.02485', '-0.04785', '0.00914', '0.00015', '0.00463', '0.01998', '0.00251']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.966% | ğŸ† Best Train Accuracy: 98.968%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.968% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.05it/s, Test_acc=67.5, Test_loss=2.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.540% | ğŸ† Best Test Accuracy: 67.540%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.10it/s, Train_acc=98.9, Train_loss=0.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00115 | Gamma1: ['2.30064', '2.30912', '2.28862', '2.25084', '2.23403', '2.24406', '2.27509', '2.29544', '2.32214', '2.29390', '2.29963', '2.29528', '2.29763'] | Gamma1 Grad: ['0.00496', '-0.00576', '-0.00057', '-0.01999', '-0.01196', '0.01385', '0.00883', '-0.00822', '-0.00384', '0.00063', '-0.00132', '-0.00286', '0.00332']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.922% | ğŸ† Best Train Accuracy: 98.968%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.968% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.79it/s, Test_acc=67.4, Test_loss=2.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.420% | ğŸ† Best Test Accuracy: 67.540%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.62it/s, Train_acc=99, Train_loss=0.062]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00113 | Gamma1: ['2.29583', '2.30514', '2.28704', '2.25465', '2.22568', '2.26580', '2.29857', '2.30728', '2.30846', '2.29740', '2.30294', '2.28799', '2.30820'] | Gamma1 Grad: ['-0.00580', '0.00144', '-0.01729', '0.03262', '-0.01940', '0.04562', '-0.00211', '0.00990', '-0.00832', '0.01570', '-0.00354', '-0.00419', '-0.01571']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.020% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.020% | ğŸ† Best Train Accuracy: 99.020%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.020% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.49it/s, Test_acc=67.7, Test_loss=2.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.680% | ğŸ† Best Test Accuracy: 67.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.30it/s, Train_acc=99, Train_loss=0.061]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00110 | Gamma1: ['2.28631', '2.28372', '2.31354', '2.23033', '2.22959', '2.28134', '2.28235', '2.31838', '2.29211', '2.30131', '2.30895', '2.28490', '2.30001'] | Gamma1 Grad: ['0.02487', '0.00915', '0.01722', '0.01382', '0.02878', '0.03465', '0.00319', '-0.01148', '-0.00968', '0.00278', '0.00521', '-0.00032', '-0.00678']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.036% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.036% | ğŸ† Best Train Accuracy: 99.036%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.036% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.37it/s, Test_acc=67.2, Test_loss=2.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.170% | ğŸ† Best Test Accuracy: 67.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.17it/s, Train_acc=99.1, Train_loss=0.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00108 | Gamma1: ['2.30138', '2.28910', '2.29034', '2.25392', '2.25188', '2.25556', '2.28232', '2.30297', '2.30385', '2.30087', '2.31053', '2.30251', '2.31550'] | Gamma1 Grad: ['-0.04949', '0.01305', '0.00887', '0.02251', '0.10676', '-0.05104', '-0.02615', '0.01612', '0.01918', '-0.00524', '-0.00654', '-0.02511', '0.00051']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.130% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.130% | ğŸ† Best Train Accuracy: 99.130%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.130% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.05it/s, Test_acc=67.1, Test_loss=2.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.070% | ğŸ† Best Test Accuracy: 67.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\FFTGated-No_FFTGated\\Results\\CIFAR100_Test_no_FFT_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "Best Test Accuracy:  67.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 11. TRAIN MODEL WITH SHEDULAR | XXX ----------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Set Seed for Reproducibility BEFORE training starts\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)  \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "# âœ… Training Loop\n",
    "num_epochs = 100 # Example: Set the total number of epochs\n",
    "for epoch in range(start_epoch, num_epochs):   # Runs training for 100 epochs\n",
    "\n",
    "    train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler, WARMUP_ACTIVATION_EPOCHS) # âœ… Pass required arguments\n",
    "\n",
    "    test(epoch)  # âœ… Test the model\n",
    "    tqdm.write(\"\")  # âœ… Clear leftover progress bar from test()\n",
    "\n",
    "\n",
    "print(\"Best Test Accuracy: \", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad23ab3-77b7-436e-8d48-ad037c39e6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'âœ… Annotated comparison plots with BEST accuracy markers saved to ./Results/Plots'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 550x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 13. FFT GATE VS NO_FFT GATE | XXX ------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.patheffects as path_effects\n",
    "import os\n",
    "\n",
    "def read_test_log(file_path):\n",
    "    test_loss_history = []\n",
    "    test_acc_history = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if \"Test Loss\" in line and \"Test Acc\" in line:\n",
    "                try:\n",
    "                    loss = float(line.split(\"Test Loss:\")[1].split(\"|\")[0].strip())\n",
    "                    acc = float(line.split(\"Test Acc:\")[1].split(\"%\")[0].strip())\n",
    "                    test_loss_history.append(loss)\n",
    "                    test_acc_history.append(acc)\n",
    "                except:\n",
    "                    continue\n",
    "    return test_loss_history, test_acc_history\n",
    "\n",
    "def plot_train_test_metrics(save_dir=\"./Results/Plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    FFT_test_log_path = f'./Results_FFTGated/CIFAR100_Test_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "    noFFT_test_log_path = f'./Results/CIFAR100_Test_{gate_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fft_test_loss, fft_test_acc = read_test_log(FFT_test_log_path)\n",
    "    nofft_test_loss, nofft_test_acc = read_test_log(noFFT_test_log_path)\n",
    "\n",
    "    num_epochs = min(len(fft_test_loss), len(nofft_test_loss))\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    COLOR_SCALE = ['#00295B', '#CF0A66']  # FFT, noFFT\n",
    "    rcParams.update({\n",
    "        \"font.size\": 11,\n",
    "        \"axes.titlesize\": 11,\n",
    "        \"axes.labelsize\": 13,\n",
    "        \"xtick.labelsize\": 11,\n",
    "        \"ytick.labelsize\": 11,\n",
    "        \"axes.labelweight\": \"bold\",\n",
    "        \"xtick.color\": \"black\",\n",
    "        \"ytick.color\": \"black\",\n",
    "    })\n",
    "\n",
    "    # Custom tick values and axis ranges\n",
    "    custom_yticks_test_loss = [1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
    "    custom_yticks_test_Accu = [10, 20, 30, 40, 50, 60, 70]\n",
    "\n",
    "    custom_xticks_test_loss = [0, 20, 40, 60, 80, 100]\n",
    "    custom_xticks_test_Accu = [0, 20, 40, 60, 80, 100]\n",
    "\n",
    "    custom_yaxis_test_loss = [1.2, 4.2]\n",
    "    custom_yaxis_test_Accu = [35, 72]\n",
    "\n",
    "    custom_xaxis_test_loss = [0, 105]\n",
    "    custom_xaxis_test_Accu = [0, 105]\n",
    "\n",
    "    # Annotation offsets\n",
    "    y_offset_loss_fft = 0.2\n",
    "    y_offset_loss_nofft = 0.07\n",
    "    x_offset_loss_fft = 3.5\n",
    "    x_offset_loss_nofft = 3.5\n",
    "\n",
    "    y_offset_acc_fft = 1\n",
    "    y_offset_acc_nofft = 3.2\n",
    "    x_offset_acc_fft = 8.5\n",
    "    x_offset_acc_nofft = 6.5\n",
    "\n",
    "    # ğŸ”· Plot Loss\n",
    "    fig1, ax1 = plt.subplots(figsize=(5.5, 3.5))\n",
    "    ax1.plot(epochs, fft_test_loss[:num_epochs], label=\"Test Loss (FFT)\", color=COLOR_SCALE[0], linewidth=2)\n",
    "    ax1.plot(epochs, nofft_test_loss[:num_epochs], label=\"Test Loss (no_FFT)\", color=COLOR_SCALE[1], linestyle='--', linewidth=2)\n",
    "    ax1.set_xlabel(\"Epoch\", fontweight='bold')\n",
    "    ax1.set_ylabel(\"Loss\", fontweight='bold')\n",
    "    ax1.set_xticks(custom_xticks_test_loss)\n",
    "    ax1.set_yticks(custom_yticks_test_loss)\n",
    "    ax1.set_xlim(custom_xaxis_test_loss)\n",
    "    ax1.set_ylim(custom_yaxis_test_loss)\n",
    "    ax1.tick_params(axis='x', width=1.5)\n",
    "    ax1.tick_params(axis='y', width=1.5)\n",
    "    for label in ax1.get_xticklabels() + ax1.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    leg1 = ax1.legend(fontsize='small', loc=\"upper right\")\n",
    "    for text in leg1.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax1.plot(epochs[-1], fft_test_loss[-1], marker='o', color=COLOR_SCALE[0], markersize=4)\n",
    "    ax1.text(epochs[-1] - x_offset_loss_fft, fft_test_loss[-1] - y_offset_loss_fft,\n",
    "             f\"{fft_test_loss[-1]:.2f}\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "    ax1.plot(epochs[-1], nofft_test_loss[-1], marker='o', color=COLOR_SCALE[1], markersize=4)\n",
    "    ax1.text(epochs[-1] - x_offset_loss_nofft, nofft_test_loss[-1] + y_offset_loss_nofft,\n",
    "             f\"{nofft_test_loss[-1]:.2f}\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"compare_test_loss_fft_vs_nofft.svg\"),\n",
    "                format='svg', transparent=True, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # ğŸ”¶ Plot Accuracy â€” Marker at Best Accuracy\n",
    "    best_epoch_fft = fft_test_acc.index(max(fft_test_acc)) + 1\n",
    "    best_acc_fft = max(fft_test_acc)\n",
    "\n",
    "    best_epoch_nofft = nofft_test_acc.index(max(nofft_test_acc)) + 1\n",
    "    best_acc_nofft = max(nofft_test_acc)\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(5.5, 3.5))\n",
    "    ax2.plot(epochs, fft_test_acc[:num_epochs], label=\"FFT-Gated\", color=COLOR_SCALE[0], linewidth=2)\n",
    "    ax2.plot(epochs, nofft_test_acc[:num_epochs], label=\"No FFT Gate\", color=COLOR_SCALE[1], linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel(\"Epoch\", fontweight='bold')\n",
    "    ax2.set_ylabel(\"Test Accuracy (%)\", fontweight='bold')\n",
    "    ax2.set_xticks(custom_xticks_test_Accu)\n",
    "    ax2.set_yticks(custom_yticks_test_Accu)\n",
    "    ax2.set_xlim(custom_xaxis_test_Accu)\n",
    "    ax2.set_ylim(custom_yaxis_test_Accu)\n",
    "    ax2.tick_params(axis='x', width=1.5)\n",
    "    ax2.tick_params(axis='y', width=1.5)\n",
    "    for label in ax2.get_xticklabels() + ax2.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    leg2 = ax2.legend(fontsize='small', loc=\"lower right\")\n",
    "    for text in leg2.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # ax2.plot(best_epoch_fft, best_acc_fft, marker='o', color=COLOR_SCALE[0], markersize=4)\n",
    "    ax2.plot(\n",
    "    best_epoch_fft, best_acc_fft - 0.21,  # ğŸ’¡Option A: Shift vertically (down a bit)\n",
    "    marker='o',\n",
    "    color=COLOR_SCALE[0],                    # fill color (pink)\n",
    "    markersize=5.5,                          # size of marker\n",
    "    markeredgecolor='black',                 # outline color\n",
    "    markeredgewidth=1                        # thickness of outline\n",
    ")    \n",
    "    ax2.text(best_epoch_fft - x_offset_acc_fft, best_acc_fft + y_offset_acc_fft,\n",
    "             f\"{best_acc_fft:.2f}%\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()]) \n",
    "    # ax2.plot(best_epoch_nofft, best_acc_nofft, marker='o', color=COLOR_SCALE[1], markersize=4)\n",
    "    ax2.plot(\n",
    "    best_epoch_nofft, best_acc_nofft - 0.4,  # ğŸ’¡Option A: Shift vertically (down a bit)\n",
    "    marker='o',\n",
    "    color=COLOR_SCALE[1],                    # fill color (pink)\n",
    "    markersize=5.5,                          # size of marker\n",
    "    markeredgecolor='black',                 # outline color\n",
    "    markeredgewidth=1                        # thickness of outline\n",
    ")    \n",
    "    ax2.text(best_epoch_nofft - x_offset_acc_nofft, best_acc_nofft - y_offset_acc_nofft,\n",
    "             f\"{best_acc_nofft:.2f}%\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"compare_test_accuracy_fft_vs_nofft.svg\"),\n",
    "                format='svg', transparent=True, bbox_inches='tight')\n",
    "    # plt.close()\n",
    "\n",
    "    return f\"âœ… Annotated comparison plots with BEST accuracy markers saved to {save_dir}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ”¶ğŸ”¹ğŸ”· CALL PLOT FUNCTION\n",
    "\n",
    "plot_train_test_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
