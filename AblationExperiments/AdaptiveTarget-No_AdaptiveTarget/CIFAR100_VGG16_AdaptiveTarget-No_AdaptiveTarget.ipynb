{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec717ade-481f-4062-b3eb-d062c28f6bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Current working directory: C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\n",
      "âœ… sys.path updated:\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\python310.zip\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\DLLs\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\n",
      "   ğŸ“‚ \n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\Pythonwin\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\models\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\activation\n",
      "âœ… FFTGate imported successfully!\n",
      "âœ… FFTGate instance created successfully!\n",
      "âœ… FFTGate_VGG imported successfully!\n",
      "CIFAR100 Training Script Initialized...\n",
      "Using device: cuda\n",
      "Parsed learning rate: 0.001 (type: <class 'float'>)\n",
      "Formatted learning rate for filenames: 0_001\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Length of training dataset: 50000\n",
      "Length of testing dataset: 10000\n",
      "Number of classes in CIFAR-100: 100\n",
      "==> Building model..\n",
      "âœ… Found 13 FFTGate layers.\n",
      "âœ… Collected 13 trainable activation parameters.\n",
      "   ğŸ”¹ Layer 0: FFTGate()\n",
      "   ğŸ”¹ Layer 1: FFTGate()\n",
      "   ğŸ”¹ Layer 2: FFTGate()\n",
      "   ğŸ”¹ Layer 3: FFTGate()\n",
      "   ğŸ”¹ Layer 4: FFTGate()\n",
      "   ğŸ”¹ Layer 5: FFTGate()\n",
      "   ğŸ”¹ Layer 6: FFTGate()\n",
      "   ğŸ”¹ Layer 7: FFTGate()\n",
      "   ğŸ”¹ Layer 8: FFTGate()\n",
      "   ğŸ”¹ Layer 9: FFTGate()\n",
      "   ğŸ”¹ Layer 10: FFTGate()\n",
      "   ğŸ”¹ Layer 11: FFTGate()\n",
      "   ğŸ”¹ Layer 12: FFTGate()\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 1.A. IMPORTS LIBRARIES | XXX -----------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\"\"\"Train CIFAR100 with PyTorch.\"\"\"\n",
    "\n",
    "# Python 2/3 compatibility\n",
    "# from __future__ import print_function\n",
    "\n",
    "\n",
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# PyTorch and related modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# torchvision for datasets and transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch_optimizer as torch_opt  # Use 'torch_opt' for torch_optimizer\n",
    "from timm.scheduler import CosineLRScheduler \n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Define currect working directory to ensure on right directory\n",
    "VGG16_PATH = r\"C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\"\n",
    "if os.getcwd() != VGG16_PATH:\n",
    "    os.chdir(VGG16_PATH)\n",
    "print(f\"âœ… Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# âœ… Define absolute paths\n",
    "PROJECT_PATH = VGG16_PATH\n",
    "MODELS_PATH = os.path.join(VGG16_PATH, \"models\")\n",
    "ACTIVATION_PATH = os.path.join(VGG16_PATH, \"activation\")\n",
    "# PAU_PATH = os.path.join(VGG16_PATH, \"pau\")\n",
    "\n",
    "# âœ… Ensure necessary paths are in sys.path\n",
    "for path in [PROJECT_PATH, MODELS_PATH, ACTIVATION_PATH]:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "# âœ… Print updated sys.path for debugging\n",
    "print(\"âœ… sys.path updated:\")\n",
    "for path in sys.path:\n",
    "    print(\"   ğŸ“‚\", path)\n",
    "\n",
    "# âœ… Import FFTGate (Check if the module exists)\n",
    "try:\n",
    "    from activation.FFTGate import FFTGate  # type: ignore\n",
    "    print(\"âœ… FFTGate imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ Import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'Activation4.py' exists inside: {ACTIVATION_PATH}\")\n",
    "\n",
    "# âœ… Test if FFTGate is callable\n",
    "try:\n",
    "    activation_test = FFTGate()\n",
    "    print(\"âœ… FFTGate instance created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error while initializing FFTGate: {e}\")\n",
    "\n",
    "# âœ… Now import FFTGate_VGG (Ensure module exists inside models/)\n",
    "try:\n",
    "    from models.FFTGate_VGG import FFTGate_VGG  # type: ignore\n",
    "    print(\"âœ… FFTGate_VGG imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ FFTGate_VGG import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'FFTGate_VGG.py' exists inside: {MODELS_PATH}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 1.B. SEEDING FOR REPRODUCIBILITY | XXX -------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def set_seed_torch(seed):\n",
    "    torch.manual_seed(seed)                          \n",
    "\n",
    "\n",
    "\n",
    "def set_seed_main(seed):\n",
    "    random.seed(seed)                                ## Python's random module\n",
    "    np.random.seed(seed)                             ## NumPy's random module\n",
    "    torch.cuda.manual_seed(seed)                     ## PyTorch's random module for CUDA\n",
    "    torch.cuda.manual_seed_all(seed)                 ## Seed for all CUDA devices\n",
    "    torch.backends.cudnn.deterministic = True        ## Ensure deterministic behavior for CuDNN\n",
    "    torch.backends.cudnn.benchmark = False           ## Disable CuDNN's autotuning for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)   \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (Optional) Import Optimizers - Uncomment as needed\n",
    "# from Opt import opt\n",
    "# from diffGrad import diffGrad\n",
    "# from diffRGrad import diffRGrad, SdiffRGrad, BetaDiffRGrad, Beta12DiffRGrad, BetaDFCDiffRGrad\n",
    "# from RADAM import Radam, BetaRadam\n",
    "# from BetaAdam import BetaAdam, BetaAdam1, BetaAdam2, BetaAdam3, BetaAdam4, BetaAdam5, BetaAdam6, BetaAdam7, BetaAdam4A\n",
    "# from AdamRM import AdamRM, AdamRM1, AdamRM2, AdamRM3, AdamRM4, AdamRM5\n",
    "# from sadam import sadam\n",
    "# from SdiffGrad import SdiffGrad\n",
    "# from SRADAM import SRADAM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 2. DEFINE MODEL Lr | XXX ---------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Main Execution (Placeholder)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"CIFAR100 Training Script Initialized...\")\n",
    "    # Add your training pipeline here\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Argument parser to get user inputs\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR100 Training')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "\n",
    "args, unknown = parser.parse_known_args()  # Avoids Jupyter argument issues\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Ensure lr is correctly parsed\n",
    "lr = args.lr  # Get learning rate from argparse\n",
    "lr_str = str(lr).replace('.', '_')  # Convert to string and replace '.' for filenames\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Parsed learning rate: {lr} (type: {type(lr)})\")\n",
    "print(f\"Formatted learning rate for filenames: {lr_str}\")\n",
    "\n",
    "# Initialize training variables\n",
    "best_acc = 0  # Best test accuracy\n",
    "start_epoch = 0  # Start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 3. LOAD DATASET | XXX ------------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "bs = 64 #set batch size\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Length of train and test datasets\n",
    "len_train = len(trainset)\n",
    "len_test = len(testset)\n",
    "print(f\"Length of training dataset: {len_train}\")\n",
    "print(f\"Length of testing dataset: {len_test}\")\n",
    "\n",
    "# âœ… Print number of classes\n",
    "num_classes_Print = len(trainset.classes)\n",
    "print(f\"Number of classes in CIFAR-100: {num_classes_Print}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 4. DYNAMIC REGULARIZATION| XXX ---------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def apply_dynamic_regularization(inputs, feature_activations, epoch,\n",
    "                                  prev_params, layer_index_map, batch_idx):\n",
    "\n",
    "\n",
    "    global activation_layers  # âœ… Reference already-collected layers\n",
    "\n",
    "    # âœ… Print gamma1 stats early in training for monitoring\n",
    "    if batch_idx == 0 and epoch <= 4:\n",
    "        print(f\"\\nğŸš¨ ENTERED apply_dynamic_regularization | Epoch={epoch} | Batch={batch_idx}\", flush=True)\n",
    "\n",
    "        # ğŸ§  Print gamma1 details\n",
    "        all_layer_info = []\n",
    "        for idx, layer in enumerate(activation_layers):\n",
    "            param = getattr(layer, \"gamma1\")\n",
    "            all_layer_info.append(f\"Layer {idx}: ID={id(param)} | Mean={param.mean().item():.5f}\")\n",
    "        print(\"ğŸ§  GAMMA1 INFO:\", \" | \".join(all_layer_info), flush=True)\n",
    "\n",
    "    # âœ… Initialize gamma1 regularization accumulator\n",
    "    gamma1_reg = 0.0\n",
    "\n",
    "    # âœ… Compute batch std and define regularization strength\n",
    "    batch_std = torch.std(inputs) + 1e-6\n",
    "    regularization_strength = 0.05 if epoch < 40 else (0.01 if epoch < 60 else 0.005)\n",
    "\n",
    "    # âœ… Track layers where noise is injected (informative)\n",
    "    noisy_layers = []\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        if idx not in layer_index_map:\n",
    "            continue\n",
    "\n",
    "        prev_layer_params = prev_params[layer_index_map[idx]]\n",
    "        param_name = \"gamma1\"\n",
    "        param = getattr(layer, param_name)\n",
    "        prev_param = prev_layer_params[param_name]\n",
    "\n",
    "        # # âœ… Target based on input stats\n",
    "        # target = compute_target(param_name, batch_std)\n",
    "\n",
    "        # # âœ… Adaptive Target Regularization\n",
    "        # gamma1_reg += regularization_strength * (param - target).pow(2).mean() * 1.2\n",
    "\n",
    "        # âœ… Adaptive Cohesion Regularization\n",
    "        cohesion = (param - prev_param).pow(2)\n",
    "        gamma1_reg += 0.005 * cohesion.mean()\n",
    "\n",
    "        # âœ… Adaptive Noise Regularization\n",
    "        epoch_AddNoise = 50\n",
    "        if epoch > epoch_AddNoise:\n",
    "            param_variation = torch.abs(param - prev_param).mean()\n",
    "            if param_variation < 0.015:\n",
    "                noise = (0.001 + 0.0004 * batch_std.item()) * torch.randn_like(param)\n",
    "                penalty = (param - (prev_param + noise)).pow(2).sum()\n",
    "                gamma1_reg += 0.00015 * penalty\n",
    "                noisy_layers.append(f\"{idx} (Î”={param_variation.item():.5f})\") # Collect index and variation\n",
    "\n",
    "    # âœ… Print noise injection summary\n",
    "    if batch_idx == 0 and epoch <= (epoch_AddNoise + 4) and noisy_layers:\n",
    "        print(f\"ğŸ”¥ Stable Noise Injected | Epoch {epoch} | Batch {batch_idx} | Layers: \" + \", \".join(noisy_layers), flush=True)\n",
    "    mags = feature_activations.abs().mean(dim=(0, 2, 3))\n",
    "    m = mags / mags.sum()\n",
    "    gamma1_reg += 0.005 * (-(m * torch.log(m + 1e-6)).sum())\n",
    "\n",
    "    return gamma1_reg\n",
    "\n",
    "\n",
    "def compute_target(param_name, batch_std):\n",
    "    if param_name == \"gamma1\":\n",
    "        return 2.0 + 0.2 * batch_std.item()  \n",
    "\n",
    "    raise ValueError(f\"Unknown param {param_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 5. INITIALIZE MODEL | XXX --------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "#net = Elliott_VGG('VGG16'); net1 = 'Elliott_VGG16'\n",
    "#net = GELU_MobileNet(); net1 = 'GELU_MobileNet'\n",
    "#net = GELU_SENet18(); net1 = 'GELU_SENet18'\n",
    "#net = PDELU_ResNet50(); net1 = 'PDELU_ResNet50'\n",
    "# net = Sigmoid_GoogLeNet(); net1 = 'Sigmoid_GoogLeNet'\n",
    "#net = GELU_DenseNet121(); net1 = 'GELU_DenseNet121'\n",
    "# net = ReLU_VGG('VGG16'); net1 = 'ReLU_VGG16'\n",
    "net = FFTGate_VGG('VGG16'); net1 = 'FFTGate_VGG16'\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9); optimizer1 = 'SGDM5'\n",
    "#optimizer = optim.Adagrad(net.parameters()); optimizer1 = 'AdaGrad'\n",
    "#optimizer = optim.Adadelta(net.parameters()); optimizer1 = 'AdaDelta'\n",
    "#optimizer = optim.RMSprop(net.parameters()); optimizer1 = 'RMSprop'\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr); optimizer1 = 'Adam'\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr, amsgrad=True); optimizer1 = 'amsgrad'\n",
    "#optimizer = diffGrad(net.parameters(), lr=args.lr); optimizer1 = 'diffGrad'\n",
    "#optimizer = Radam(net.parameters(), lr=args.lr); optimizer1 = 'Radam'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 6. INITIALIZE ACTIVATION PARAMETERS, OPTIMIZERS & SCHEDULERS | XXX ---------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# âœ… Step 1: Collect Activation Parameters from ALL Layers (Ensure Compatibility with DataParallel)\n",
    "if isinstance(net, torch.nn.DataParallel):\n",
    "    features = net.module.features\n",
    "else:\n",
    "    features = net.features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 2: Recursively search for FFTGate layers\n",
    "activation_params = []\n",
    "activation_layers = []\n",
    "\n",
    "for layer in features:\n",
    "    if isinstance(layer, FFTGate):  \n",
    "        activation_layers.append(layer)\n",
    "        activation_params.append(layer.gamma1)  # âœ… Only gamma1 is trainable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 3: Define Unfreeze Epoch\n",
    "unfreeze_activation_epoch = 1  # âœ… Change this value if needed\n",
    "# unfreeze_activation_epoch = 10  # âœ… Delay unfreezing until epoch 10\n",
    "\n",
    "\n",
    "# âœ… Define the warm-up epoch value\n",
    "# WARMUP_ACTIVATION_EPOCHS = 5  # The number of epochs for warm-up\n",
    "WARMUP_ACTIVATION_EPOCHS = 0  # The number of epochs for warm-up\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initially Freeze Activation Parameters\n",
    "for param in activation_params:\n",
    "    param.requires_grad = False  # ğŸš« Keep frozen before the unfreeze epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initialize Activation Optimizers (Using AdamW for Better Weight Decay)\n",
    "activation_optimizers = {\n",
    "    \"gamma1\": torch.optim.AdamW(activation_params, lr=0.0015, weight_decay=1e-6)  # ğŸ”º Reduce LR from 0.005 â†’ 0.0025\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 5: Initialize Activation Schedulers with Warm Restarts (Per Parameter Type)\n",
    "activation_schedulers = {\n",
    "    \"gamma1\": CosineAnnealingWarmRestarts(\n",
    "        activation_optimizers[\"gamma1\"],\n",
    "        T_0=10,      # Shorter cycle to explore aggressively\n",
    "        T_mult=2,    # Increase cycle length gradually\n",
    "        eta_min=5e-5  # âœ… recommended safer modification\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 6: Print collected activation layers and parameters\n",
    "if activation_layers and activation_params:\n",
    "    print(f\"âœ… Found {len(activation_layers)} FFTGate layers.\")\n",
    "    print(f\"âœ… Collected {len(activation_params)} trainable activation parameters.\")\n",
    "    \n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        print(f\"   ğŸ”¹ Layer {idx}: {layer}\")\n",
    "\n",
    "elif activation_layers and not activation_params:\n",
    "    print(f\"âš  Warning: Found {len(activation_layers)} FFTGate layers, but no trainable parameters were collected.\")\n",
    "\n",
    "elif activation_params and not activation_layers:\n",
    "    print(f\"âš  Warning: Collected {len(activation_params)} activation parameters, but no FFTGate layers were recorded.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš  Warning: No FFTGate layers or activation parameters found! Skipping activation optimizer.\")\n",
    "    activation_optimizers = None\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 7. INITIALIZE MAIN OPTIMIZER SCHEDULER | XXX -------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Step 6: Define MultiStepLR for Main Optimizer\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "main_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 8. MODEL CHECK POINT | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists('checkpoint'):\n",
    "    os.makedirs('checkpoint')\n",
    "\n",
    "if not os.path.exists('Results'):\n",
    "    os.makedirs('Results')\n",
    "\n",
    "# Construct checkpoint path\n",
    "checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr}_{net1}_{optimizer1}.t7'\n",
    "\n",
    "# Resume checkpoint only if file exists\n",
    "if args.resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 9. DEFINE TRAIN LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Used for naming files \n",
    "target_mode = \"no_target\"  # Options: \"no_target\", \"target\"\n",
    "\n",
    "# Training\n",
    "\n",
    "def train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler , WARMUP_ACTIVATION_EPOCHS):\n",
    "    global train_loss_history, best_train_acc, prev_params, recent_test_acc, gamma1_history, activation_layers, test_acc_history, train_acc_history, target_mode  # ğŸŸ¢ğŸŸ¢ğŸŸ¢\n",
    "\n",
    "    if epoch == 0:\n",
    "        train_loss_history = []\n",
    "        train_acc_history = []\n",
    "        best_train_acc = 0.0\n",
    "        recent_test_acc = 0.0\n",
    "        gamma1_history = {}         # âœ… Initialize history\n",
    "        test_acc_history = []       # âœ… test accuracy history\n",
    "\n",
    "\n",
    "\n",
    "    prev_params = {}\n",
    "    layer_index_map = {idx: idx for idx in range(len(activation_layers))}  \n",
    "\n",
    "    # âœ… Cache previous gamma1 values from activation layers\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_accuracy = 0.0\n",
    "\n",
    "    # âœ… Initialize log history\n",
    "    log_history = []\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Define path to store Training log\n",
    "    save_paths = {\n",
    "       \n",
    "        \"log_history\": f\"C:\\\\Users\\\\emeka\\\\Research\\\\ModelCUDA\\\\Big_Data_Journal\\\\Comparison\\\\Code\\\\Paper\\\\github2\\\\AblationExperiments\\\\AdaptiveTarget-No_AdaptiveTarget\\\\Results\\\\FFTGate\\\\FFTGate_training_logs.txt\"  # âœ… Training log_history \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 1: Unfreeze Activation Parameters (Only Once Per Epoch)\n",
    "    if epoch == unfreeze_activation_epoch:\n",
    "        print(\"\\nğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\")\n",
    "        for layer in net.module.features if isinstance(net, torch.nn.DataParallel) else net.features:\n",
    "            if isinstance(layer, FFTGate):   \n",
    "                layer.gamma1.requires_grad = True  # âœ… Only gamma1 is trainable\n",
    "        print(\"âœ… Activation Parameters Unfrozen! ğŸš€\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 2: Gradual Warm-up for Activation Learning Rates (AFTER Unfreezing)\n",
    "    warmup_start = unfreeze_activation_epoch  # ğŸ”¹ Start warm-up when unfreezing happens\n",
    "    warmup_end = unfreeze_activation_epoch + WARMUP_ACTIVATION_EPOCHS  # ğŸ”¹ End warm-up period\n",
    "\n",
    "    # âœ… Adjust learning rates **only** during the warm-up phase\n",
    "    if warmup_start <= epoch < warmup_end:\n",
    "        warmup_factor = (epoch - warmup_start + 1) / WARMUP_ACTIVATION_EPOCHS  \n",
    "\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            for param_group in act_scheduler.optimizer.param_groups:\n",
    "                if \"initial_lr\" not in param_group:\n",
    "                    param_group[\"initial_lr\"] = param_group[\"lr\"]  # ğŸ”¹ Store initial LR\n",
    "                param_group[\"lr\"] = param_group[\"initial_lr\"] * warmup_factor  # ğŸ”¹ Scale LR\n",
    "\n",
    "        # âœ… Debugging output to track warm-up process\n",
    "        print(f\"ğŸ”¥ Warm-up Epoch {epoch}: Scaling LR by {warmup_factor:.3f}\")\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            print(f\"  ğŸ”¹ {name} LR: {act_scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    activation_history = []  # ğŸ”´ Initialize empty history at start of epoch (outside batch loop)\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Training Loop\n",
    "    with tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch}\") as progress:\n",
    "        for batch_idx, (inputs, targets) in progress:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # zero_grad activation parameter\n",
    "            for opt in activation_optimizers.values():\n",
    "                opt.zero_grad()\n",
    "\n",
    "\n",
    "            # âœ… Forward Pass\n",
    "            outputs = net(inputs, epoch=epoch, train_accuracy=train_accuracy, targets=targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            \n",
    "            feature_activations = features(inputs)  # Feature activations\n",
    "\n",
    "\n",
    "            # âœ… Collect Activation History | âœ… Per-layer mean activations\n",
    "            batch_means = [layer.saved_output.mean().item() for layer in activation_layers]\n",
    "            activation_history.extend(batch_means)\n",
    "\n",
    "            # âœ… Apply Decay strategy to history for each activation layer\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    if isinstance(layer, FFTGate):\n",
    "                        layer.decay_spectral_history(epoch, num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Compute Training Accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            train_accuracy = 100. * correct / total if total > 0 else 0.0  # Compute training accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Call Regularization Function for the Activation Parameter\n",
    "            if epoch > 0:\n",
    "                gamma1_reg = apply_dynamic_regularization(\n",
    "                    inputs, feature_activations, epoch,\n",
    "                    prev_params, layer_index_map, batch_idx\n",
    "                )\n",
    "                loss += gamma1_reg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… ğŸ¯ Adaptive Gradient Clipping of gamma1  \n",
    "            for layer in features:\n",
    "                if isinstance(layer, FFTGate):  # âœ… Ensure layer has gamma1 before clipping\n",
    "                    torch.nn.utils.clip_grad_norm_([layer.gamma1], max_norm=0.7)\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Apply Optimizer Step for Model Parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # âœ… Apply Optimizer Steps for Activation Parameters (Only if Unfrozen)\n",
    "            if epoch >= unfreeze_activation_epoch:\n",
    "                for opt in activation_optimizers.values():\n",
    "                    opt.step()\n",
    "\n",
    "\n",
    "            # âœ… Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Clamping of gamma1 (Applied AFTER Optimizer Step)\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    layer.gamma1.clamp_(0.1, 6.0)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Update progress bar\n",
    "            progress.set_postfix(Train_loss=round(train_loss / (batch_idx + 1), 3),\n",
    "                                 Train_acc=train_accuracy)  \n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step the main optimizer scheduler (ONLY for model parameters)\n",
    "    main_scheduler.step()\n",
    "\n",
    "    # âœ… Step the activation parameter schedulers (ONLY for activation parameters) | Epoch-wise stepping\n",
    "    if epoch >= unfreeze_activation_epoch:\n",
    "        for name, act_scheduler in activation_schedulers.items():  \n",
    "            act_scheduler.step()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… ONLY update prev_params here AFTER all updates | âœ… Update prev_params AFTER training epoch\n",
    "    for idx, layer in enumerate(activation_layers):      \n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Logging Activation Parameters & Gradients\n",
    "    last_batch_grads = {\"Gamma1 Grad\": []}\n",
    "    current_params = {\"Gamma1\": []}\n",
    "\n",
    "    for layer in features:\n",
    "        if isinstance(layer, FFTGate):  \n",
    "            # âœ… Convert gradients to scalar floats and format to 5 decimal places (removes device='cuda:0' and tensor(...))\n",
    "            last_batch_grads[\"Gamma1 Grad\"].append(f\"{layer.gamma1.grad.item():.5f}\" if layer.gamma1.grad is not None else \"None\")\n",
    "\n",
    "            # âœ… Collect current parameter values (already scalar), formatted to 5 decimal places\n",
    "            current_params[\"Gamma1\"].append(f\"{layer.gamma1.item():.5f}\")\n",
    "\n",
    "    # âœ… Build log message (showing params and gradients for ALL layers)\n",
    "    log_msg = (\n",
    "        f\"Epoch {epoch}: M_Optimizer LR => {optimizer.param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1 LR => {activation_optimizers['gamma1'].param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1: {current_params['Gamma1']} | \"\n",
    "        f\"Gamma1 Grad: {last_batch_grads['Gamma1 Grad']}\"\n",
    "    )\n",
    "\n",
    "    log_history.append(log_msg)\n",
    "    print(log_msg)  # âœ… Prints only once per epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (Clear old logs)\n",
    "    if epoch == 0:  # âœ… Only clear at the start of training\n",
    "        with open(save_paths[\"log_history\"], \"w\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Save logs once per epoch (Append new logs)\n",
    "    if log_history:\n",
    "        with open(save_paths[\"log_history\"], \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_history) + \"\\n\")         # âœ… Ensure each entry is on a new line\n",
    "        print(f\"ğŸ“œ Logs saved to {save_paths['log_history']}!\")  # âœ… Only prints once per epoch\n",
    "    else:\n",
    "        print(\"âš  No logs to save!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final training accuracy for the epoch\n",
    "    final_train_loss = train_loss / len(trainloader)\n",
    "    final_train_acc = 100. * correct / total\n",
    "\n",
    "    # âœ… Append to history\n",
    "    train_loss_history.append(final_train_loss)\n",
    "\n",
    "    # Append per-epoch training accuracy\n",
    "    train_acc_history.append(final_train_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save training results (without affecting best accuracy tracking)\n",
    "    train_results_path = f'./Results/CIFAR100_Train_{target_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "    # âœ… Clear the log file at the start of training (Epoch 0)\n",
    "    if epoch == 0 and os.path.exists(train_results_path):\n",
    "        with open(train_results_path, 'w') as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs only once\n",
    "\n",
    "    # âœ… Append new training results for each epoch\n",
    "    with open(train_results_path, 'a') as f:\n",
    "        f.write(f\"Epoch {epoch} | Train Loss: {final_train_loss:.3f} | Train Acc: {final_train_acc:.3f}%\\n\")\n",
    "\n",
    "    if final_train_acc > best_train_acc:\n",
    "        best_train_acc = final_train_acc  # âœ… Update best training accuracy\n",
    "        print(f\"ğŸ† New Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "    # âœ… Append the best training accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):  # Only log once at the final epoch\n",
    "        with open(train_results_path, 'a') as f:\n",
    "            f.write(f\"\\nğŸ† Best Training Accuracy: {best_train_acc:.3f}%\\n\")  \n",
    "\n",
    "    # âœ… Print both Final and Best Training Accuracy\n",
    "    print(f\"ğŸ“Š Train Accuracy: {final_train_acc:.3f}% | ğŸ† Best Train Accuracy: {best_train_acc:.3f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"ğŸ“œ Training logs saved to {train_results_path}!\")\n",
    "    print(f\"ğŸ† Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"ğŸ“ Sizes â†’ ActivationHist: {len(activation_history)} | TestAccHist: {len(test_acc_history)} | TrainLossHist: {len(train_loss_history)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # return final_train_loss, final_train_acc, feature_activations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 10. DEFINE TEST LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(epoch, save_results=True):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and optionally saves the results.\n",
    "    \n",
    "    Args:\n",
    "    - epoch (int): The current epoch number.\n",
    "    - save_results (bool): Whether to save results to a file.\n",
    "\n",
    "    Returns:\n",
    "    - acc (float): Test accuracy percentage.\n",
    "    \"\"\"\n",
    "    global best_acc, val_accuracy, target_mode  \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # âœ… Ensure activation function parameters are clamped before evaluation\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(testloader), total=len(testloader), desc=f\"Testing Epoch {epoch}\") as progress:\n",
    "            for batch_idx, (inputs, targets) in progress:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                # âœ… Pass validation accuracy to activation function\n",
    "                val_accuracy = 100. * correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "                # âœ… Update progress bar with loss & accuracy\n",
    "                progress.set_postfix(Test_loss=round(test_loss / (batch_idx + 1), 3),\n",
    "                                     Test_acc=round(val_accuracy, 3))\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final test accuracy\n",
    "    final_test_loss = test_loss / len(testloader)\n",
    "    final_test_acc = 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Ensure \"Results\" folder exists (just like training logs)\n",
    "    results_dir = os.path.join(PROJECT_PATH, \"Results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # âœ… Define log file path for test results\n",
    "    test_results_path = os.path.join(results_dir, f'CIFAR100_Test_{target_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt')\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (clear old logs)\n",
    "    if epoch == 0:\n",
    "        with open(test_results_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Append new test results for each epoch (same style as training)\n",
    "    with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Epoch {epoch} | Test Loss: {final_test_loss:.3f} | Test Acc: {final_test_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save checkpoint if accuracy improves (does NOT interfere with logging)\n",
    "    if final_test_acc > best_acc:\n",
    "        print('ğŸ† Saving best model...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': final_test_acc,  # âœ… Ensures the best test accuracy is saved in checkpoint\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Ensure checkpoint directory exists\n",
    "        checkpoint_dir = \"checkpoint\"\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "        # âœ… Format learning rate properly before saving filename\n",
    "        lr_str = str(lr).replace('.', '_')\n",
    "        checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr_str}_{net1}_{optimizer1}.t7'\n",
    "        torch.save(state, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "        best_acc = final_test_acc  # âœ… Update best accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Append the best test accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):\n",
    "        with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\nğŸ† Best Test Accuracy: {best_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Print both Final and Best Test Accuracy (always executed)\n",
    "    print(f\"ğŸ“Š Test Accuracy: {final_test_acc:.3f}% | ğŸ† Best Test Accuracy: {best_acc:.3f}%\")\n",
    "    print(f\"ğŸ“œ Test logs saved to {test_results_path}!\")\n",
    "\n",
    "\n",
    "    global recent_test_acc\n",
    "    recent_test_acc = final_test_acc  # Capture latest test accuracy for next train() call | Store latest test accuracy\n",
    "\n",
    "    test_acc_history.append(final_test_acc)\n",
    "\n",
    "    return final_test_acc  # âœ… Return the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573c41c-6384-4d98-8244-3b3db8e97190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:24<00:00, 31.99it/s, Train_acc=3.32, Train_loss=4.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000'] | Gamma1 Grad: ['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 3.318% (Updated)\n",
      "ğŸ“Š Train Accuracy: 3.318% | ğŸ† Best Train Accuracy: 3.318%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 3.318% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.89it/s, Test_acc=5.57, Test_loss=4.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 5.570% | ğŸ† Best Test Accuracy: 5.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "\n",
      "ğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\n",
      "âœ… Activation Parameters Unfrozen! ğŸš€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=1 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1740812433376 | Mean=1.50000 | Layer 1: ID=1740812434256 | Mean=1.50000 | Layer 2: ID=1740812435136 | Mean=1.50000 | Layer 3: ID=1740811565584 | Mean=1.50000 | Layer 4: ID=1740812228928 | Mean=1.50000 | Layer 5: ID=1740779637392 | Mean=1.50000 | Layer 6: ID=1740779637152 | Mean=1.50000 | Layer 7: ID=1740779636512 | Mean=1.50000 | Layer 8: ID=1740779635232 | Mean=1.50000 | Layer 9: ID=1741479133328 | Mean=1.50000 | Layer 10: ID=1741479134208 | Mean=1.50000 | Layer 11: ID=1741479135168 | Mean=1.50000 | Layer 12: ID=1741479136048 | Mean=1.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.92it/s, Train_acc=6.89, Train_loss=3.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['1.62149', '1.70355', '1.57857', '1.52780', '1.48826', '1.44972', '1.41787', '1.43428', '1.42900', '1.45857', '1.36140', '1.38568', '1.34046'] | Gamma1 Grad: ['0.02444', '-0.00401', '0.02007', '-0.00367', '-0.01653', '-0.01799', '-0.02253', '0.00090', '-0.02339', '-0.01909', '-0.00756', '-0.04755', '0.00403']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 6.892% (Updated)\n",
      "ğŸ“Š Train Accuracy: 6.892% | ğŸ† Best Train Accuracy: 6.892%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 6.892% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.02it/s, Test_acc=7.84, Test_loss=3.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 7.840% | ğŸ† Best Test Accuracy: 7.840%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=2 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1740812433376 | Mean=1.62149 | Layer 1: ID=1740812434256 | Mean=1.70355 | Layer 2: ID=1740812435136 | Mean=1.57857 | Layer 3: ID=1740811565584 | Mean=1.52780 | Layer 4: ID=1740812228928 | Mean=1.48826 | Layer 5: ID=1740779637392 | Mean=1.44972 | Layer 6: ID=1740779637152 | Mean=1.41787 | Layer 7: ID=1740779636512 | Mean=1.43428 | Layer 8: ID=1740779635232 | Mean=1.42900 | Layer 9: ID=1741479133328 | Mean=1.45857 | Layer 10: ID=1741479134208 | Mean=1.36140 | Layer 11: ID=1741479135168 | Mean=1.38568 | Layer 12: ID=1741479136048 | Mean=1.34046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.63it/s, Train_acc=10.6, Train_loss=3.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['1.79554', '1.95681', '1.77666', '1.58175', '1.52110', '1.38685', '1.33921', '1.41416', '1.38207', '1.41924', '1.23744', '1.42220', '1.16919'] | Gamma1 Grad: ['-0.02758', '-0.00124', '0.00605', '-0.01545', '-0.01844', '-0.01280', '-0.01697', '0.01768', '-0.02750', '-0.01356', '0.01835', '0.01467', '-0.05312']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 10.584% (Updated)\n",
      "ğŸ“Š Train Accuracy: 10.584% | ğŸ† Best Train Accuracy: 10.584%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 10.584% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.10it/s, Test_acc=13.4, Test_loss=3.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 13.430% | ğŸ† Best Test Accuracy: 13.430%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=3 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1740812433376 | Mean=1.79554 | Layer 1: ID=1740812434256 | Mean=1.95681 | Layer 2: ID=1740812435136 | Mean=1.77666 | Layer 3: ID=1740811565584 | Mean=1.58175 | Layer 4: ID=1740812228928 | Mean=1.52110 | Layer 5: ID=1740779637392 | Mean=1.38685 | Layer 6: ID=1740779637152 | Mean=1.33921 | Layer 7: ID=1740779636512 | Mean=1.41416 | Layer 8: ID=1740779635232 | Mean=1.38207 | Layer 9: ID=1741479133328 | Mean=1.41924 | Layer 10: ID=1741479134208 | Mean=1.23744 | Layer 11: ID=1741479135168 | Mean=1.42220 | Layer 12: ID=1741479136048 | Mean=1.16919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.04it/s, Train_acc=15, Train_loss=3.38]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['1.98998', '2.19797', '1.97166', '1.68982', '1.65595', '1.40768', '1.28233', '1.36814', '1.27393', '1.37339', '1.13249', '1.37989', '1.11099'] | Gamma1 Grad: ['0.01566', '0.01194', '-0.00829', '0.01546', '0.00996', '0.01476', '-0.01447', '0.00297', '-0.05540', '-0.04216', '-0.01724', '-0.17542', '-0.10998']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 15.020% (Updated)\n",
      "ğŸ“Š Train Accuracy: 15.020% | ğŸ† Best Train Accuracy: 15.020%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 15.020% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.53it/s, Test_acc=18.7, Test_loss=3.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 18.740% | ğŸ† Best Test Accuracy: 18.740%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=4 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=1740812433376 | Mean=1.98998 | Layer 1: ID=1740812434256 | Mean=2.19797 | Layer 2: ID=1740812435136 | Mean=1.97166 | Layer 3: ID=1740811565584 | Mean=1.68982 | Layer 4: ID=1740812228928 | Mean=1.65595 | Layer 5: ID=1740779637392 | Mean=1.40768 | Layer 6: ID=1740779637152 | Mean=1.28233 | Layer 7: ID=1740779636512 | Mean=1.36814 | Layer 8: ID=1740779635232 | Mean=1.27393 | Layer 9: ID=1741479133328 | Mean=1.37339 | Layer 10: ID=1741479134208 | Mean=1.13249 | Layer 11: ID=1741479135168 | Mean=1.37989 | Layer 12: ID=1741479136048 | Mean=1.11099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.67it/s, Train_acc=20.1, Train_loss=3.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.17789', '2.40724', '2.14625', '1.80848', '1.80226', '1.53097', '1.26171', '1.39170', '1.19210', '1.34526', '1.02741', '1.33835', '1.14901'] | Gamma1 Grad: ['0.03058', '0.00198', '-0.00213', '-0.02517', '0.01158', '-0.01911', '0.01592', '-0.00796', '0.00381', '0.01257', '0.00933', '0.04534', '0.08607']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 20.082% (Updated)\n",
      "ğŸ“Š Train Accuracy: 20.082% | ğŸ† Best Train Accuracy: 20.082%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 20.082% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.47it/s, Test_acc=22.8, Test_loss=2.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 22.770% | ğŸ† Best Test Accuracy: 22.770%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.84it/s, Train_acc=24.2, Train_loss=2.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.34327', '2.59583', '2.31983', '1.92208', '1.95698', '1.69800', '1.25261', '1.41459', '1.15466', '1.33358', '0.89874', '1.30864', '1.19760'] | Gamma1 Grad: ['-0.00646', '0.01843', '0.01438', '-0.00480', '-0.00076', '-0.00936', '0.01222', '-0.00271', '-0.02449', '0.03784', '0.01240', '-0.07884', '-0.03202']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 24.160% (Updated)\n",
      "ğŸ“Š Train Accuracy: 24.160% | ğŸ† Best Train Accuracy: 24.160%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 24.160% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.17it/s, Test_acc=27.9, Test_loss=2.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 27.930% | ğŸ† Best Test Accuracy: 27.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.13it/s, Train_acc=28.2, Train_loss=2.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.51118', '2.73937', '2.44822', '2.03097', '2.08662', '1.83455', '1.31196', '1.47675', '1.15521', '1.35079', '0.79578', '1.28665', '1.24996'] | Gamma1 Grad: ['0.00132', '0.00088', '0.02162', '-0.00772', '-0.01046', '-0.00465', '-0.00631', '-0.01721', '-0.00557', '0.01068', '-0.02918', '-0.04410', '-0.01798']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 28.180% (Updated)\n",
      "ğŸ“Š Train Accuracy: 28.180% | ğŸ† Best Train Accuracy: 28.180%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 28.180% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.32it/s, Test_acc=29.1, Test_loss=2.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 29.150% | ğŸ† Best Test Accuracy: 29.150%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.96it/s, Train_acc=32.4, Train_loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.63288', '2.88112', '2.53680', '2.11542', '2.19710', '1.97022', '1.38450', '1.55277', '1.17680', '1.36931', '0.69881', '1.26194', '1.26744'] | Gamma1 Grad: ['-0.00348', '-0.00007', '0.00151', '0.00640', '0.00600', '0.01320', '0.00641', '0.02945', '0.00440', '0.01181', '0.00967', '-0.08904', '0.18284']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 32.396% (Updated)\n",
      "ğŸ“Š Train Accuracy: 32.396% | ğŸ† Best Train Accuracy: 32.396%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 32.396% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.29it/s, Test_acc=33.2, Test_loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 33.170% | ğŸ† Best Test Accuracy: 33.170%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.60it/s, Train_acc=36.2, Train_loss=2.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.71582', '3.01031', '2.62502', '2.17870', '2.30792', '2.09640', '1.43013', '1.65041', '1.20089', '1.38079', '0.62556', '1.24071', '1.29162'] | Gamma1 Grad: ['-0.01740', '-0.01904', '-0.00170', '0.02708', '0.00698', '-0.01434', '-0.00071', '0.00760', '0.05306', '0.00041', '-0.01943', '0.05678', '0.10057']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 36.164% (Updated)\n",
      "ğŸ“Š Train Accuracy: 36.164% | ğŸ† Best Train Accuracy: 36.164%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 36.164% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.48it/s, Test_acc=37.8, Test_loss=2.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 37.770% | ğŸ† Best Test Accuracy: 37.770%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.99it/s, Train_acc=39.4, Train_loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.81081', '3.10983', '2.69289', '2.22337', '2.36753', '2.19201', '1.47307', '1.74105', '1.25222', '1.41744', '0.57923', '1.21813', '1.32275'] | Gamma1 Grad: ['-0.00742', '-0.00904', '-0.00293', '-0.01577', '0.00607', '-0.00330', '0.00201', '0.00497', '-0.01490', '-0.03412', '-0.00932', '0.04972', '-0.01506']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 39.420% (Updated)\n",
      "ğŸ“Š Train Accuracy: 39.420% | ğŸ† Best Train Accuracy: 39.420%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 39.420% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.50it/s, Test_acc=42.6, Test_loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 42.570% | ğŸ† Best Test Accuracy: 42.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.13it/s, Train_acc=43, Train_loss=2.07]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.87993', '3.19554', '2.72793', '2.27331', '2.43190', '2.29007', '1.52145', '1.83977', '1.30461', '1.44345', '0.53229', '1.20637', '1.35982'] | Gamma1 Grad: ['-0.01120', '0.00121', '-0.00163', '-0.01004', '-0.00059', '0.01492', '0.01453', '0.01230', '-0.00273', '0.00275', '0.07143', '-0.02526', '-0.01813']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 42.962% (Updated)\n",
      "ğŸ“Š Train Accuracy: 42.962% | ğŸ† Best Train Accuracy: 42.962%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 42.962% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 10 | TrainLossHist: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.99it/s, Test_acc=43.1, Test_loss=2.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 43.140% | ğŸ† Best Test Accuracy: 43.140%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.48it/s, Train_acc=45.8, Train_loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.94173', '3.29599', '2.79941', '2.33181', '2.49057', '2.36227', '1.61774', '1.97176', '1.37712', '1.45526', '0.46870', '1.17398', '1.41610'] | Gamma1 Grad: ['-0.00037', '0.00730', '0.00143', '0.00708', '-0.01718', '-0.02096', '-0.01886', '-0.00219', '0.01797', '0.00296', '0.05022', '-0.04067', '0.07288']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 45.760% (Updated)\n",
      "ğŸ“Š Train Accuracy: 45.760% | ğŸ† Best Train Accuracy: 45.760%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 45.760% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.76it/s, Test_acc=46.5, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 46.530% | ğŸ† Best Test Accuracy: 46.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.27it/s, Train_acc=48.4, Train_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.98441', '3.37370', '2.83991', '2.36332', '2.53295', '2.43667', '1.66822', '2.08759', '1.46445', '1.48508', '0.42586', '1.16270', '1.48212'] | Gamma1 Grad: ['-0.00186', '0.00037', '0.00157', '0.01454', '0.00267', '-0.00226', '-0.01558', '0.00554', '-0.02023', '-0.01586', '-0.12027', '-0.00099', '0.06947']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 48.352% (Updated)\n",
      "ğŸ“Š Train Accuracy: 48.352% | ğŸ† Best Train Accuracy: 48.352%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 48.352% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.00it/s, Test_acc=47.9, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 47.890% | ğŸ† Best Test Accuracy: 47.890%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.52it/s, Train_acc=50.8, Train_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['3.01684', '3.45958', '2.87923', '2.40646', '2.54742', '2.50267', '1.71780', '2.19468', '1.53983', '1.50625', '0.38384', '1.18592', '1.51702'] | Gamma1 Grad: ['0.01383', '0.00501', '-0.00845', '0.03141', '-0.01029', '-0.00894', '0.01002', '0.00286', '0.00289', '0.00713', '-0.02909', '0.02214', '0.00258']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 50.768% (Updated)\n",
      "ğŸ“Š Train Accuracy: 50.768% | ğŸ† Best Train Accuracy: 50.768%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 50.768% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.48it/s, Test_acc=50.9, Test_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 50.880% | ğŸ† Best Test Accuracy: 50.880%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.17it/s, Train_acc=53, Train_loss=1.67]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['3.05984', '3.52052', '2.90401', '2.42693', '2.54027', '2.53394', '1.79440', '2.27656', '1.62018', '1.54787', '0.35863', '1.18288', '1.56097'] | Gamma1 Grad: ['0.00666', '-0.00389', '-0.00308', '-0.01727', '0.00519', '-0.00262', '-0.03260', '-0.00002', '-0.00186', '0.01085', '0.18642', '0.06651', '-0.00936']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 53.040% (Updated)\n",
      "ğŸ“Š Train Accuracy: 53.040% | ğŸ† Best Train Accuracy: 53.040%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 53.040% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.02it/s, Test_acc=52.2, Test_loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 52.190% | ğŸ† Best Test Accuracy: 52.190%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.63it/s, Train_acc=55, Train_loss=1.6]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['3.10797', '3.58664', '2.91219', '2.45189', '2.54910', '2.59964', '1.85563', '2.35986', '1.69490', '1.56052', '0.35532', '1.20508', '1.59035'] | Gamma1 Grad: ['-0.01891', '-0.01670', '-0.00476', '0.00758', '0.02060', '0.00584', '-0.02167', '0.01618', '-0.00477', '0.03195', '-0.03086', '0.00016', '0.06984']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 54.994% (Updated)\n",
      "ğŸ“Š Train Accuracy: 54.994% | ğŸ† Best Train Accuracy: 54.994%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 54.994% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.47it/s, Test_acc=53.3, Test_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 53.330% | ğŸ† Best Test Accuracy: 53.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.48it/s, Train_acc=56.8, Train_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['3.12622', '3.63200', '2.91253', '2.48339', '2.54134', '2.61995', '1.92237', '2.44876', '1.74505', '1.58521', '0.35891', '1.23598', '1.63257'] | Gamma1 Grad: ['0.02143', '-0.00187', '0.00160', '0.00590', '0.00603', '-0.00107', '0.02806', '-0.01707', '-0.01679', '-0.01159', '0.02687', '0.04127', '0.00272']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 56.812% (Updated)\n",
      "ğŸ“Š Train Accuracy: 56.812% | ğŸ† Best Train Accuracy: 56.812%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 56.812% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.57it/s, Test_acc=52.9, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 52.890% | ğŸ† Best Test Accuracy: 53.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.18it/s, Train_acc=58.8, Train_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['3.14529', '3.67957', '2.90574', '2.48439', '2.55329', '2.62229', '2.00111', '2.51832', '1.79513', '1.60078', '0.35852', '1.21486', '1.68484'] | Gamma1 Grad: ['-0.00617', '0.00014', '-0.01229', '-0.00768', '0.00801', '0.00001', '-0.00289', '-0.00031', '-0.01128', '-0.01051', '-0.02158', '0.01175', '0.02582']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 58.770% (Updated)\n",
      "ğŸ“Š Train Accuracy: 58.770% | ğŸ† Best Train Accuracy: 58.770%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 58.770% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.28it/s, Test_acc=55.4, Test_loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 55.350% | ğŸ† Best Test Accuracy: 55.350%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.34it/s, Train_acc=60.1, Train_loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['3.16808', '3.76162', '2.90697', '2.52443', '2.58101', '2.68821', '2.06792', '2.58137', '1.88129', '1.64484', '0.35651', '1.22391', '1.72121'] | Gamma1 Grad: ['0.00942', '-0.00238', '-0.00074', '-0.03442', '-0.01568', '-0.02477', '0.01414', '0.00097', '-0.02059', '-0.00121', '-0.14750', '-0.01290', '0.05051']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 60.114% (Updated)\n",
      "ğŸ“Š Train Accuracy: 60.114% | ğŸ† Best Train Accuracy: 60.114%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 60.114% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.67it/s, Test_acc=56.8, Test_loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 56.760% | ğŸ† Best Test Accuracy: 56.760%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.29it/s, Train_acc=61.6, Train_loss=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['3.21191', '3.83626', '2.90408', '2.52689', '2.56174', '2.69046', '2.13569', '2.63530', '1.94031', '1.65760', '0.34417', '1.25123', '1.76555'] | Gamma1 Grad: ['-0.00458', '0.00606', '0.00375', '-0.01629', '0.00465', '-0.00139', '0.00954', '0.00086', '0.01176', '0.01039', '-0.11188', '-0.06681', '-0.02635']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 61.562% (Updated)\n",
      "ğŸ“Š Train Accuracy: 61.562% | ğŸ† Best Train Accuracy: 61.562%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 61.562% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.92it/s, Test_acc=57.3, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 57.330% | ğŸ† Best Test Accuracy: 57.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.07it/s, Train_acc=63.6, Train_loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['3.23686', '3.86610', '2.89735', '2.52831', '2.56946', '2.67710', '2.20830', '2.69031', '1.99576', '1.66842', '0.35650', '1.25216', '1.80690'] | Gamma1 Grad: ['-0.01396', '-0.00151', '-0.00813', '-0.00929', '-0.03625', '-0.01682', '-0.00774', '-0.00070', '-0.00106', '0.01728', '0.07743', '-0.08701', '-0.05905']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 63.552% (Updated)\n",
      "ğŸ“Š Train Accuracy: 63.552% | ğŸ† Best Train Accuracy: 63.552%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 63.552% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 20 | TrainLossHist: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.49it/s, Test_acc=57.4, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 57.440% | ğŸ† Best Test Accuracy: 57.440%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.43it/s, Train_acc=64.7, Train_loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['3.24345', '3.89710', '2.88272', '2.53557', '2.54854', '2.69168', '2.24067', '2.72335', '2.06070', '1.72615', '0.38773', '1.25645', '1.83860'] | Gamma1 Grad: ['0.00760', '0.00885', '0.01693', '-0.00778', '-0.00664', '-0.00885', '-0.00800', '0.01061', '0.00409', '-0.01305', '-0.01505', '0.05259', '-0.02774']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 64.688% (Updated)\n",
      "ğŸ“Š Train Accuracy: 64.688% | ğŸ† Best Train Accuracy: 64.688%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 64.688% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.78it/s, Test_acc=58.1, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 58.120% | ğŸ† Best Test Accuracy: 58.120%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.03it/s, Train_acc=66.3, Train_loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['3.28556', '3.91931', '2.89387', '2.54609', '2.54772', '2.68712', '2.28966', '2.76416', '2.13388', '1.73541', '0.38108', '1.25172', '1.88369'] | Gamma1 Grad: ['0.00139', '0.00014', '0.00353', '0.01732', '-0.03087', '0.02328', '0.00722', '-0.00384', '-0.00705', '-0.01559', '-0.06061', '-0.03257', '0.05456']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 66.284% (Updated)\n",
      "ğŸ“Š Train Accuracy: 66.284% | ğŸ† Best Train Accuracy: 66.284%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 66.284% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.71it/s, Test_acc=58.4, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 58.390% | ğŸ† Best Test Accuracy: 58.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.59it/s, Train_acc=67.2, Train_loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['3.29438', '3.93033', '2.88228', '2.54166', '2.54652', '2.70253', '2.33307', '2.80691', '2.18108', '1.75748', '0.38380', '1.28165', '1.90723'] | Gamma1 Grad: ['-0.01679', '0.00423', '-0.05893', '0.06021', '-0.03858', '-0.03324', '0.00580', '-0.00449', '0.00670', '-0.00291', '-0.07097', '0.07024', '0.01619']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 67.154% (Updated)\n",
      "ğŸ“Š Train Accuracy: 67.154% | ğŸ† Best Train Accuracy: 67.154%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 67.154% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.74it/s, Test_acc=59.5, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 59.460% | ğŸ† Best Test Accuracy: 59.460%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.31it/s, Train_acc=68.2, Train_loss=1.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['3.31286', '3.94658', '2.88265', '2.54907', '2.53790', '2.70044', '2.36224', '2.83811', '2.24078', '1.78326', '0.39655', '1.29028', '1.93818'] | Gamma1 Grad: ['0.00937', '0.02735', '0.01278', '0.03766', '-0.00488', '0.02977', '0.00417', '-0.01178', '-0.00000', '0.00341', '-0.30653', '-0.21030', '-0.00123']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 68.248% (Updated)\n",
      "ğŸ“Š Train Accuracy: 68.248% | ğŸ† Best Train Accuracy: 68.248%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 68.248% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.08it/s, Test_acc=60.2, Test_loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.250% | ğŸ† Best Test Accuracy: 60.250%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.63it/s, Train_acc=69.6, Train_loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['3.34495', '3.94595', '2.87016', '2.54628', '2.54520', '2.69368', '2.42088', '2.86688', '2.31413', '1.80959', '0.39601', '1.30543', '1.95510'] | Gamma1 Grad: ['-0.00493', '-0.00438', '-0.01431', '0.02128', '-0.00565', '0.00776', '0.02571', '-0.00373', '0.00776', '0.00701', '0.08451', '0.00226', '-0.00472']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 69.570% (Updated)\n",
      "ğŸ“Š Train Accuracy: 69.570% | ğŸ† Best Train Accuracy: 69.570%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 69.570% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.87it/s, Test_acc=59.6, Test_loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 59.630% | ğŸ† Best Test Accuracy: 60.250%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.84it/s, Train_acc=70.7, Train_loss=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['3.33498', '3.95862', '2.87285', '2.53416', '2.54256', '2.70289', '2.45758', '2.88779', '2.36209', '1.81949', '0.41854', '1.29821', '1.96378'] | Gamma1 Grad: ['0.00263', '-0.00015', '0.01169', '-0.04433', '-0.01282', '-0.01439', '-0.01225', '-0.00559', '-0.00138', '-0.01950', '-0.03487', '0.00284', '0.00590']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 70.700% (Updated)\n",
      "ğŸ“Š Train Accuracy: 70.700% | ğŸ† Best Train Accuracy: 70.700%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 70.700% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.72it/s, Test_acc=60.2, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 60.230% | ğŸ† Best Test Accuracy: 60.250%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.62it/s, Train_acc=71.9, Train_loss=0.958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['3.32699', '3.97549', '2.85815', '2.54207', '2.53882', '2.71608', '2.49047', '2.92665', '2.40656', '1.85973', '0.42064', '1.31247', '1.98419'] | Gamma1 Grad: ['0.01639', '-0.00944', '-0.00013', '0.01801', '-0.06104', '0.02047', '-0.00796', '0.00808', '0.00229', '0.00001', '0.07534', '-0.00914', '0.03084']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 71.942% (Updated)\n",
      "ğŸ“Š Train Accuracy: 71.942% | ğŸ† Best Train Accuracy: 71.942%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 71.942% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.88it/s, Test_acc=60.7, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.660% | ğŸ† Best Test Accuracy: 60.660%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.14it/s, Train_acc=73, Train_loss=0.918]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['3.33042', '3.97600', '2.85625', '2.53262', '2.51396', '2.71980', '2.50951', '2.94481', '2.45791', '1.87460', '0.42621', '1.31613', '2.01850'] | Gamma1 Grad: ['0.00559', '-0.00784', '0.03328', '-0.02747', '0.02340', '-0.00097', '0.00555', '-0.00105', '0.00502', '0.00068', '-0.05158', '0.03291', '-0.01717']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 73.038% (Updated)\n",
      "ğŸ“Š Train Accuracy: 73.038% | ğŸ† Best Train Accuracy: 73.038%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 73.038% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.14it/s, Test_acc=60.6, Test_loss=1.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 60.610% | ğŸ† Best Test Accuracy: 60.660%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.79it/s, Train_acc=73.7, Train_loss=0.885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['3.31986', '3.97372', '2.84878', '2.54227', '2.48791', '2.69934', '2.55226', '2.96264', '2.50202', '1.90148', '0.44226', '1.30960', '2.03593'] | Gamma1 Grad: ['0.00935', '0.00946', '0.02367', '0.01375', '-0.01523', '-0.02270', '0.00953', '-0.00496', '-0.01554', '0.00610', '0.01821', '-0.11132', '0.05116']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 73.676% (Updated)\n",
      "ğŸ“Š Train Accuracy: 73.676% | ğŸ† Best Train Accuracy: 73.676%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 73.676% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.64it/s, Test_acc=61, Test_loss=1.45]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.040% | ğŸ† Best Test Accuracy: 61.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.02it/s, Train_acc=74.8, Train_loss=0.855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['3.33174', '3.97986', '2.85578', '2.55807', '2.50599', '2.70014', '2.57989', '2.99134', '2.54607', '1.92635', '0.43993', '1.32367', '2.05137'] | Gamma1 Grad: ['0.00094', '0.00183', '0.00239', '0.00496', '0.00025', '0.01338', '0.00321', '0.00991', '-0.00184', '0.00054', '-0.10244', '-0.03437', '-0.01166']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 74.786% (Updated)\n",
      "ğŸ“Š Train Accuracy: 74.786% | ğŸ† Best Train Accuracy: 74.786%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 74.786% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 30 | TrainLossHist: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.10it/s, Test_acc=61.7, Test_loss=1.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.670% | ğŸ† Best Test Accuracy: 61.670%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.32it/s, Train_acc=76, Train_loss=0.81]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['3.35502', '3.97695', '2.85620', '2.55965', '2.50849', '2.65284', '2.62175', '3.01838', '2.57715', '1.92672', '0.45652', '1.32746', '2.06242'] | Gamma1 Grad: ['0.00500', '0.00870', '0.01401', '0.00800', '0.05248', '-0.03327', '0.00844', '0.00531', '0.00675', '0.01393', '0.19961', '-0.04270', '-0.04440']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 75.984% (Updated)\n",
      "ğŸ“Š Train Accuracy: 75.984% | ğŸ† Best Train Accuracy: 75.984%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 75.984% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.81it/s, Test_acc=62.1, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.060% | ğŸ† Best Test Accuracy: 62.060%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.51it/s, Train_acc=76.8, Train_loss=0.775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['3.35302', '3.99286', '2.81676', '2.53160', '2.50334', '2.60705', '2.65453', '3.01990', '2.61863', '1.95151', '0.46554', '1.33243', '2.07961'] | Gamma1 Grad: ['-0.00886', '-0.00075', '-0.03202', '0.02663', '-0.02993', '0.01010', '0.02270', '0.00351', '-0.00325', '-0.01913', '-0.10710', '-0.04733', '0.00912']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 76.800% (Updated)\n",
      "ğŸ“Š Train Accuracy: 76.800% | ğŸ† Best Train Accuracy: 76.800%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 76.800% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.20it/s, Test_acc=61.7, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 61.670% | ğŸ† Best Test Accuracy: 62.060%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.33it/s, Train_acc=77.4, Train_loss=0.756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['3.39663', '4.00079', '2.84658', '2.52980', '2.47207', '2.63412', '2.68005', '3.02960', '2.64583', '1.98989', '0.49233', '1.34832', '2.09574'] | Gamma1 Grad: ['0.00507', '0.00884', '0.01479', '0.10740', '0.04043', '-0.02482', '0.01905', '-0.00975', '-0.00885', '0.00589', '0.04170', '-0.10663', '0.05920']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 77.410% (Updated)\n",
      "ğŸ“Š Train Accuracy: 77.410% | ğŸ† Best Train Accuracy: 77.410%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 77.410% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.61it/s, Test_acc=61.9, Test_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 61.910% | ğŸ† Best Test Accuracy: 62.060%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.27it/s, Train_acc=78.5, Train_loss=0.728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['3.41185', '4.01131', '2.85172', '2.52363', '2.49170', '2.62336', '2.70271', '3.05607', '2.68558', '2.01465', '0.46439', '1.35989', '2.09168'] | Gamma1 Grad: ['0.00619', '-0.02786', '-0.02766', '-0.07286', '0.02590', '0.04557', '-0.04255', '-0.04394', '-0.02079', '0.00790', '-0.04464', '-0.11099', '0.01139']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 78.468% (Updated)\n",
      "ğŸ“Š Train Accuracy: 78.468% | ğŸ† Best Train Accuracy: 78.468%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 78.468% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.25it/s, Test_acc=62.7, Test_loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.660% | ğŸ† Best Test Accuracy: 62.660%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.57it/s, Train_acc=79.1, Train_loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00144 | Gamma1: ['3.37518', '4.01065', '2.83749', '2.52645', '2.46342', '2.63958', '2.72019', '3.09433', '2.72427', '2.03766', '0.47404', '1.32835', '2.09531'] | Gamma1 Grad: ['0.01270', '0.00017', '-0.00732', '0.01555', '-0.03413', '0.02967', '0.02821', '-0.00312', '0.00247', '-0.01526', '-0.00361', '-0.02011', '0.04314']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 79.054% (Updated)\n",
      "ğŸ“Š Train Accuracy: 79.054% | ğŸ† Best Train Accuracy: 79.054%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 79.054% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.30it/s, Test_acc=62.5, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.530% | ğŸ† Best Test Accuracy: 62.660%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.99it/s, Train_acc=80, Train_loss=0.673]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['3.37826', '3.99024', '2.82350', '2.53584', '2.47213', '2.65286', '2.77480', '3.10909', '2.74843', '2.05810', '0.47801', '1.34169', '2.08493'] | Gamma1 Grad: ['0.01887', '0.00172', '0.02076', '0.01198', '0.08698', '-0.00008', '0.02682', '0.01997', '-0.00742', '0.01647', '-0.07478', '-0.00341', '0.09770']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 79.968% (Updated)\n",
      "ğŸ“Š Train Accuracy: 79.968% | ğŸ† Best Train Accuracy: 79.968%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 79.968% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.86it/s, Test_acc=62.8, Test_loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.810% | ğŸ† Best Test Accuracy: 62.810%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.26it/s, Train_acc=80.7, Train_loss=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00139 | Gamma1: ['3.38153', '3.98663', '2.78280', '2.49392', '2.43660', '2.65603', '2.77675', '3.12230', '2.79120', '2.04960', '0.49964', '1.30594', '2.09604'] | Gamma1 Grad: ['0.00756', '-0.01143', '0.02196', '0.04325', '-0.02195', '0.04353', '0.01908', '-0.00845', '-0.00577', '-0.00645', '0.02933', '-0.06873', '0.04795']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 80.718% (Updated)\n",
      "ğŸ“Š Train Accuracy: 80.718% | ğŸ† Best Train Accuracy: 80.718%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 80.718% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.39it/s, Test_acc=62.8, Test_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.780% | ğŸ† Best Test Accuracy: 62.810%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.56it/s, Train_acc=81.5, Train_loss=0.62] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['3.39083', '3.97931', '2.76326', '2.47407', '2.42705', '2.66033', '2.78100', '3.13784', '2.84566', '2.05616', '0.51632', '1.33335', '2.13033'] | Gamma1 Grad: ['-0.00132', '-0.00275', '-0.00670', '0.02202', '0.07036', '-0.08933', '0.00182', '0.01000', '-0.00387', '-0.00114', '0.22430', '0.03868', '0.09167']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 81.508% (Updated)\n",
      "ğŸ“Š Train Accuracy: 81.508% | ğŸ† Best Train Accuracy: 81.508%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 81.508% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.83it/s, Test_acc=62.9, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.860% | ğŸ† Best Test Accuracy: 62.860%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.30it/s, Train_acc=82.1, Train_loss=0.601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00133 | Gamma1: ['3.38307', '3.98728', '2.76600', '2.49266', '2.45397', '2.62998', '2.81264', '3.13918', '2.85326', '2.07192', '0.49048', '1.33921', '2.13434'] | Gamma1 Grad: ['-0.00271', '0.01483', '0.02331', '-0.03545', '-0.02545', '0.05053', '0.00262', '-0.00202', '-0.00186', '-0.01224', '0.02164', '-0.02991', '-0.02904']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.096% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.096% | ğŸ† Best Train Accuracy: 82.096%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.096% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.27it/s, Test_acc=63.5, Test_loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.510% | ğŸ† Best Test Accuracy: 63.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.28it/s, Train_acc=82.7, Train_loss=0.575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['3.38645', '3.97962', '2.78247', '2.50473', '2.42266', '2.61386', '2.80603', '3.12641', '2.89035', '2.08719', '0.50097', '1.35073', '2.14928'] | Gamma1 Grad: ['-0.02638', '-0.02963', '-0.02580', '-0.06909', '0.07817', '-0.07445', '0.01805', '0.02317', '0.00119', '0.02065', '0.10635', '0.04116', '-0.06551']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.714% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.714% | ğŸ† Best Train Accuracy: 82.714%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.714% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 40 | TrainLossHist: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.81it/s, Test_acc=63.3, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.300% | ğŸ† Best Test Accuracy: 63.510%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.19it/s, Train_acc=83.2, Train_loss=0.556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00125 | Gamma1: ['3.37204', '4.00914', '2.76740', '2.47488', '2.37856', '2.58032', '2.81909', '3.14872', '2.92191', '2.10408', '0.50785', '1.32497', '2.14371'] | Gamma1 Grad: ['0.00513', '0.02534', '0.00763', '0.02280', '-0.03655', '-0.05441', '0.02543', '-0.00128', '0.00527', '0.03626', '0.13121', '-0.03585', '-0.03820']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.248% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.248% | ğŸ† Best Train Accuracy: 83.248%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.248% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.45it/s, Test_acc=63.6, Test_loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.650% | ğŸ† Best Test Accuracy: 63.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.37it/s, Train_acc=83.9, Train_loss=0.537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['3.37546', '4.01573', '2.76148', '2.47039', '2.37221', '2.62876', '2.81982', '3.15826', '2.95672', '2.13609', '0.53738', '1.33857', '2.16906'] | Gamma1 Grad: ['0.01140', '-0.00206', '0.01889', '-0.00592', '0.00889', '-0.05855', '-0.00434', '0.01626', '-0.00173', '0.00697', '0.09443', '-0.08763', '-0.04585']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.900% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.900% | ğŸ† Best Train Accuracy: 83.900%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.900% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.33it/s, Test_acc=63.6, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.570% | ğŸ† Best Test Accuracy: 63.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.62it/s, Train_acc=84.2, Train_loss=0.525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00115 | Gamma1: ['3.34494', '4.00799', '2.77597', '2.45724', '2.37144', '2.59783', '2.82344', '3.15538', '2.96738', '2.14304', '0.52247', '1.32967', '2.14154'] | Gamma1 Grad: ['0.01055', '-0.00515', '0.01759', '0.00392', '0.00814', '0.01051', '-0.00133', '-0.01259', '-0.00129', '-0.01001', '-0.09567', '-0.00866', '-0.02337']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 84.190% (Updated)\n",
      "ğŸ“Š Train Accuracy: 84.190% | ğŸ† Best Train Accuracy: 84.190%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 84.190% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.29it/s, Test_acc=63.7, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.710% | ğŸ† Best Test Accuracy: 63.710%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.90it/s, Train_acc=85.2, Train_loss=0.493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['3.35436', '4.01042', '2.79597', '2.44314', '2.39595', '2.56847', '2.82903', '3.17736', '3.00355', '2.17625', '0.55205', '1.31766', '2.15779'] | Gamma1 Grad: ['-0.00071', '-0.02251', '-0.00833', '0.04351', '-0.02165', '-0.00304', '-0.01877', '0.00153', '0.00639', '0.00138', '0.12311', '-0.00464', '0.00174']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.214% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.214% | ğŸ† Best Train Accuracy: 85.214%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.214% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.36it/s, Test_acc=64.5, Test_loss=1.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.530% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.43it/s, Train_acc=85.5, Train_loss=0.484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00105 | Gamma1: ['3.34645', '4.00041', '2.78989', '2.41927', '2.38980', '2.55471', '2.83360', '3.16867', '3.01912', '2.18189', '0.55931', '1.33892', '2.14481'] | Gamma1 Grad: ['0.03439', '-0.01466', '0.03753', '0.04806', '0.10101', '-0.00974', '0.02625', '-0.03513', '0.00387', '-0.00859', '0.02182', '-0.04359', '0.04319']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.450% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.450% | ğŸ† Best Train Accuracy: 85.450%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.450% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.75it/s, Test_acc=63.5, Test_loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.460% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:31<00:00, 24.81it/s, Train_acc=86, Train_loss=0.465]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['3.39001', '4.00061', '2.79649', '2.42247', '2.35701', '2.54258', '2.84475', '3.17664', '3.05060', '2.20529', '0.53112', '1.34061', '2.14462'] | Gamma1 Grad: ['-0.00215', '0.00070', '0.00804', '0.01584', '-0.00701', '-0.01116', '-0.00234', '-0.00235', '-0.00420', '0.00094', '-0.05333', '-0.03124', '0.01135']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.984% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.984% | ğŸ† Best Train Accuracy: 85.984%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.984% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.30it/s, Test_acc=64.3, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.330% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.42it/s, Train_acc=86.6, Train_loss=0.449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00094 | Gamma1: ['3.41713', '3.98984', '2.76416', '2.41705', '2.35704', '2.56947', '2.85688', '3.17139', '3.06108', '2.21432', '0.54344', '1.36538', '2.15308'] | Gamma1 Grad: ['-0.00253', '-0.00685', '0.00783', '0.00595', '-0.06914', '-0.04464', '-0.00304', '0.02020', '-0.00590', '-0.01499', '-0.08909', '0.01957', '0.04130']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.594% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.594% | ğŸ† Best Train Accuracy: 86.594%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.594% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.56it/s, Test_acc=64.2, Test_loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.180% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.06it/s, Train_acc=86.9, Train_loss=0.44] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['3.41497', '4.01256', '2.73787', '2.40420', '2.30980', '2.57059', '2.86414', '3.15433', '3.08597', '2.20408', '0.53221', '1.34357', '2.12708'] | Gamma1 Grad: ['0.03945', '-0.02293', '-0.03530', '0.09562', '0.02232', '-0.07954', '-0.03464', '-0.05371', '0.01008', '-0.03180', '0.00645', '-0.06419', '0.12036']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.884% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.884% | ğŸ† Best Train Accuracy: 86.884%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.884% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.81it/s, Test_acc=63.4, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.410% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.37it/s, Train_acc=87.5, Train_loss=0.42] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00083 | Gamma1: ['3.42084', '4.01910', '2.73321', '2.38678', '2.31055', '2.56297', '2.85452', '3.13868', '3.11375', '2.21730', '0.57399', '1.36653', '2.13993'] | Gamma1 Grad: ['-0.01008', '-0.01084', '-0.04788', '-0.04945', '-0.10279', '-0.07200', '-0.01248', '-0.01433', '-0.00070', '0.00409', '0.06814', '-0.00221', '0.04369']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.468% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.468% | ğŸ† Best Train Accuracy: 87.468%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.468% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.23it/s, Test_acc=63.5, Test_loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.540% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.28it/s, Train_acc=87.9, Train_loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['3.39921', '4.04446', '2.74464', '2.40976', '2.31762', '2.52983', '2.85290', '3.12945', '3.12727', '2.24028', '0.55511', '1.35592', '2.14058'] | Gamma1 Grad: ['0.01050', '0.01116', '0.00252', '0.02786', '-0.07737', '0.09045', '-0.01707', '0.01863', '-0.00895', '-0.00173', '0.03029', '0.08586', '0.01829']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.854% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.854% | ğŸ† Best Train Accuracy: 87.854%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.854% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 50 | TrainLossHist: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.37it/s, Test_acc=63.5, Test_loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.550% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 51 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.62it/s, Train_acc=88, Train_loss=0.4]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00072 | Gamma1: ['3.38293', '4.02959', '2.72781', '2.38651', '2.33429', '2.54608', '2.84688', '3.13664', '3.14556', '2.22871', '0.54624', '1.34583', '2.13327'] | Gamma1 Grad: ['0.01580', '-0.00162', '-0.03065', '-0.02223', '-0.02520', '0.03162', '-0.01484', '-0.01701', '-0.00115', '-0.00617', '-0.01575', '-0.03515', '-0.01431']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.982% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.982% | ğŸ† Best Train Accuracy: 87.982%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.982% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.79it/s, Test_acc=63.8, Test_loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.750% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 52 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.01it/s, Train_acc=88.5, Train_loss=0.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['3.39391', '4.02229', '2.71090', '2.37852', '2.30544', '2.54837', '2.84396', '3.13081', '3.16926', '2.24773', '0.57456', '1.31586', '2.12381'] | Gamma1 Grad: ['-0.00512', '-0.00479', '-0.03147', '0.05441', '-0.03718', '0.02003', '0.02855', '-0.00323', '-0.00184', '-0.00392', '-0.09950', '-0.00515', '-0.02663']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.496% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.496% | ğŸ† Best Train Accuracy: 88.496%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.496% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.37it/s, Test_acc=64.1, Test_loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.140% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 53 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.98it/s, Train_acc=88.7, Train_loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00061 | Gamma1: ['3.40320', '4.02943', '2.68093', '2.38873', '2.26649', '2.52908', '2.82625', '3.13046', '3.18114', '2.25670', '0.58396', '1.33140', '2.11930'] | Gamma1 Grad: ['0.00128', '-0.00569', '-0.05931', '0.05553', '0.01246', '-0.00478', '-0.02631', '0.01871', '0.00252', '0.01187', '0.21217', '0.08115', '0.04649']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.692% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.692% | ğŸ† Best Train Accuracy: 88.692%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.692% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.38it/s, Test_acc=64.1, Test_loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.130% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 54 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.59it/s, Train_acc=89.1, Train_loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['3.40419', '4.03626', '2.66452', '2.35605', '2.26348', '2.48308', '2.81429', '3.11427', '3.18356', '2.27045', '0.59772', '1.32576', '2.08905'] | Gamma1 Grad: ['0.02487', '-0.01751', '-0.03117', '0.05331', '0.06495', '0.09222', '-0.00032', '0.00031', '-0.01484', '0.00098', '0.10074', '-0.07172', '-0.06802']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.076% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.076% | ğŸ† Best Train Accuracy: 89.076%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.076% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.92it/s, Test_acc=64.5, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.460% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.81it/s, Train_acc=89.6, Train_loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00050 | Gamma1: ['3.41482', '4.03783', '2.66247', '2.35458', '2.28011', '2.47039', '2.81754', '3.11700', '3.19238', '2.29678', '0.60448', '1.32739', '2.08196'] | Gamma1 Grad: ['0.01058', '-0.00084', '-0.00614', '0.02394', '-0.02406', '0.00061', '-0.00215', '0.02791', '-0.00506', '0.00656', '-0.01899', '0.07098', '0.01739']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.592% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.592% | ğŸ† Best Train Accuracy: 89.592%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.592% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.30it/s, Test_acc=63.7, Test_loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.700% | ğŸ† Best Test Accuracy: 64.530%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.81it/s, Train_acc=89.9, Train_loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['3.40016', '4.03878', '2.63259', '2.32572', '2.27445', '2.47127', '2.81290', '3.09241', '3.21623', '2.31149', '0.61456', '1.31634', '2.05774'] | Gamma1 Grad: ['-0.01718', '0.01545', '0.03180', '-0.02227', '-0.03645', '-0.01570', '0.00832', '0.00706', '0.00241', '0.01314', '-0.00732', '0.01818', '-0.04966']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.902% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.902% | ğŸ† Best Train Accuracy: 89.902%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.902% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.77it/s, Test_acc=64.5, Test_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.550% | ğŸ† Best Test Accuracy: 64.550%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.78it/s, Train_acc=90.3, Train_loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00040 | Gamma1: ['3.40324', '4.04237', '2.65129', '2.30630', '2.26814', '2.48332', '2.80535', '3.09490', '3.22298', '2.33165', '0.60494', '1.29376', '2.04968'] | Gamma1 Grad: ['0.00362', '0.00987', '0.00377', '0.03230', '0.00906', '0.00183', '0.00252', '0.00852', '-0.00235', '-0.00185', '0.01087', '0.02811', '-0.00444']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.262% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.262% | ğŸ† Best Train Accuracy: 90.262%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.262% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.30it/s, Test_acc=64, Test_loss=1.7]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.990% | ğŸ† Best Test Accuracy: 64.550%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.25it/s, Train_acc=90.3, Train_loss=0.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['3.39468', '4.05422', '2.65297', '2.31168', '2.26565', '2.45791', '2.81389', '3.09208', '3.23905', '2.34537', '0.61241', '1.27128', '2.03583'] | Gamma1 Grad: ['-0.00435', '0.01679', '0.00065', '0.00493', '-0.03482', '0.00311', '0.01247', '-0.00809', '-0.00124', '0.01143', '-0.08065', '0.07711', '-0.00439']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.332% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.332% | ğŸ† Best Train Accuracy: 90.332%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.332% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.44it/s, Test_acc=63.9, Test_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.930% | ğŸ† Best Test Accuracy: 64.550%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.47it/s, Train_acc=90.7, Train_loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00030 | Gamma1: ['3.39289', '4.05604', '2.64988', '2.30977', '2.26134', '2.47203', '2.78643', '3.09101', '3.24896', '2.37353', '0.60294', '1.29202', '2.03736'] | Gamma1 Grad: ['0.00208', '-0.00000', '-0.00082', '0.00230', '0.01764', '-0.05321', '0.02129', '-0.00614', '0.00831', '0.00143', '0.00560', '0.01689', '0.02726']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.744% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.744% | ğŸ† Best Train Accuracy: 90.744%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.744% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.32it/s, Test_acc=64.4, Test_loss=1.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.410% | ğŸ† Best Test Accuracy: 64.550%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.94it/s, Train_acc=91, Train_loss=0.305]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['3.38551', '4.05299', '2.64471', '2.30742', '2.23655', '2.50035', '2.77266', '3.09871', '3.26109', '2.36817', '0.61643', '1.29995', '2.03653'] | Gamma1 Grad: ['0.00235', '-0.00459', '-0.00034', '0.00989', '-0.00901', '-0.00416', '-0.00049', '-0.00612', '0.00098', '0.00124', '0.03415', '-0.03476', '0.00068']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.028% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.028% | ğŸ† Best Train Accuracy: 91.028%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.028% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 60 | TrainLossHist: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.02it/s, Test_acc=64.8, Test_loss=1.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.750% | ğŸ† Best Test Accuracy: 64.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.55it/s, Train_acc=91.2, Train_loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00022 | Gamma1: ['3.40107', '4.04883', '2.63498', '2.28696', '2.24994', '2.47199', '2.77772', '3.09034', '3.27105', '2.36796', '0.59993', '1.28143', '2.02084'] | Gamma1 Grad: ['-0.00432', '0.01443', '-0.06305', '-0.03180', '0.05391', '-0.02454', '0.05954', '-0.00274', '-0.00468', '0.00462', '0.07127', '-0.03836', '-0.03707']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.216% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.216% | ğŸ† Best Train Accuracy: 91.216%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.216% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.90it/s, Test_acc=64, Test_loss=1.79]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.960% | ğŸ† Best Test Accuracy: 64.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.51it/s, Train_acc=91.4, Train_loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['3.38062', '4.05471', '2.63737', '2.27039', '2.23329', '2.44637', '2.76778', '3.10168', '3.27158', '2.38834', '0.60145', '1.27474', '2.01588'] | Gamma1 Grad: ['0.00080', '-0.00225', '0.06258', '-0.03104', '0.03277', '-0.03306', '0.02381', '-0.08860', '-0.00368', '-0.00715', '-0.01947', '-0.03253', '-0.02978']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.384% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.384% | ğŸ† Best Train Accuracy: 91.384%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.384% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.94it/s, Test_acc=64.4, Test_loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.440% | ğŸ† Best Test Accuracy: 64.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.14it/s, Train_acc=91.6, Train_loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00016 | Gamma1: ['3.36369', '4.02680', '2.64240', '2.27634', '2.23415', '2.42140', '2.75349', '3.10088', '3.27579', '2.40313', '0.61175', '1.30702', '2.00171'] | Gamma1 Grad: ['0.00513', '0.00305', '0.01089', '-0.03634', '-0.00290', '0.04624', '-0.03637', '-0.01070', '-0.00481', '-0.00057', '0.03811', '0.04155', '-0.07289']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.594% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.594% | ğŸ† Best Train Accuracy: 91.594%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.594% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.17it/s, Test_acc=64.7, Test_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.720% | ğŸ† Best Test Accuracy: 64.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.64it/s, Train_acc=91.7, Train_loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['3.35870', '4.02426', '2.62514', '2.27316', '2.23664', '2.44151', '2.73454', '3.10333', '3.28717', '2.42120', '0.61495', '1.30273', '1.99685'] | Gamma1 Grad: ['-0.00909', '-0.00778', '0.02408', '-0.08339', '0.01815', '0.05412', '-0.10372', '0.00082', '0.00242', '0.01315', '0.14014', '0.07394', '-0.00207']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.734% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.734% | ğŸ† Best Train Accuracy: 91.734%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.734% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.13it/s, Test_acc=65, Test_loss=1.72]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 65.040% | ğŸ† Best Test Accuracy: 65.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.46it/s, Train_acc=92.2, Train_loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00011 | Gamma1: ['3.36500', '4.02536', '2.62229', '2.25575', '2.21495', '2.43100', '2.73795', '3.07654', '3.29412', '2.43903', '0.62171', '1.28891', '1.98680'] | Gamma1 Grad: ['0.00667', '-0.01352', '-0.03149', '0.06172', '-0.08088', '0.00308', '-0.00440', '0.00855', '0.00014', '-0.00744', '-0.05936', '-0.06358', '0.01009']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.230% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.230% | ğŸ† Best Train Accuracy: 92.230%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.230% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.96it/s, Test_acc=64.7, Test_loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.690% | ğŸ† Best Test Accuracy: 65.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.82it/s, Train_acc=92.3, Train_loss=0.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['3.39906', '4.02509', '2.62883', '2.28707', '2.24814', '2.42524', '2.73145', '3.05814', '3.30475', '2.44638', '0.63267', '1.29181', '1.99386'] | Gamma1 Grad: ['0.00072', '-0.00490', '-0.00199', '-0.01991', '0.00634', '0.01333', '-0.02451', '-0.01881', '-0.00057', '-0.00513', '-0.00465', '-0.01260', '-0.00326']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.262% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.262% | ğŸ† Best Train Accuracy: 92.262%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.262% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.82it/s, Test_acc=64.7, Test_loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.740% | ğŸ† Best Test Accuracy: 65.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.81it/s, Train_acc=92.5, Train_loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['3.40063', '4.00984', '2.60084', '2.27333', '2.21610', '2.40642', '2.73644', '3.04957', '3.32422', '2.45451', '0.62851', '1.29101', '1.99237'] | Gamma1 Grad: ['-0.01831', '0.00794', '-0.01664', '-0.01083', '0.00562', '0.00944', '-0.01776', '-0.01822', '0.00188', '-0.00935', '0.00048', '0.06215', '-0.01079']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.536% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.536% | ğŸ† Best Train Accuracy: 92.536%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.536% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.07it/s, Test_acc=65.2, Test_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 65.190% | ğŸ† Best Test Accuracy: 65.190%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.40it/s, Train_acc=92.6, Train_loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['3.38823', '4.02082', '2.58111', '2.26826', '2.21685', '2.40554', '2.76208', '3.04389', '3.32499', '2.47382', '0.64240', '1.28979', '1.96661'] | Gamma1 Grad: ['-0.00479', '0.00281', '-0.00158', '0.00152', '0.00310', '0.01825', '0.00270', '-0.00497', '0.00001', '0.00080', '-0.01123', '0.01251', '0.00594']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.592% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.592% | ğŸ† Best Train Accuracy: 92.592%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.592% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.15it/s, Test_acc=65.1, Test_loss=1.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.060% | ğŸ† Best Test Accuracy: 65.190%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.56it/s, Train_acc=92.8, Train_loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['3.39172', '4.02174', '2.59843', '2.28186', '2.21652', '2.40807', '2.75605', '3.02068', '3.32940', '2.48334', '0.64057', '1.29593', '1.95913'] | Gamma1 Grad: ['0.00828', '-0.00151', '0.02519', '-0.01904', '0.02658', '0.04311', '-0.02296', '0.02977', '-0.00387', '0.02251', '0.06103', '0.05809', '-0.02324']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.836% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.836% | ğŸ† Best Train Accuracy: 92.836%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.836% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.08it/s, Test_acc=64.3, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.280% | ğŸ† Best Test Accuracy: 65.190%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.01it/s, Train_acc=92.9, Train_loss=0.248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['3.37321', '4.02603', '2.58456', '2.27493', '2.21331', '2.42403', '2.73791', '3.02444', '3.33850', '2.49198', '0.65213', '1.29675', '1.95277'] | Gamma1 Grad: ['-0.00417', '-0.02229', '-0.03367', '-0.05036', '0.08923', '0.03690', '-0.02196', '-0.03364', '-0.00055', '0.00159', '-0.06519', '-0.03635', '-0.05643']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.920% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.920% | ğŸ† Best Train Accuracy: 92.920%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.920% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 70 | TrainLossHist: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.18it/s, Test_acc=64.7, Test_loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.670% | ğŸ† Best Test Accuracy: 65.190%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.76it/s, Train_acc=93.3, Train_loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['3.39223', '3.99927', '2.56177', '2.26515', '2.23720', '2.38641', '2.74017', '2.99965', '3.35725', '2.50487', '0.63610', '1.27223', '1.95255'] | Gamma1 Grad: ['-0.00120', '-0.00822', '0.00865', '0.02016', '0.04340', '-0.05291', '0.00677', '0.00580', '0.00253', '0.01041', '0.16892', '-0.04174', '0.01239']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.258% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.258% | ğŸ† Best Train Accuracy: 93.258%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.258% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.17it/s, Test_acc=64.4, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.410% | ğŸ† Best Test Accuracy: 65.190%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.95it/s, Train_acc=93.2, Train_loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['3.39388', '4.01239', '2.58683', '2.22400', '2.21411', '2.40926', '2.70797', '2.98800', '3.37397', '2.50518', '0.61938', '1.25336', '1.92738'] | Gamma1 Grad: ['-0.00414', '0.00474', '0.03637', '-0.07826', '0.04618', '0.01509', '-0.01793', '0.00375', '0.00124', '0.00239', '0.14909', '0.00154', '0.08259']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.170% | ğŸ† Best Train Accuracy: 93.258%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.258% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.56it/s, Test_acc=64.8, Test_loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.820% | ğŸ† Best Test Accuracy: 65.190%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.21it/s, Train_acc=93.5, Train_loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['3.38730', '4.02413', '2.52321', '2.22525', '2.20926', '2.37463', '2.72462', '2.97602', '3.39309', '2.49826', '0.62799', '1.24355', '1.92916'] | Gamma1 Grad: ['-0.00498', '0.02117', '0.02726', '0.05660', '-0.01666', '0.03341', '-0.01135', '-0.00608', '-0.00305', '0.00014', '-0.04188', '-0.08093', '0.03509']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.462% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.462% | ğŸ† Best Train Accuracy: 93.462%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.462% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.65it/s, Test_acc=65.2, Test_loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 65.230% | ğŸ† Best Test Accuracy: 65.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.85it/s, Train_acc=93.7, Train_loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['3.39487', '4.00212', '2.54214', '2.22805', '2.19296', '2.38922', '2.69456', '2.95693', '3.40318', '2.51178', '0.63262', '1.25246', '1.92346'] | Gamma1 Grad: ['0.00975', '0.00296', '0.00125', '-0.01484', '0.04507', '-0.02873', '0.00202', '-0.00960', '0.00119', '0.00354', '-0.05673', '0.00206', '-0.02743']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.674% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.674% | ğŸ† Best Train Accuracy: 93.674%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.674% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.23it/s, Test_acc=64.8, Test_loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.800% | ğŸ† Best Test Accuracy: 65.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.17it/s, Train_acc=93.9, Train_loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['3.39310', '4.00175', '2.58615', '2.24155', '2.19987', '2.38506', '2.70221', '2.94356', '3.42419', '2.51207', '0.63847', '1.25234', '1.90649'] | Gamma1 Grad: ['-0.01173', '0.02439', '-0.05564', '-0.04707', '-0.14774', '-0.02054', '-0.05518', '0.01325', '-0.00369', '0.02281', '0.32080', '-0.01572', '-0.07484']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.898% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.898% | ğŸ† Best Train Accuracy: 93.898%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.898% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.71it/s, Test_acc=64.4, Test_loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.360% | ğŸ† Best Test Accuracy: 65.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.04it/s, Train_acc=93.7, Train_loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['3.37959', '3.97575', '2.58728', '2.20313', '2.20028', '2.40534', '2.68850', '2.93571', '3.42239', '2.52180', '0.65636', '1.22429', '1.91389'] | Gamma1 Grad: ['-0.00215', '0.00281', '-0.05069', '-0.03383', '0.01486', '0.04453', '-0.10233', '0.00288', '-0.00014', '0.00941', '0.17048', '-0.00231', '0.00787']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.744% | ğŸ† Best Train Accuracy: 93.898%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.898% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.19it/s, Test_acc=64.7, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.680% | ğŸ† Best Test Accuracy: 65.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.84it/s, Train_acc=94, Train_loss=0.219]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00147 | Gamma1: ['3.39428', '3.96752', '2.57433', '2.24262', '2.19126', '2.36026', '2.66935', '2.96324', '3.41268', '2.53624', '0.63003', '1.21861', '1.92457'] | Gamma1 Grad: ['-0.00654', '0.00295', '-0.02067', '-0.00413', '0.07233', '0.03214', '-0.00499', '-0.06213', '0.00676', '0.00387', '0.02435', '0.09253', '0.00621']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.026% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.026% | ğŸ† Best Train Accuracy: 94.026%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.026% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.95it/s, Test_acc=64.5, Test_loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.530% | ğŸ† Best Test Accuracy: 65.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.74it/s, Train_acc=93.9, Train_loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['3.39606', '3.96584', '2.56949', '2.23825', '2.18073', '2.36598', '2.65555', '2.97784', '3.42229', '2.54248', '0.66058', '1.21480', '1.88472'] | Gamma1 Grad: ['-0.00604', '0.00089', '-0.00348', '-0.02035', '0.10167', '0.06196', '0.00072', '0.01440', '-0.00220', '0.00235', '-0.14793', '0.10805', '0.01538']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.854% | ğŸ† Best Train Accuracy: 94.026%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.026% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.59it/s, Test_acc=64.9, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.910% | ğŸ† Best Test Accuracy: 65.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.85it/s, Train_acc=94.6, Train_loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00146 | Gamma1: ['3.39406', '3.98772', '2.53189', '2.22219', '2.18904', '2.33099', '2.64119', '2.96809', '3.43028', '2.55202', '0.67428', '1.21746', '1.89669'] | Gamma1 Grad: ['0.00378', '-0.04268', '-0.11488', '-0.01867', '-0.02715', '-0.02277', '0.01536', '-0.04067', '0.00715', '0.00043', '0.13803', '-0.02493', '0.07302']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.586% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.586% | ğŸ† Best Train Accuracy: 94.586%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.586% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 77.58it/s, Test_acc=64.2, Test_loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.230% | ğŸ† Best Test Accuracy: 65.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.31it/s, Train_acc=96.8, Train_loss=0.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00144 | Gamma1: ['3.41144', '3.98775', '2.57520', '2.21182', '2.12993', '2.37457', '2.62933', '2.97727', '3.42178', '2.58193', '0.74816', '1.21159', '1.90158'] | Gamma1 Grad: ['0.01254', '0.01230', '-0.04635', '-0.03579', '-0.03246', '0.05486', '-0.02946', '0.02553', '-0.00580', '0.00740', '0.06430', '-0.15339', '0.00145']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 96.840% (Updated)\n",
      "ğŸ“Š Train Accuracy: 96.840% | ğŸ† Best Train Accuracy: 96.840%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 96.840% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 80 | TrainLossHist: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.26it/s, Test_acc=67.1, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.080% | ğŸ† Best Test Accuracy: 67.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.71it/s, Train_acc=97.8, Train_loss=0.099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00143 | Gamma1: ['3.40790', '3.99476', '2.59052', '2.22234', '2.12973', '2.33249', '2.63722', '2.96554', '3.42015', '2.60374', '0.80222', '1.21360', '1.92399'] | Gamma1 Grad: ['-0.00124', '0.00255', '0.01069', '-0.04101', '0.03824', '-0.02155', '-0.02339', '0.03161', '0.00289', '0.01011', '0.05796', '-0.08457', '-0.00653']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.828% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.828% | ğŸ† Best Train Accuracy: 97.828%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.828% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.95it/s, Test_acc=67.3, Test_loss=1.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.300% | ğŸ† Best Test Accuracy: 67.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.84it/s, Train_acc=98.3, Train_loss=0.088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00142 | Gamma1: ['3.41697', '4.00076', '2.60782', '2.21105', '2.12711', '2.31651', '2.64563', '2.97215', '3.41600', '2.62922', '0.87124', '1.22428', '1.95759'] | Gamma1 Grad: ['-0.01400', '-0.00167', '-0.00851', '0.00809', '-0.03960', '0.05483', '-0.03193', '0.01008', '0.00230', '0.00427', '0.03434', '0.01467', '-0.01971']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.310% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.310% | ğŸ† Best Train Accuracy: 98.310%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.310% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.24it/s, Test_acc=67.1, Test_loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.140% | ğŸ† Best Test Accuracy: 67.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.19it/s, Train_acc=98.4, Train_loss=0.083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00141 | Gamma1: ['3.41606', '3.98045', '2.58111', '2.16916', '2.12503', '2.32234', '2.64802', '2.98185', '3.42542', '2.64412', '0.88705', '1.20761', '1.96767'] | Gamma1 Grad: ['-0.01354', '0.00235', '-0.03902', '0.07582', '0.02580', '-0.06787', '0.03790', '-0.02533', '0.00005', '-0.00562', '-0.06265', '0.02477', '-0.00288']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.410% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.410% | ğŸ† Best Train Accuracy: 98.410%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.410% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.37it/s, Test_acc=67.4, Test_loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.390% | ğŸ† Best Test Accuracy: 67.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.68it/s, Train_acc=98.6, Train_loss=0.077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00139 | Gamma1: ['3.40582', '3.96921', '2.59170', '2.17674', '2.11229', '2.33529', '2.63870', '2.97331', '3.43061', '2.63577', '0.89065', '1.21895', '2.00511'] | Gamma1 Grad: ['0.00462', '0.01200', '0.04883', '0.00644', '0.06555', '0.03116', '0.03362', '-0.01254', '-0.00081', '0.00134', '0.04098', '-0.02979', '0.00388']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.618% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.618% | ğŸ† Best Train Accuracy: 98.618%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.618% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.38it/s, Test_acc=67.7, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.650% | ğŸ† Best Test Accuracy: 67.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.01it/s, Train_acc=98.6, Train_loss=0.074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00138 | Gamma1: ['3.40510', '3.95804', '2.60447', '2.15908', '2.10794', '2.32542', '2.66011', '2.97173', '3.42761', '2.63188', '0.90554', '1.23340', '2.01239'] | Gamma1 Grad: ['-0.00062', '-0.00011', '-0.00055', '-0.00335', '0.00169', '-0.00040', '-0.00139', '0.00296', '-0.00028', '-0.00009', '0.00102', '0.00050', '-0.00084']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.620% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.620% | ğŸ† Best Train Accuracy: 98.620%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.620% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.47it/s, Test_acc=67.5, Test_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.500% | ğŸ† Best Test Accuracy: 67.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.20it/s, Train_acc=98.7, Train_loss=0.073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00136 | Gamma1: ['3.42192', '3.94660', '2.59046', '2.12978', '2.10654', '2.36153', '2.64697', '2.98962', '3.43642', '2.64456', '0.94842', '1.23515', '2.04504'] | Gamma1 Grad: ['0.00528', '-0.00956', '0.00293', '0.10475', '-0.11901', '-0.06838', '0.01086', '-0.01073', '0.01356', '0.00518', '-0.01295', '0.01465', '0.01289']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.706% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.706% | ğŸ† Best Train Accuracy: 98.706%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.706% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.14it/s, Test_acc=67.7, Test_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.670% | ğŸ† Best Test Accuracy: 67.670%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.21it/s, Train_acc=98.8, Train_loss=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00134 | Gamma1: ['3.41427', '3.92736', '2.56717', '2.13988', '2.08694', '2.36595', '2.64233', '2.96753', '3.43431', '2.66134', '0.95124', '1.27798', '2.06222'] | Gamma1 Grad: ['0.01103', '0.00700', '0.01804', '-0.05925', '0.02625', '0.05356', '-0.02431', '0.01745', '0.00234', '-0.01014', '-0.07650', '-0.01973', '-0.01444']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.776% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.776% | ğŸ† Best Train Accuracy: 98.776%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.776% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.49it/s, Test_acc=67.6, Test_loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.610% | ğŸ† Best Test Accuracy: 67.670%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.94it/s, Train_acc=98.9, Train_loss=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00133 | Gamma1: ['3.41365', '3.92014', '2.59007', '2.08992', '2.10058', '2.37234', '2.65189', '2.95984', '3.42755', '2.66770', '0.98851', '1.28809', '2.09774'] | Gamma1 Grad: ['0.00066', '0.00904', '0.02996', '0.03602', '0.01340', '-0.00110', '0.01060', '-0.01730', '0.00205', '-0.00280', '0.05066', '-0.00273', '-0.00522']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.906% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.906% | ğŸ† Best Train Accuracy: 98.906%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.906% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.38it/s, Test_acc=67.9, Test_loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.910% | ğŸ† Best Test Accuracy: 67.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.79it/s, Train_acc=98.9, Train_loss=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00131 | Gamma1: ['3.42899', '3.92114', '2.57628', '2.08959', '2.13290', '2.34234', '2.65811', '2.94490', '3.43499', '2.67155', '1.01589', '1.28124', '2.14302'] | Gamma1 Grad: ['-0.04586', '0.00961', '0.04076', '-0.12574', '0.13913', '0.00474', '0.09113', '-0.09367', '-0.00021', '-0.01407', '-0.06960', '0.03419', '-0.04705']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.884% | ğŸ† Best Train Accuracy: 98.906%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.906% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.15it/s, Test_acc=67.8, Test_loss=2.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.760% | ğŸ† Best Test Accuracy: 67.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.22it/s, Train_acc=98.9, Train_loss=0.064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00129 | Gamma1: ['3.45653', '3.93762', '2.57189', '2.08674', '2.10104', '2.31663', '2.64745', '2.97203', '3.42646', '2.68237', '1.02847', '1.28576', '2.17688'] | Gamma1 Grad: ['-0.00433', '0.00043', '-0.01368', '0.01659', '0.00753', '-0.01073', '-0.00580', '0.00327', '-0.00211', '-0.00025', '0.00390', '-0.00076', '0.00771']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.928% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.928% | ğŸ† Best Train Accuracy: 98.928%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.928% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 90 | TrainLossHist: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.14it/s, Test_acc=67.5, Test_loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.510% | ğŸ† Best Test Accuracy: 67.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.69it/s, Train_acc=99, Train_loss=0.064]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00127 | Gamma1: ['3.45236', '3.94304', '2.55328', '2.09504', '2.10442', '2.29523', '2.65370', '2.97775', '3.43318', '2.68725', '1.02518', '1.29501', '2.20712'] | Gamma1 Grad: ['-0.01796', '0.01657', '0.01765', '-0.00146', '0.11292', '-0.00363', '-0.01845', '-0.02197', '-0.00123', '-0.00255', '-0.00449', '0.01724', '-0.01632']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.956% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.956% | ğŸ† Best Train Accuracy: 98.956%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.956% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.75it/s, Test_acc=67.5, Test_loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.500% | ğŸ† Best Test Accuracy: 67.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.80it/s, Train_acc=99.1, Train_loss=0.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00125 | Gamma1: ['3.45064', '3.93983', '2.54911', '2.07858', '2.08602', '2.32481', '2.63515', '2.96232', '3.43053', '2.69884', '1.06248', '1.28077', '2.22244'] | Gamma1 Grad: ['0.01274', '0.00315', '0.01191', '0.07541', '-0.01992', '0.00077', '0.02719', '0.00352', '-0.00234', '0.00090', '-0.01419', '-0.01762', '0.01288']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.068% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.068% | ğŸ† Best Train Accuracy: 99.068%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.068% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.20it/s, Test_acc=67.6, Test_loss=2.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.640% | ğŸ† Best Test Accuracy: 67.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.50it/s, Train_acc=99.1, Train_loss=0.061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00122 | Gamma1: ['3.45038', '3.91509', '2.55644', '2.04600', '2.06765', '2.31724', '2.62083', '2.96927', '3.42838', '2.70573', '1.07106', '1.31007', '2.24753'] | Gamma1 Grad: ['-0.03414', '0.03507', '-0.11294', '0.06516', '-0.21673', '-0.21953', '0.08125', '-0.04039', '0.00024', '0.00652', '-0.00493', '-0.01987', '0.02944']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.072% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.072% | ğŸ† Best Train Accuracy: 99.072%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.072% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 78.49it/s, Test_acc=67.8, Test_loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.830% | ğŸ† Best Test Accuracy: 67.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.69it/s, Train_acc=99.1, Train_loss=0.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00120 | Gamma1: ['3.45244', '3.90901', '2.56776', '2.02168', '2.09417', '2.30510', '2.64086', '2.96420', '3.42079', '2.70914', '1.06903', '1.29637', '2.25852'] | Gamma1 Grad: ['0.00869', '-0.00124', '0.00353', '0.01610', '-0.02919', '-0.07895', '-0.02832', '0.01528', '-0.00452', '-0.00083', '-0.01104', '-0.00519', '0.00429']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.064% | ğŸ† Best Train Accuracy: 99.072%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.072% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 79.07it/s, Test_acc=67.5, Test_loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.500% | ğŸ† Best Test Accuracy: 67.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.11it/s, Train_acc=99.2, Train_loss=0.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00118 | Gamma1: ['3.46045', '3.90981', '2.53996', '2.04334', '2.07458', '2.28659', '2.64703', '2.95837', '3.42064', '2.72154', '1.08339', '1.25246', '2.29822'] | Gamma1 Grad: ['-0.01621', '-0.01085', '-0.00677', '-0.00125', '0.00366', '-0.02351', '0.00538', '-0.00212', '-0.00392', '-0.00229', '-0.06615', '0.04677', '-0.01808']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.190% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.190% | ğŸ† Best Train Accuracy: 99.190%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.190% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.97it/s, Test_acc=67.6, Test_loss=2.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.610% | ğŸ† Best Test Accuracy: 67.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.91it/s, Train_acc=99.2, Train_loss=0.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00115 | Gamma1: ['3.46456', '3.90053', '2.52867', '2.05709', '2.07393', '2.28674', '2.62296', '2.94535', '3.42760', '2.72235', '1.07308', '1.26004', '2.32403'] | Gamma1 Grad: ['0.00201', '0.00044', '-0.00224', '-0.00630', '-0.02523', '-0.00521', '-0.00980', '-0.00962', '0.00142', '0.00291', '0.00199', '0.01010', '0.00694']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.212% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.212% | ğŸ† Best Train Accuracy: 99.212%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.212% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.88it/s, Test_acc=67.9, Test_loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.930% | ğŸ† Best Test Accuracy: 67.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.01it/s, Train_acc=99.2, Train_loss=0.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00113 | Gamma1: ['3.47309', '3.89512', '2.54214', '2.06177', '2.08899', '2.29569', '2.57807', '2.94711', '3.41533', '2.73543', '1.07790', '1.26598', '2.34408'] | Gamma1 Grad: ['-0.01666', '0.02491', '0.04444', '0.03957', '0.13755', '-0.05443', '0.00333', '-0.00698', '-0.00438', '-0.01662', '-0.03334', '-0.07790', '0.02924']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.188% | ğŸ† Best Train Accuracy: 99.212%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.212% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.14it/s, Test_acc=67.6, Test_loss=2.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.580% | ğŸ† Best Test Accuracy: 67.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.33it/s, Train_acc=99.2, Train_loss=0.055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00110 | Gamma1: ['3.47299', '3.90853', '2.52306', '2.05397', '2.07067', '2.27877', '2.57051', '2.94711', '3.41449', '2.73930', '1.07791', '1.29379', '2.37715'] | Gamma1 Grad: ['0.00114', '-0.00099', '0.00241', '-0.00227', '0.00257', '0.00105', '0.00238', '-0.00342', '0.00008', '0.00024', '-0.01733', '-0.01215', '-0.00451']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.236% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.236% | ğŸ† Best Train Accuracy: 99.236%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.236% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.53it/s, Test_acc=67.5, Test_loss=2.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.540% | ğŸ† Best Test Accuracy: 67.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.59it/s, Train_acc=99.2, Train_loss=0.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00108 | Gamma1: ['3.46820', '3.90800', '2.50372', '2.06050', '2.09645', '2.27976', '2.59132', '2.94043', '3.39435', '2.74794', '1.09239', '1.31506', '2.38171'] | Gamma1 Grad: ['-0.00559', '-0.00764', '-0.00367', '0.03671', '0.02483', '-0.00781', '-0.01637', '-0.00812', '0.00588', '-0.00134', '-0.01668', '0.01879', '-0.00332']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.168% | ğŸ† Best Train Accuracy: 99.236%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.236% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.30it/s, Test_acc=67.5, Test_loss=2.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.510% | ğŸ† Best Test Accuracy: 67.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\AblationExperiments\\AdaptiveTarget-No_AdaptiveTarget\\Results\\CIFAR100_Test_no_target_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "Best Test Accuracy:  67.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 11. TRAIN MODEL WITH SHEDULAR | XXX ----------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Set Seed for Reproducibility BEFORE training starts\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)  \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "# âœ… Training Loop\n",
    "num_epochs = 100 # Example: Set the total number of epochs\n",
    "for epoch in range(start_epoch, num_epochs):   # Runs training for 100 epochs\n",
    "\n",
    "    train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler, WARMUP_ACTIVATION_EPOCHS) # âœ… Pass required arguments\n",
    "\n",
    "    test(epoch)  # âœ… Test the model\n",
    "    tqdm.write(\"\")  # âœ… Clear leftover progress bar from test()\n",
    "\n",
    "\n",
    "print(\"Best Test Accuracy: \", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51b858-94f7-4925-a5d0-d77a4b2cad83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'âœ… Annotated comparison plots with BEST accuracy markers saved to ./Results/Plots'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 550x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 13.  ADAPTIVE REGULARIZATION VS NO ADAPTIVE REGULARIZATION | XXX -----------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.patheffects as path_effects\n",
    "import os\n",
    "\n",
    "def read_test_log(file_path):\n",
    "    test_loss_history = []\n",
    "    test_acc_history = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if \"Test Loss\" in line and \"Test Acc\" in line:\n",
    "                try:\n",
    "                    loss = float(line.split(\"Test Loss:\")[1].split(\"|\")[0].strip())\n",
    "                    acc = float(line.split(\"Test Acc:\")[1].split(\"%\")[0].strip())\n",
    "                    test_loss_history.append(loss)\n",
    "                    test_acc_history.append(acc)\n",
    "                except:\n",
    "                    continue\n",
    "    return test_loss_history, test_acc_history\n",
    "\n",
    "def plot_train_test_metrics(save_dir=\"./Results/Plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    TargetPenalty_test_log_path = f'./Results_AdaptiveTarget/CIFAR100_Test_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "    noTargetPenalty_test_log_path = f'./Results/CIFAR100_Test_{target_mode}_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "\n",
    "    TargetPenalty_test_loss, TargetPenalty_test_acc = read_test_log(TargetPenalty_test_log_path)\n",
    "    noTargetPenalty_test_loss, noTargetPenalty_test_acc = read_test_log(noTargetPenalty_test_log_path)\n",
    "\n",
    "    num_epochs = min(len(TargetPenalty_test_loss), len(noTargetPenalty_test_loss))\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    COLOR_SCALE = ['#00295B', '#CF0A66']  # TargetPenalty, noTargetPenalty\n",
    "    rcParams.update({\n",
    "        \"font.size\": 11,\n",
    "        \"axes.titlesize\": 11,\n",
    "        \"axes.labelsize\": 13,\n",
    "        \"xtick.labelsize\": 11,\n",
    "        \"ytick.labelsize\": 11,\n",
    "        \"axes.labelweight\": \"bold\",\n",
    "        \"xtick.color\": \"black\",\n",
    "        \"ytick.color\": \"black\",\n",
    "    })\n",
    "\n",
    "    # Custom settings\n",
    "    custom_yticks_test_loss = [1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
    "    custom_yticks_test_acc = [10, 20, 30, 40, 50, 60, 70]\n",
    "    custom_xticks = [0, 20, 40, 60, 80, 100]\n",
    "    custom_yaxis_test_loss = [1.2, 4.2]\n",
    "    custom_yaxis_test_acc = [35, 72]\n",
    "    custom_xaxis = [0, 105]\n",
    "\n",
    "    # Offsets\n",
    "    y_offset_loss_tp = 0.2\n",
    "    y_offset_loss_ntp = 0.07\n",
    "    x_offset_loss_tp = 3.5\n",
    "    x_offset_loss_ntp = 3.5\n",
    "\n",
    "    y_offset_acc_tp = 1\n",
    "    y_offset_acc_ntp = 3.2\n",
    "    x_offset_acc_tp = 8.5\n",
    "    x_offset_acc_ntp = 6.5\n",
    "\n",
    "    # ğŸ”· Plot Test Loss\n",
    "    fig1, ax1 = plt.subplots(figsize=(5.5, 3.5))\n",
    "    ax1.plot(epochs, TargetPenalty_test_loss[:num_epochs], label=\"TargetPenalty\", color=COLOR_SCALE[0], linewidth=2)\n",
    "    ax1.plot(epochs, noTargetPenalty_test_loss[:num_epochs], label=\"No TargetPenalty\", color=COLOR_SCALE[1], linestyle='--', linewidth=2)\n",
    "    ax1.set_xlabel(\"Epoch\", fontweight='bold')\n",
    "    ax1.set_ylabel(\"Test Loss\", fontweight='bold')\n",
    "    ax1.set_xticks(custom_xticks)\n",
    "    ax1.set_yticks(custom_yticks_test_loss)\n",
    "    ax1.set_xlim(custom_xaxis)\n",
    "    ax1.set_ylim(custom_yaxis_test_loss)\n",
    "    ax1.tick_params(axis='x', width=1.5)\n",
    "    ax1.tick_params(axis='y', width=1.5)\n",
    "    for label in ax1.get_xticklabels() + ax1.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    leg1 = ax1.legend(fontsize='small', loc=\"upper right\")\n",
    "    for text in leg1.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Add final loss markers\n",
    "    ax1.plot(epochs[-1], TargetPenalty_test_loss[-1], marker='o', color=COLOR_SCALE[0], markersize=4)\n",
    "    ax1.text(epochs[-1] - x_offset_loss_tp, TargetPenalty_test_loss[-1] - y_offset_loss_tp,\n",
    "             f\"{TargetPenalty_test_loss[-1]:.2f}\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "    ax1.plot(epochs[-1], noTargetPenalty_test_loss[-1], marker='o', color=COLOR_SCALE[1], markersize=4)\n",
    "    ax1.text(epochs[-1] - x_offset_loss_ntp, noTargetPenalty_test_loss[-1] + y_offset_loss_ntp,\n",
    "             f\"{noTargetPenalty_test_loss[-1]:.2f}\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"compare_test_loss_targetpenalty_vs_no_targetpenalty.svg\"),\n",
    "                format='svg', transparent=True, bbox_inches='tight')\n",
    "    plt.close(fig1)\n",
    "\n",
    "    # ğŸ”¶ Plot Test Accuracy\n",
    "    best_epoch_tp = TargetPenalty_test_acc.index(max(TargetPenalty_test_acc)) + 1\n",
    "    best_acc_tp = max(TargetPenalty_test_acc)\n",
    "    best_epoch_ntp = noTargetPenalty_test_acc.index(max(noTargetPenalty_test_acc)) + 1\n",
    "    best_acc_ntp = max(noTargetPenalty_test_acc)\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(5.5, 3.5))\n",
    "    ax2.plot(epochs, TargetPenalty_test_acc[:num_epochs], label=\"Adaptive Target Penalty\", color=COLOR_SCALE[0], linewidth=2)\n",
    "    ax2.plot(epochs, noTargetPenalty_test_acc[:num_epochs], label=\"No Adaptive Target Penalty\", color=COLOR_SCALE[1], linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel(\"Epoch\", fontweight='bold')\n",
    "    ax2.set_ylabel(\"Test Accuracy (%)\", fontweight='bold')\n",
    "    ax2.set_xticks(custom_xticks)\n",
    "    ax2.set_yticks(custom_yticks_test_acc)\n",
    "    ax2.set_xlim(custom_xaxis)\n",
    "    ax2.set_ylim(custom_yaxis_test_acc)\n",
    "    ax2.tick_params(axis='x', width=1.5)\n",
    "    ax2.tick_params(axis='y', width=1.5)\n",
    "    for label in ax2.get_xticklabels() + ax2.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "    leg2 = ax2.legend(fontsize='small', loc=\"lower right\")\n",
    "    for text in leg2.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Markers for best accuracy\n",
    "    ax2.plot(best_epoch_tp, best_acc_tp - 0.21, marker='o', color=COLOR_SCALE[0], markersize=5.5, markeredgecolor='black', markeredgewidth=1)\n",
    "    ax2.text(best_epoch_tp - x_offset_acc_tp, best_acc_tp + y_offset_acc_tp,\n",
    "             f\"{best_acc_tp:.2f}%\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "    ax2.plot(best_epoch_ntp, best_acc_ntp - 0.4, marker='o', color=COLOR_SCALE[1], markersize=5.5, markeredgecolor='black', markeredgewidth=1)\n",
    "    ax2.text(best_epoch_ntp - x_offset_acc_ntp, best_acc_ntp - y_offset_acc_ntp,\n",
    "             f\"{best_acc_ntp:.2f}%\", fontsize=10, color='black', fontweight='bold',\n",
    "             path_effects=[path_effects.Stroke(linewidth=1.5, foreground='white'), path_effects.Normal()])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"compare_test_accuracy_targetpenalty_vs_no_targetpenalty.svg\"),\n",
    "                format='svg', transparent=True, bbox_inches='tight')\n",
    "    # plt.close(fig2)\n",
    "\n",
    "    return f\"âœ… Annotated comparison plots with BEST accuracy markers saved to {save_dir}\"\n",
    "\n",
    "# ğŸ”· Call the function\n",
    "plot_train_test_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
