{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96816f-da38-4ac7-9d6f-13f358470117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Current working directory: C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\n",
      "âœ… sys.path updated:\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\python310.zip\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\DLLs\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\n",
      "   ğŸ“‚ \n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\Pythonwin\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\models\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\activation\n",
      "âœ… FFTGate imported successfully!\n",
      "âœ… FFTGate instance created successfully!\n",
      "âœ… FFTGate_VGG imported successfully!\n",
      "CIFAR100 Training Script Initialized...\n",
      "Using device: cuda\n",
      "Parsed learning rate: 0.001 (type: <class 'float'>)\n",
      "Formatted learning rate for filenames: 0_001\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Length of training dataset: 50000\n",
      "Length of testing dataset: 10000\n",
      "Number of classes in CIFAR-100: 100\n",
      "==> Building model..\n",
      "âœ… Found 13 FFTGate layers.\n",
      "âœ… Collected 13 trainable activation parameters.\n",
      "   ğŸ”¹ Layer 0: FFTGate()\n",
      "   ğŸ”¹ Layer 1: FFTGate()\n",
      "   ğŸ”¹ Layer 2: FFTGate()\n",
      "   ğŸ”¹ Layer 3: FFTGate()\n",
      "   ğŸ”¹ Layer 4: FFTGate()\n",
      "   ğŸ”¹ Layer 5: FFTGate()\n",
      "   ğŸ”¹ Layer 6: FFTGate()\n",
      "   ğŸ”¹ Layer 7: FFTGate()\n",
      "   ğŸ”¹ Layer 8: FFTGate()\n",
      "   ğŸ”¹ Layer 9: FFTGate()\n",
      "   ğŸ”¹ Layer 10: FFTGate()\n",
      "   ğŸ”¹ Layer 11: FFTGate()\n",
      "   ğŸ”¹ Layer 12: FFTGate()\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 1.A. IMPORTS LIBRARIES | XXX -----------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\"\"\"Train CIFAR100 with PyTorch.\"\"\"\n",
    "\n",
    "# Python 2/3 compatibility\n",
    "# from __future__ import print_function\n",
    "\n",
    "\n",
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# PyTorch and related modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# torchvision for datasets and transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch_optimizer as torch_opt  # Use 'torch_opt' for torch_optimizer\n",
    "from timm.scheduler import CosineLRScheduler \n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Define currect working directory to ensure on right directory\n",
    "VGG16_PATH = r\"C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\"\n",
    "if os.getcwd() != VGG16_PATH:\n",
    "    os.chdir(VGG16_PATH)\n",
    "print(f\"âœ… Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# âœ… Define absolute paths\n",
    "PROJECT_PATH = VGG16_PATH\n",
    "MODELS_PATH = os.path.join(VGG16_PATH, \"models\")\n",
    "ACTIVATION_PATH = os.path.join(VGG16_PATH, \"activation\")\n",
    "# PAU_PATH = os.path.join(VGG16_PATH, \"pau\")\n",
    "\n",
    "# âœ… Ensure necessary paths are in sys.path\n",
    "for path in [PROJECT_PATH, MODELS_PATH, ACTIVATION_PATH]:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "# âœ… Print updated sys.path for debugging\n",
    "print(\"âœ… sys.path updated:\")\n",
    "for path in sys.path:\n",
    "    print(\"   ğŸ“‚\", path)\n",
    "\n",
    "# âœ… Import FFTGate (Check if the module exists)\n",
    "try:\n",
    "    from activation.FFTGate import FFTGate  # type: ignore\n",
    "    print(\"âœ… FFTGate imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ Import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'Activation4.py' exists inside: {ACTIVATION_PATH}\")\n",
    "\n",
    "# âœ… Test if FFTGate is callable\n",
    "try:\n",
    "    activation_test = FFTGate()\n",
    "    print(\"âœ… FFTGate instance created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error while initializing FFTGate: {e}\")\n",
    "\n",
    "# âœ… Now import FFTGate_VGG (Ensure module exists inside models/)\n",
    "try:\n",
    "    from models.FFTGate_VGG import FFTGate_VGG  # type: ignore\n",
    "    print(\"âœ… FFTGate_VGG imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ FFTGate_VGG import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'FFTGate_VGG.py' exists inside: {MODELS_PATH}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 1.B. SEEDING FOR REPRODUCIBILITY | XXX -------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def set_seed_torch(seed):\n",
    "    torch.manual_seed(seed)                          \n",
    "\n",
    "\n",
    "\n",
    "def set_seed_main(seed):\n",
    "    random.seed(seed)                                ## Python's random module\n",
    "    np.random.seed(seed)                             ## NumPy's random module\n",
    "    torch.cuda.manual_seed(seed)                     ## PyTorch's random module for CUDA\n",
    "    torch.cuda.manual_seed_all(seed)                 ## Seed for all CUDA devices\n",
    "    torch.backends.cudnn.deterministic = True        ## Ensure deterministic behavior for CuDNN\n",
    "    torch.backends.cudnn.benchmark = False           ## Disable CuDNN's autotuning for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)   \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (Optional) Import Optimizers - Uncomment as needed\n",
    "# from Opt import opt\n",
    "# from diffGrad import diffGrad\n",
    "# from diffRGrad import diffRGrad, SdiffRGrad, BetaDiffRGrad, Beta12DiffRGrad, BetaDFCDiffRGrad\n",
    "# from RADAM import Radam, BetaRadam\n",
    "# from BetaAdam import BetaAdam, BetaAdam1, BetaAdam2, BetaAdam3, BetaAdam4, BetaAdam5, BetaAdam6, BetaAdam7, BetaAdam4A\n",
    "# from AdamRM import AdamRM, AdamRM1, AdamRM2, AdamRM3, AdamRM4, AdamRM5\n",
    "# from sadam import sadam\n",
    "# from SdiffGrad import SdiffGrad\n",
    "# from SRADAM import SRADAM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 2. DEFINE MODEL Lr | XXX ---------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Main Execution (Placeholder)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"CIFAR100 Training Script Initialized...\")\n",
    "    # Add your training pipeline here\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Argument parser to get user inputs\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR100 Training')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "\n",
    "args, unknown = parser.parse_known_args()  # Avoids Jupyter argument issues\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Ensure lr is correctly parsed\n",
    "lr = args.lr  # Get learning rate from argparse\n",
    "lr_str = str(lr).replace('.', '_')  # Convert to string and replace '.' for filenames\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Parsed learning rate: {lr} (type: {type(lr)})\")\n",
    "print(f\"Formatted learning rate for filenames: {lr_str}\")\n",
    "\n",
    "# Initialize training variables\n",
    "best_acc = 0  # Best test accuracy\n",
    "start_epoch = 0  # Start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 3. LOAD DATASET | XXX ------------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "bs = 64 #set batch size\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Length of train and test datasets\n",
    "len_train = len(trainset)\n",
    "len_test = len(testset)\n",
    "print(f\"Length of training dataset: {len_train}\")\n",
    "print(f\"Length of testing dataset: {len_test}\")\n",
    "\n",
    "# âœ… Print number of classes\n",
    "num_classes_Print = len(trainset.classes)\n",
    "print(f\"Number of classes in CIFAR-100: {num_classes_Print}\")\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 4. DYNAMIC REGULARIZATION| XXX ---------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def apply_dynamic_regularization(inputs, feature_activations, epoch,\n",
    "                                  prev_params, layer_index_map, batch_idx):\n",
    "\n",
    "\n",
    "    global activation_layers  # âœ… Reference already-collected layers\n",
    "\n",
    "    # âœ… Print gamma1 stats early in training for monitoring\n",
    "    if batch_idx == 0 and epoch <= 4:\n",
    "        print(f\"\\nğŸš¨ ENTERED apply_dynamic_regularization | Epoch={epoch} | Batch={batch_idx}\", flush=True)\n",
    "\n",
    "        # ğŸ§  Print gamma1 details\n",
    "        all_layer_info = []\n",
    "        for idx, layer in enumerate(activation_layers):\n",
    "            param = getattr(layer, \"gamma1\")\n",
    "            all_layer_info.append(f\"Layer {idx}: ID={id(param)} | Mean={param.mean().item():.5f}\")\n",
    "        print(\"ğŸ§  GAMMA1 INFO:\", \" | \".join(all_layer_info), flush=True)\n",
    "\n",
    "    # âœ… Initialize gamma1 regularization accumulator\n",
    "    gamma1_reg = 0.0\n",
    "\n",
    "    # âœ… Compute batch std and define regularization strength\n",
    "    batch_std = torch.std(inputs) + 1e-6\n",
    "    regularization_strength = 0.05 if epoch < 40 else (0.01 if epoch < 60 else 0.005)\n",
    "\n",
    "    # âœ… Track layers where noise is injected (informative)\n",
    "    noisy_layers = []\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        if idx not in layer_index_map:\n",
    "            continue\n",
    "\n",
    "        prev_layer_params = prev_params[layer_index_map[idx]]\n",
    "        param_name = \"gamma1\"\n",
    "        param = getattr(layer, param_name)\n",
    "        prev_param = prev_layer_params[param_name]\n",
    "\n",
    "        # âœ… Target based on input stats\n",
    "        target = compute_target(param_name, batch_std)\n",
    "\n",
    "        # âœ… Adaptive Target Regularization\n",
    "        gamma1_reg += regularization_strength * (param - target).pow(2).mean() * 1.2\n",
    "\n",
    "        # âœ… Adaptive Cohesion Regularization\n",
    "        cohesion = (param - prev_param).pow(2)\n",
    "        gamma1_reg += 0.005 * cohesion.mean()\n",
    "\n",
    "        # âœ… Adaptive Noise Regularization\n",
    "        epoch_AddNoise = 50\n",
    "        if epoch > epoch_AddNoise:\n",
    "            param_variation = torch.abs(param - prev_param).mean()\n",
    "            if param_variation < 0.015:\n",
    "                noise = (0.001 + 0.0004 * batch_std.item()) * torch.randn_like(param)\n",
    "                penalty = (param - (prev_param + noise)).pow(2).sum()\n",
    "                gamma1_reg += 0.00015 * penalty\n",
    "                noisy_layers.append(f\"{idx} (Î”={param_variation.item():.5f})\") # Collect index and variation\n",
    "\n",
    "    # âœ… Print noise injection summary\n",
    "    if batch_idx == 0 and epoch <= (epoch_AddNoise + 4) and noisy_layers:\n",
    "        print(f\"ğŸ”¥ Stable Noise Injected | Epoch {epoch} | Batch {batch_idx} | Layers: \" + \", \".join(noisy_layers), flush=True)\n",
    "    mags = feature_activations.abs().mean(dim=(0, 2, 3))\n",
    "    m = mags / mags.sum()\n",
    "    gamma1_reg += 0.005 * (-(m * torch.log(m + 1e-6)).sum())\n",
    "\n",
    "    return gamma1_reg\n",
    "\n",
    "\n",
    "def compute_target(param_name, batch_std):\n",
    "    if param_name == \"gamma1\":\n",
    "        return 2.0 + 0.2 * batch_std.item()  \n",
    "\n",
    "    raise ValueError(f\"Unknown param {param_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 5. INITIALIZE MODEL | XXX --------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "#net = Elliott_VGG('VGG16'); net1 = 'Elliott_VGG16'\n",
    "#net = GELU_MobileNet(); net1 = 'GELU_MobileNet'\n",
    "#net = GELU_SENet18(); net1 = 'GELU_SENet18'\n",
    "#net = PDELU_ResNet50(); net1 = 'PDELU_ResNet50'\n",
    "# net = Sigmoid_GoogLeNet(); net1 = 'Sigmoid_GoogLeNet'\n",
    "#net = GELU_DenseNet121(); net1 = 'GELU_DenseNet121'\n",
    "# net = ReLU_VGG('VGG16'); net1 = 'ReLU_VGG16'\n",
    "net = FFTGate_VGG('VGG16'); net1 = 'FFTGate_VGG16'\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9); optimizer1 = 'SGDM5'\n",
    "#optimizer = optim.Adagrad(net.parameters()); optimizer1 = 'AdaGrad'\n",
    "#optimizer = optim.Adadelta(net.parameters()); optimizer1 = 'AdaDelta'\n",
    "#optimizer = optim.RMSprop(net.parameters()); optimizer1 = 'RMSprop'\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr); optimizer1 = 'Adam'\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr, amsgrad=True); optimizer1 = 'amsgrad'\n",
    "#optimizer = diffGrad(net.parameters(), lr=args.lr); optimizer1 = 'diffGrad'\n",
    "#optimizer = Radam(net.parameters(), lr=args.lr); optimizer1 = 'Radam'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 6. INITIALIZE ACTIVATION PARAMETERS, OPTIMIZERS & SCHEDULERS | XXX ---------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# âœ… Step 1: Collect Activation Parameters from ALL Layers (Ensure Compatibility with DataParallel)\n",
    "if isinstance(net, torch.nn.DataParallel):\n",
    "    features = net.module.features\n",
    "else:\n",
    "    features = net.features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 2: Recursively search for FFTGate layers\n",
    "activation_params = []\n",
    "activation_layers = []\n",
    "\n",
    "for layer in features:\n",
    "    if isinstance(layer, FFTGate):  \n",
    "        activation_layers.append(layer)\n",
    "        activation_params.append(layer.gamma1)  # âœ… Only gamma1 is trainable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 3: Define Unfreeze Epoch\n",
    "unfreeze_activation_epoch = 1  # âœ… Change this value if needed\n",
    "# unfreeze_activation_epoch = 10  # âœ… Delay unfreezing until epoch 10\n",
    "\n",
    "\n",
    "# âœ… Define the warm-up epoch value\n",
    "# WARMUP_ACTIVATION_EPOCHS = 5  # The number of epochs for warm-up\n",
    "WARMUP_ACTIVATION_EPOCHS = 0  # The number of epochs for warm-up\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initially Freeze Activation Parameters\n",
    "for param in activation_params:\n",
    "    param.requires_grad = False  # ğŸš« Keep frozen before the unfreeze epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 4: Initialize Activation Optimizers (Using AdamW for Better Weight Decay)\n",
    "activation_optimizers = {\n",
    "    \"gamma1\": torch.optim.AdamW(activation_params, lr=0.0015, weight_decay=1e-6)  # ğŸ”º Reduce LR from 0.005 â†’ 0.0025\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 5: Initialize Activation Schedulers with Warm Restarts (Per Parameter Type)\n",
    "activation_schedulers = {\n",
    "    \"gamma1\": CosineAnnealingWarmRestarts(\n",
    "        activation_optimizers[\"gamma1\"],\n",
    "        T_0=10,      # Shorter cycle to explore aggressively\n",
    "        T_mult=2,    # Increase cycle length gradually\n",
    "        eta_min=5e-5  # âœ… recommended safer modification\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 6: Print collected activation layers and parameters\n",
    "if activation_layers and activation_params:\n",
    "    print(f\"âœ… Found {len(activation_layers)} FFTGate layers.\")\n",
    "    print(f\"âœ… Collected {len(activation_params)} trainable activation parameters.\")\n",
    "    \n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        print(f\"   ğŸ”¹ Layer {idx}: {layer}\")\n",
    "\n",
    "elif activation_layers and not activation_params:\n",
    "    print(f\"âš  Warning: Found {len(activation_layers)} FFTGate layers, but no trainable parameters were collected.\")\n",
    "\n",
    "elif activation_params and not activation_layers:\n",
    "    print(f\"âš  Warning: Collected {len(activation_params)} activation parameters, but no FFTGate layers were recorded.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš  Warning: No FFTGate layers or activation parameters found! Skipping activation optimizer.\")\n",
    "    activation_optimizers = None\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 7. INITIALIZE MAIN OPTIMIZER SCHEDULER | XXX -------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Step 6: Define MultiStepLR for Main Optimizer\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "main_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 8. MODEL CHECK POINT | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists('checkpoint'):\n",
    "    os.makedirs('checkpoint')\n",
    "\n",
    "if not os.path.exists('Results'):\n",
    "    os.makedirs('Results')\n",
    "\n",
    "# Construct checkpoint path\n",
    "checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr}_{net1}_{optimizer1}.t7'\n",
    "\n",
    "# Resume checkpoint only if file exists\n",
    "if args.resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 9. DEFINE TRAIN LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Training\n",
    "\n",
    "def train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler , WARMUP_ACTIVATION_EPOCHS):\n",
    "    global train_loss_history, best_train_acc, prev_params, recent_test_acc, gamma1_history, activation_layers, test_acc_history  # ğŸŸ¢ğŸŸ¢ğŸŸ¢\n",
    "\n",
    "    if epoch == 0:\n",
    "        train_loss_history = []\n",
    "        best_train_acc = 0.0\n",
    "        recent_test_acc = 0.0\n",
    "        gamma1_history = {}         # âœ… Initialize history\n",
    "        test_acc_history = []       # âœ… test accuracy history\n",
    "\n",
    "\n",
    "\n",
    "    prev_params = {}\n",
    "    layer_index_map = {idx: idx for idx in range(len(activation_layers))}  \n",
    "\n",
    "    # âœ… Cache previous gamma1 values from activation layers\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_accuracy = 0.0\n",
    "\n",
    "    # âœ… Initialize log history\n",
    "    log_history = []\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Define path to store Training log\n",
    "    save_paths = {\n",
    "       \n",
    "        \"log_history\": f\"C:\\\\Users\\\\emeka\\\\Research\\\\ModelCUDA\\\\Big_Data_Journal\\\\Comparison\\\\Code\\\\Paper\\\\github2\\\\CIFAR100\\\\VGG16\\\\Results\\\\FFTGate\\\\FFTGate_training_logs.txt\"  # âœ… Training log_history \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 1: Unfreeze Activation Parameters (Only Once Per Epoch)\n",
    "    if epoch == unfreeze_activation_epoch:\n",
    "        print(\"\\nğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\")\n",
    "        for layer in net.module.features if isinstance(net, torch.nn.DataParallel) else net.features:\n",
    "            if isinstance(layer, FFTGate):   \n",
    "                layer.gamma1.requires_grad = True  # âœ… Only gamma1 is trainable\n",
    "        print(\"âœ… Activation Parameters Unfrozen! ğŸš€\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 2: Gradual Warm-up for Activation Learning Rates (AFTER Unfreezing)\n",
    "    warmup_start = unfreeze_activation_epoch  # ğŸ”¹ Start warm-up when unfreezing happens\n",
    "    warmup_end = unfreeze_activation_epoch + WARMUP_ACTIVATION_EPOCHS  # ğŸ”¹ End warm-up period\n",
    "\n",
    "    # âœ… Adjust learning rates **only** during the warm-up phase\n",
    "    if warmup_start <= epoch < warmup_end:\n",
    "        warmup_factor = (epoch - warmup_start + 1) / WARMUP_ACTIVATION_EPOCHS  \n",
    "\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            for param_group in act_scheduler.optimizer.param_groups:\n",
    "                if \"initial_lr\" not in param_group:\n",
    "                    param_group[\"initial_lr\"] = param_group[\"lr\"]  # ğŸ”¹ Store initial LR\n",
    "                param_group[\"lr\"] = param_group[\"initial_lr\"] * warmup_factor  # ğŸ”¹ Scale LR\n",
    "\n",
    "        # âœ… Debugging output to track warm-up process\n",
    "        print(f\"ğŸ”¥ Warm-up Epoch {epoch}: Scaling LR by {warmup_factor:.3f}\")\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            print(f\"  ğŸ”¹ {name} LR: {act_scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    activation_history = []  # ğŸ”´ Initialize empty history at start of epoch (outside batch loop)\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Training Loop\n",
    "    with tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch}\") as progress:\n",
    "        for batch_idx, (inputs, targets) in progress:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # zero_grad activation parameter\n",
    "            for opt in activation_optimizers.values():\n",
    "                opt.zero_grad()\n",
    "\n",
    "\n",
    "            # âœ… Forward Pass\n",
    "            outputs = net(inputs, epoch=epoch, train_accuracy=train_accuracy, targets=targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            \n",
    "            feature_activations = features(inputs)  # Feature activations\n",
    "\n",
    "\n",
    "            # âœ… Collect Activation History | âœ… Per-layer mean activations\n",
    "            batch_means = [layer.saved_output.mean().item() for layer in activation_layers]\n",
    "            activation_history.extend(batch_means)\n",
    "\n",
    "            # âœ… Apply Decay strategy to history for each activation layer\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    if isinstance(layer, FFTGate):\n",
    "                        layer.decay_spectral_history(epoch, num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Compute Training Accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            train_accuracy = 100. * correct / total if total > 0 else 0.0  # Compute training accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Call Regularization Function for the Activation Parameter\n",
    "            if epoch > 0:\n",
    "                gamma1_reg = apply_dynamic_regularization(\n",
    "                    inputs, feature_activations, epoch,\n",
    "                    prev_params, layer_index_map, batch_idx\n",
    "                )\n",
    "                loss += gamma1_reg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… ğŸ¯ Adaptive Gradient Clipping of gamma1  \n",
    "            for layer in features:\n",
    "                if isinstance(layer, FFTGate):  # âœ… Ensure layer has gamma1 before clipping\n",
    "                    torch.nn.utils.clip_grad_norm_([layer.gamma1], max_norm=0.7)\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Apply Optimizer Step for Model Parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # âœ… Apply Optimizer Steps for Activation Parameters (Only if Unfrozen)\n",
    "            if epoch >= unfreeze_activation_epoch:\n",
    "                for opt in activation_optimizers.values():\n",
    "                    opt.step()\n",
    "\n",
    "\n",
    "            # âœ… Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Clamping of gamma1 (Applied AFTER Optimizer Step)\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    layer.gamma1.clamp_(0.1, 6.0)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Update progress bar\n",
    "            progress.set_postfix(Train_loss=round(train_loss / (batch_idx + 1), 3),\n",
    "                                 Train_acc=train_accuracy)  \n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step the main optimizer scheduler (ONLY for model parameters)\n",
    "    main_scheduler.step()\n",
    "\n",
    "    # âœ… Step the activation parameter schedulers (ONLY for activation parameters) | Epoch-wise stepping\n",
    "    if epoch >= unfreeze_activation_epoch:\n",
    "        for name, act_scheduler in activation_schedulers.items():  \n",
    "            act_scheduler.step()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… ONLY update prev_params here AFTER all updates | âœ… Update prev_params AFTER training epoch\n",
    "    for idx, layer in enumerate(activation_layers):      \n",
    "        prev_params[idx] = {\n",
    "            \"gamma1\": layer.gamma1.clone().detach()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Logging Activation Parameters & Gradients\n",
    "    last_batch_grads = {\"Gamma1 Grad\": []}\n",
    "    current_params = {\"Gamma1\": []}\n",
    "\n",
    "    for layer in features:\n",
    "        if isinstance(layer, FFTGate):  \n",
    "            # âœ… Convert gradients to scalar floats and format to 5 decimal places (removes device='cuda:0' and tensor(...))\n",
    "            last_batch_grads[\"Gamma1 Grad\"].append(f\"{layer.gamma1.grad.item():.5f}\" if layer.gamma1.grad is not None else \"None\")\n",
    "\n",
    "            # âœ… Collect current parameter values (already scalar), formatted to 5 decimal places\n",
    "            current_params[\"Gamma1\"].append(f\"{layer.gamma1.item():.5f}\")\n",
    "\n",
    "    # âœ… Build log message (showing params and gradients for ALL layers)\n",
    "    log_msg = (\n",
    "        f\"Epoch {epoch}: M_Optimizer LR => {optimizer.param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1 LR => {activation_optimizers['gamma1'].param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1: {current_params['Gamma1']} | \"\n",
    "        f\"Gamma1 Grad: {last_batch_grads['Gamma1 Grad']}\"\n",
    "    )\n",
    "\n",
    "    log_history.append(log_msg)\n",
    "    print(log_msg)  # âœ… Prints only once per epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (Clear old logs)\n",
    "    if epoch == 0:  # âœ… Only clear at the start of training\n",
    "        with open(save_paths[\"log_history\"], \"w\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Save logs once per epoch (Append new logs)\n",
    "    if log_history:\n",
    "        with open(save_paths[\"log_history\"], \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_history) + \"\\n\")         # âœ… Ensure each entry is on a new line\n",
    "        print(f\"ğŸ“œ Logs saved to {save_paths['log_history']}!\")  # âœ… Only prints once per epoch\n",
    "    else:\n",
    "        print(\"âš  No logs to save!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final training accuracy for the epoch\n",
    "    final_train_loss = train_loss / len(trainloader)\n",
    "    final_train_acc = 100. * correct / total\n",
    "\n",
    "    # âœ… Append to history\n",
    "    train_loss_history.append(final_train_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save training results (without affecting best accuracy tracking)\n",
    "    train_results_path = f'./Results/CIFAR100_Train_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "    # âœ… Clear the log file at the start of training (Epoch 0)\n",
    "    if epoch == 0 and os.path.exists(train_results_path):\n",
    "        with open(train_results_path, 'w') as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs only once\n",
    "\n",
    "    # âœ… Append new training results for each epoch\n",
    "    with open(train_results_path, 'a') as f:\n",
    "        f.write(f\"Epoch {epoch} | Train Loss: {final_train_loss:.3f} | Train Acc: {final_train_acc:.3f}%\\n\")\n",
    "\n",
    "    if final_train_acc > best_train_acc:\n",
    "        best_train_acc = final_train_acc  # âœ… Update best training accuracy\n",
    "        print(f\"ğŸ† New Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "    # âœ… Append the best training accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):  # Only log once at the final epoch\n",
    "        with open(train_results_path, 'a') as f:\n",
    "            f.write(f\"\\nğŸ† Best Training Accuracy: {best_train_acc:.3f}%\\n\")  \n",
    "\n",
    "    # âœ… Print both Final and Best Training Accuracy\n",
    "    print(f\"ğŸ“Š Train Accuracy: {final_train_acc:.3f}% | ğŸ† Best Train Accuracy: {best_train_acc:.3f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"ğŸ“œ Training logs saved to {train_results_path}!\")\n",
    "    print(f\"ğŸ† Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"ğŸ“ Sizes â†’ ActivationHist: {len(activation_history)} | TestAccHist: {len(test_acc_history)} | TrainLossHist: {len(train_loss_history)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # return final_train_loss, final_train_acc, feature_activations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 10. DEFINE TEST LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(epoch, save_results=True):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and optionally saves the results.\n",
    "    \n",
    "    Args:\n",
    "    - epoch (int): The current epoch number.\n",
    "    - save_results (bool): Whether to save results to a file.\n",
    "\n",
    "    Returns:\n",
    "    - acc (float): Test accuracy percentage.\n",
    "    \"\"\"\n",
    "    global best_acc, val_accuracy \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # âœ… Ensure activation function parameters are clamped before evaluation\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(testloader), total=len(testloader), desc=f\"Testing Epoch {epoch}\") as progress:\n",
    "            for batch_idx, (inputs, targets) in progress:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                # âœ… Pass validation accuracy to activation function\n",
    "                val_accuracy = 100. * correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "                # âœ… Update progress bar with loss & accuracy\n",
    "                progress.set_postfix(Test_loss=round(test_loss / (batch_idx + 1), 3),\n",
    "                                     Test_acc=round(val_accuracy, 3))\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final test accuracy\n",
    "    final_test_loss = test_loss / len(testloader)\n",
    "    final_test_acc = 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Ensure \"Results\" folder exists (just like training logs)\n",
    "    results_dir = os.path.join(PROJECT_PATH, \"Results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # âœ… Define log file path for test results\n",
    "    test_results_path = os.path.join(results_dir, f'CIFAR100_Test_B{bs}_LR{lr}_{net1}_{optimizer1}.txt')\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (clear old logs)\n",
    "    if epoch == 0:\n",
    "        with open(test_results_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Append new test results for each epoch (same style as training)\n",
    "    with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Epoch {epoch} | Test Loss: {final_test_loss:.3f} | Test Acc: {final_test_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save checkpoint if accuracy improves (does NOT interfere with logging)\n",
    "    if final_test_acc > best_acc:\n",
    "        print('ğŸ† Saving best model...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': final_test_acc,  # âœ… Ensures the best test accuracy is saved in checkpoint\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Ensure checkpoint directory exists\n",
    "        checkpoint_dir = \"checkpoint\"\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "        # âœ… Format learning rate properly before saving filename\n",
    "        lr_str = str(lr).replace('.', '_')\n",
    "        checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr_str}_{net1}_{optimizer1}.t7'\n",
    "        torch.save(state, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "        best_acc = final_test_acc  # âœ… Update best accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Append the best test accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):\n",
    "        with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\nğŸ† Best Test Accuracy: {best_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Print both Final and Best Test Accuracy (always executed)\n",
    "    print(f\"ğŸ“Š Test Accuracy: {final_test_acc:.3f}% | ğŸ† Best Test Accuracy: {best_acc:.3f}%\")\n",
    "    print(f\"ğŸ“œ Test logs saved to {test_results_path}!\")\n",
    "\n",
    "\n",
    "    global recent_test_acc\n",
    "    recent_test_acc = final_test_acc  # Capture latest test accuracy for next train() call | Store latest test accuracy\n",
    "\n",
    "    return final_test_acc  # âœ… Return the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d12b6d0-56ee-4b58-8c6c-83bbde2f998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:23<00:00, 32.64it/s, Train_acc=3.32, Train_loss=4.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000'] | Gamma1 Grad: ['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 3.318% (Updated)\n",
      "ğŸ“Š Train Accuracy: 3.318% | ğŸ† Best Train Accuracy: 3.318%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 3.318% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.46it/s, Test_acc=5.57, Test_loss=4.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 5.570% | ğŸ† Best Test Accuracy: 5.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "\n",
      "ğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\n",
      "âœ… Activation Parameters Unfrozen! ğŸš€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=1 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2329893388672 | Mean=1.50000 | Layer 1: ID=2329893393632 | Mean=1.50000 | Layer 2: ID=2329925812848 | Mean=1.50000 | Layer 3: ID=2329925813808 | Mean=1.50000 | Layer 4: ID=2329925814768 | Mean=1.50000 | Layer 5: ID=2329925815728 | Mean=1.50000 | Layer 6: ID=2329925816608 | Mean=1.50000 | Layer 7: ID=2329925817568 | Mean=1.50000 | Layer 8: ID=2329925818528 | Mean=1.50000 | Layer 9: ID=2329925819408 | Mean=1.50000 | Layer 10: ID=2329925820368 | Mean=1.50000 | Layer 11: ID=2329925821328 | Mean=1.50000 | Layer 12: ID=2329925822288 | Mean=1.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.93it/s, Train_acc=6.78, Train_loss=4.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.22795', '2.23494', '2.23014', '2.22774', '2.22733', '2.22671', '2.22577', '2.22691', '2.22785', '2.22699', '2.22252', '2.21943', '2.20776'] | Gamma1 Grad: ['0.00933', '-0.00427', '0.00018', '-0.00439', '-0.00850', '-0.00989', '-0.00877', '-0.00363', '-0.01255', '-0.01158', '-0.00627', '-0.02307', '0.00432']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 6.780% (Updated)\n",
      "ğŸ“Š Train Accuracy: 6.780% | ğŸ† Best Train Accuracy: 6.780%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 6.780% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.17it/s, Test_acc=6.95, Test_loss=3.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 6.950% | ğŸ† Best Test Accuracy: 6.950%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=2 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2329893388672 | Mean=2.22795 | Layer 1: ID=2329893393632 | Mean=2.23494 | Layer 2: ID=2329925812848 | Mean=2.23014 | Layer 3: ID=2329925813808 | Mean=2.22774 | Layer 4: ID=2329925814768 | Mean=2.22733 | Layer 5: ID=2329925815728 | Mean=2.22671 | Layer 6: ID=2329925816608 | Mean=2.22577 | Layer 7: ID=2329925817568 | Mean=2.22691 | Layer 8: ID=2329925818528 | Mean=2.22785 | Layer 9: ID=2329925819408 | Mean=2.22699 | Layer 10: ID=2329925820368 | Mean=2.22252 | Layer 11: ID=2329925821328 | Mean=2.21943 | Layer 12: ID=2329925822288 | Mean=2.20776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.63it/s, Train_acc=10.3, Train_loss=3.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.28884', '2.29982', '2.29246', '2.28344', '2.28527', '2.28075', '2.28175', '2.28316', '2.28382', '2.28266', '2.28037', '2.28211', '2.25888'] | Gamma1 Grad: ['-0.02591', '-0.00145', '0.00414', '-0.00905', '-0.00516', '-0.00106', '-0.00231', '0.00298', '-0.00554', '-0.00431', '-0.00276', '0.01448', '-0.01988']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 10.312% (Updated)\n",
      "ğŸ“Š Train Accuracy: 10.312% | ğŸ† Best Train Accuracy: 10.312%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 10.312% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.51it/s, Test_acc=13.1, Test_loss=3.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 13.100% | ğŸ† Best Test Accuracy: 13.100%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=3 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2329893388672 | Mean=2.28884 | Layer 1: ID=2329893393632 | Mean=2.29982 | Layer 2: ID=2329925812848 | Mean=2.29246 | Layer 3: ID=2329925813808 | Mean=2.28344 | Layer 4: ID=2329925814768 | Mean=2.28527 | Layer 5: ID=2329925815728 | Mean=2.28075 | Layer 6: ID=2329925816608 | Mean=2.28175 | Layer 7: ID=2329925817568 | Mean=2.28316 | Layer 8: ID=2329925818528 | Mean=2.28382 | Layer 9: ID=2329925819408 | Mean=2.28266 | Layer 10: ID=2329925820368 | Mean=2.28037 | Layer 11: ID=2329925821328 | Mean=2.28211 | Layer 12: ID=2329925822288 | Mean=2.25888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.75it/s, Train_acc=14.7, Train_loss=3.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.30452', '2.30929', '2.30599', '2.29050', '2.29394', '2.29187', '2.28966', '2.28794', '2.28910', '2.28720', '2.28508', '2.27090', '2.26998'] | Gamma1 Grad: ['-0.00124', '0.01234', '0.00805', '0.00987', '-0.00235', '0.00098', '-0.00544', '0.00027', '-0.01430', '-0.01423', '-0.00561', '-0.03088', '-0.04595']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 14.658% (Updated)\n",
      "ğŸ“Š Train Accuracy: 14.658% | ğŸ† Best Train Accuracy: 14.658%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 14.658% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.77it/s, Test_acc=18.6, Test_loss=3.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 18.550% | ğŸ† Best Test Accuracy: 18.550%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=4 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2329893388672 | Mean=2.30452 | Layer 1: ID=2329893393632 | Mean=2.30929 | Layer 2: ID=2329925812848 | Mean=2.30599 | Layer 3: ID=2329925813808 | Mean=2.29050 | Layer 4: ID=2329925814768 | Mean=2.29394 | Layer 5: ID=2329925815728 | Mean=2.29187 | Layer 6: ID=2329925816608 | Mean=2.28966 | Layer 7: ID=2329925817568 | Mean=2.28794 | Layer 8: ID=2329925818528 | Mean=2.28910 | Layer 9: ID=2329925819408 | Mean=2.28720 | Layer 10: ID=2329925820368 | Mean=2.28508 | Layer 11: ID=2329925821328 | Mean=2.27090 | Layer 12: ID=2329925822288 | Mean=2.26998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.07it/s, Train_acc=19.7, Train_loss=3.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.30142', '2.31566', '2.30612', '2.29783', '2.29718', '2.29525', '2.29362', '2.29456', '2.29035', '2.29254', '2.29215', '2.27742', '2.27782'] | Gamma1 Grad: ['0.02010', '-0.01584', '0.00732', '-0.00592', '0.00057', '-0.00173', '-0.00359', '0.00631', '-0.00354', '-0.00451', '0.00100', '0.00979', '0.01989']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 19.664% (Updated)\n",
      "ğŸ“Š Train Accuracy: 19.664% | ğŸ† Best Train Accuracy: 19.664%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 19.664% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.75it/s, Test_acc=22.3, Test_loss=2.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 22.340% | ğŸ† Best Test Accuracy: 22.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.94it/s, Train_acc=24, Train_loss=2.91]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.31396', '2.32951', '2.31623', '2.30199', '2.30410', '2.29932', '2.29610', '2.29563', '2.29588', '2.29123', '2.28973', '2.28172', '2.28559'] | Gamma1 Grad: ['-0.00715', '0.03196', '0.02884', '-0.00434', '-0.00035', '0.00328', '0.01173', '0.00527', '-0.00270', '0.01462', '0.00813', '-0.01765', '-0.01875']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 23.988% (Updated)\n",
      "ğŸ“Š Train Accuracy: 23.988% | ğŸ† Best Train Accuracy: 23.988%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 23.988% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.95it/s, Test_acc=27.1, Test_loss=2.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 27.050% | ğŸ† Best Test Accuracy: 27.050%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.15it/s, Train_acc=27.7, Train_loss=2.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.31912', '2.31693', '2.30322', '2.29344', '2.30172', '2.29663', '2.29033', '2.29164', '2.29101', '2.29411', '2.29270', '2.28287', '2.28815'] | Gamma1 Grad: ['-0.02130', '-0.01486', '0.00909', '-0.01530', '-0.00769', '0.00322', '0.00263', '-0.00912', '0.00154', '0.00471', '0.00523', '0.01145', '0.00806']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 27.738% (Updated)\n",
      "ğŸ“Š Train Accuracy: 27.738% | ğŸ† Best Train Accuracy: 27.738%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 27.738% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.24it/s, Test_acc=28.8, Test_loss=2.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 28.790% | ğŸ† Best Test Accuracy: 28.790%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.29it/s, Train_acc=31.8, Train_loss=2.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.31611', '2.30953', '2.30621', '2.30120', '2.29471', '2.29868', '2.29516', '2.29585', '2.29454', '2.29309', '2.29558', '2.28610', '2.28851'] | Gamma1 Grad: ['-0.00238', '0.00565', '-0.00667', '0.00570', '0.00136', '0.00472', '0.00199', '-0.00137', '-0.00201', '-0.00452', '0.00170', '-0.02368', '0.10247']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 31.802% (Updated)\n",
      "ğŸ“Š Train Accuracy: 31.802% | ğŸ† Best Train Accuracy: 31.802%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 31.802% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.88it/s, Test_acc=32.7, Test_loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 32.680% | ğŸ† Best Test Accuracy: 32.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.43it/s, Train_acc=35.6, Train_loss=2.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.31906', '2.32422', '2.32015', '2.29657', '2.29969', '2.30031', '2.29222', '2.29704', '2.29778', '2.29779', '2.29597', '2.28510', '2.29539'] | Gamma1 Grad: ['-0.00784', '-0.01541', '0.00626', '0.02285', '0.00338', '-0.01002', '0.00616', '-0.00156', '0.01095', '0.00003', '-0.00092', '0.02153', '0.07887']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 35.616% (Updated)\n",
      "ğŸ“Š Train Accuracy: 35.616% | ğŸ† Best Train Accuracy: 35.616%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 35.616% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.02it/s, Test_acc=36.3, Test_loss=2.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 36.320% | ğŸ† Best Test Accuracy: 36.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.14it/s, Train_acc=38.9, Train_loss=2.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.31945', '2.31567', '2.31731', '2.29417', '2.30094', '2.29687', '2.28601', '2.29714', '2.29911', '2.29887', '2.29669', '2.27922', '2.29607'] | Gamma1 Grad: ['-0.01222', '-0.01300', '-0.01587', '-0.02295', '0.00700', '-0.00674', '0.00743', '0.00600', '-0.00267', '-0.01260', '-0.00180', '0.00868', '-0.00895']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 38.878% (Updated)\n",
      "ğŸ“Š Train Accuracy: 38.878% | ğŸ† Best Train Accuracy: 38.878%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 38.878% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.69it/s, Test_acc=42, Test_loss=2.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 42.010% | ğŸ† Best Test Accuracy: 42.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.20it/s, Train_acc=42.6, Train_loss=2.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.31593', '2.31946', '2.31958', '2.29723', '2.30439', '2.30470', '2.29356', '2.30038', '2.29461', '2.29507', '2.29851', '2.28756', '2.29678'] | Gamma1 Grad: ['-0.02378', '-0.03103', '0.00215', '-0.01518', '0.00255', '0.00728', '0.00429', '-0.00024', '0.00662', '-0.00005', '0.00565', '0.00593', '0.00921']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 42.584% (Updated)\n",
      "ğŸ“Š Train Accuracy: 42.584% | ğŸ† Best Train Accuracy: 42.584%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 42.584% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.54it/s, Test_acc=41.9, Test_loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 41.910% | ğŸ† Best Test Accuracy: 42.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.11it/s, Train_acc=45.4, Train_loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.31190', '2.31094', '2.32117', '2.29791', '2.30358', '2.30844', '2.28715', '2.29200', '2.29662', '2.29234', '2.29288', '2.28071', '2.30138'] | Gamma1 Grad: ['-0.00967', '0.01198', '0.01322', '-0.00865', '-0.00627', '-0.01812', '0.00522', '-0.00039', '0.00516', '0.00798', '-0.00181', '-0.00470', '0.05069']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 45.368% (Updated)\n",
      "ğŸ“Š Train Accuracy: 45.368% | ğŸ† Best Train Accuracy: 45.368%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 45.368% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.87it/s, Test_acc=46.3, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 46.340% | ğŸ† Best Test Accuracy: 46.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.88it/s, Train_acc=48, Train_loss=1.87]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.30581', '2.31613', '2.31299', '2.29396', '2.31024', '2.30826', '2.28510', '2.29722', '2.28825', '2.28890', '2.29048', '2.27453', '2.29948'] | Gamma1 Grad: ['-0.00411', '0.01445', '-0.00104', '0.00955', '0.01228', '-0.01129', '-0.00113', '-0.00042', '-0.00452', '0.00152', '-0.00302', '0.00117', '0.04521']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 47.972% (Updated)\n",
      "ğŸ“Š Train Accuracy: 47.972% | ğŸ† Best Train Accuracy: 47.972%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 47.972% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.37it/s, Test_acc=49.2, Test_loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 49.210% | ğŸ† Best Test Accuracy: 49.210%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.93it/s, Train_acc=50.4, Train_loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.32401', '2.33179', '2.30877', '2.29862', '2.30028', '2.31004', '2.29433', '2.31388', '2.30652', '2.29924', '2.28384', '2.29501', '2.30425'] | Gamma1 Grad: ['0.01317', '0.04420', '-0.05501', '0.03276', '-0.02279', '-0.00831', '0.00486', '-0.00015', '0.00730', '0.00692', '0.00479', '0.02136', '0.04560']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 50.362% (Updated)\n",
      "ğŸ“Š Train Accuracy: 50.362% | ğŸ† Best Train Accuracy: 50.362%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 50.362% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.98it/s, Test_acc=50.8, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 50.750% | ğŸ† Best Test Accuracy: 50.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.47it/s, Train_acc=52.7, Train_loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.31004', '2.32952', '2.31872', '2.28208', '2.31185', '2.29832', '2.28511', '2.29961', '2.29408', '2.28563', '2.28174', '2.30272', '2.29541'] | Gamma1 Grad: ['0.02175', '-0.00278', '0.00516', '-0.05175', '0.00059', '-0.00302', '-0.01966', '-0.00989', '0.00287', '0.00153', '0.00040', '0.01602', '-0.02398']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 52.748% (Updated)\n",
      "ğŸ“Š Train Accuracy: 52.748% | ğŸ† Best Train Accuracy: 52.748%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 52.748% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.65it/s, Test_acc=50.7, Test_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 50.700% | ğŸ† Best Test Accuracy: 50.750%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.50it/s, Train_acc=54.8, Train_loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['2.30914', '2.30790', '2.30298', '2.30349', '2.30264', '2.31261', '2.29526', '2.29976', '2.30129', '2.29923', '2.29926', '2.28454', '2.29439'] | Gamma1 Grad: ['0.04509', '-0.01031', '0.02336', '-0.00153', '-0.01677', '0.01134', '-0.00453', '0.01892', '0.01282', '0.02207', '0.01272', '0.00082', '0.07534']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 54.808% (Updated)\n",
      "ğŸ“Š Train Accuracy: 54.808% | ğŸ† Best Train Accuracy: 54.808%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 54.808% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.45it/s, Test_acc=52.9, Test_loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 52.930% | ğŸ† Best Test Accuracy: 52.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.12it/s, Train_acc=56.3, Train_loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.30564', '2.31861', '2.31022', '2.30022', '2.30269', '2.31707', '2.29494', '2.30311', '2.28614', '2.28806', '2.29570', '2.28963', '2.30124'] | Gamma1 Grad: ['0.06745', '-0.00660', '0.01990', '-0.00870', '-0.00069', '0.01989', '0.01924', '0.00671', '0.00539', '0.00427', '0.00122', '0.02216', '-0.01129']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 56.290% (Updated)\n",
      "ğŸ“Š Train Accuracy: 56.290% | ğŸ† Best Train Accuracy: 56.290%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 56.290% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 90.33it/s, Test_acc=53.4, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 53.350% | ğŸ† Best Test Accuracy: 53.350%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.00it/s, Train_acc=58.3, Train_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['2.31847', '2.31544', '2.29697', '2.30197', '2.29769', '2.31385', '2.29566', '2.30607', '2.29777', '2.29105', '2.29502', '2.28436', '2.31730'] | Gamma1 Grad: ['-0.01297', '0.00964', '0.00964', '-0.01345', '-0.00278', '-0.01047', '-0.00141', '-0.00219', '-0.00052', '-0.00223', '-0.00436', '-0.00077', '0.03937']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 58.320% (Updated)\n",
      "ğŸ“Š Train Accuracy: 58.320% | ğŸ† Best Train Accuracy: 58.320%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 58.320% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.97it/s, Test_acc=54.9, Test_loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 54.930% | ğŸ† Best Test Accuracy: 54.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.34it/s, Train_acc=59.9, Train_loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.31037', '2.32956', '2.28599', '2.30087', '2.31624', '2.31329', '2.29524', '2.30096', '2.29609', '2.29888', '2.29496', '2.28913', '2.31099'] | Gamma1 Grad: ['0.01783', '-0.04290', '0.01935', '0.02963', '-0.01311', '0.00796', '-0.00879', '-0.00192', '-0.00699', '-0.00139', '-0.00142', '-0.00250', '0.03150']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 59.864% (Updated)\n",
      "ğŸ“Š Train Accuracy: 59.864% | ğŸ† Best Train Accuracy: 59.864%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 59.864% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.22it/s, Test_acc=55.6, Test_loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 55.560% | ğŸ† Best Test Accuracy: 55.560%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.75it/s, Train_acc=61.4, Train_loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['2.31032', '2.32892', '2.30398', '2.28295', '2.31196', '2.29479', '2.29541', '2.30208', '2.29956', '2.29943', '2.29464', '2.28675', '2.30517'] | Gamma1 Grad: ['0.00095', '0.03513', '-0.03057', '-0.00982', '0.00083', '-0.00720', '0.00224', '0.00244', '-0.00100', '0.00435', '0.00468', '-0.01442', '-0.02521']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 61.408% (Updated)\n",
      "ğŸ“Š Train Accuracy: 61.408% | ğŸ† Best Train Accuracy: 61.408%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 61.408% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.49it/s, Test_acc=57.4, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 57.410% | ğŸ† Best Test Accuracy: 57.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.32it/s, Train_acc=62.9, Train_loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.31049', '2.31326', '2.28951', '2.29833', '2.30943', '2.30552', '2.29914', '2.29825', '2.29199', '2.29254', '2.28868', '2.28510', '2.30239'] | Gamma1 Grad: ['0.01844', '-0.03893', '0.02050', '0.00215', '-0.04958', '-0.04229', '-0.00461', '0.00830', '0.00922', '0.01524', '0.00358', '-0.03336', '-0.01744']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 62.900% (Updated)\n",
      "ğŸ“Š Train Accuracy: 62.900% | ğŸ† Best Train Accuracy: 62.900%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 62.900% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.23it/s, Test_acc=57.2, Test_loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 57.210% | ğŸ† Best Test Accuracy: 57.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.45it/s, Train_acc=64.1, Train_loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['2.30037', '2.31205', '2.30213', '2.29877', '2.30422', '2.30516', '2.29559', '2.29973', '2.29533', '2.29436', '2.29850', '2.29097', '2.30310'] | Gamma1 Grad: ['0.02825', '0.04476', '0.06778', '-0.00691', '0.02241', '0.01790', '-0.01412', '-0.00069', '-0.00634', '0.00171', '0.00599', '0.01950', '-0.01695']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 64.092% (Updated)\n",
      "ğŸ“Š Train Accuracy: 64.092% | ğŸ† Best Train Accuracy: 64.092%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 64.092% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 91.36it/s, Test_acc=58.4, Test_loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 58.440% | ğŸ† Best Test Accuracy: 58.440%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.11it/s, Train_acc=65.6, Train_loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.31166', '2.30365', '2.30151', '2.29231', '2.30498', '2.29780', '2.29297', '2.30700', '2.29725', '2.29379', '2.29177', '2.28818', '2.30336'] | Gamma1 Grad: ['0.02584', '-0.02835', '0.04616', '-0.01460', '-0.01118', '0.02200', '0.00065', '-0.00839', '-0.01570', '-0.01098', '-0.00790', '0.01017', '0.02394']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 65.620% (Updated)\n",
      "ğŸ“Š Train Accuracy: 65.620% | ğŸ† Best Train Accuracy: 65.620%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 65.620% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.80it/s, Test_acc=59.2, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 59.240% | ğŸ† Best Test Accuracy: 59.240%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.79it/s, Train_acc=66.7, Train_loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.30211', '2.30726', '2.30204', '2.28808', '2.31075', '2.30821', '2.29629', '2.30564', '2.29625', '2.29486', '2.29474', '2.28691', '2.30840'] | Gamma1 Grad: ['-0.00633', '0.01475', '-0.09762', '-0.00574', '0.00910', '-0.02231', '0.01183', '-0.01242', '-0.00331', '-0.00061', '-0.01316', '0.00772', '0.04727']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 66.666% (Updated)\n",
      "ğŸ“Š Train Accuracy: 66.666% | ğŸ† Best Train Accuracy: 66.666%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 66.666% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.91it/s, Test_acc=60.5, Test_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.450% | ğŸ† Best Test Accuracy: 60.450%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.81it/s, Train_acc=67.8, Train_loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.30420', '2.30468', '2.30533', '2.29444', '2.30506', '2.30430', '2.29695', '2.29759', '2.30131', '2.29623', '2.29641', '2.29246', '2.30512'] | Gamma1 Grad: ['-0.02510', '0.03043', '-0.01714', '0.04024', '-0.00212', '-0.02404', '0.02546', '-0.00962', '0.00565', '0.00147', '0.00186', '0.00397', '-0.02210']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 67.752% (Updated)\n",
      "ğŸ“Š Train Accuracy: 67.752% | ğŸ† Best Train Accuracy: 67.752%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 67.752% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 78.91it/s, Test_acc=59.3, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 59.340% | ğŸ† Best Test Accuracy: 60.450%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.12it/s, Train_acc=69.1, Train_loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['2.31103', '2.31088', '2.30369', '2.30801', '2.30529', '2.30251', '2.29268', '2.29987', '2.29262', '2.28783', '2.28688', '2.29439', '2.30652'] | Gamma1 Grad: ['-0.05887', '-0.01094', '0.00660', '-0.02780', '-0.02295', '-0.01470', '-0.00120', '0.00144', '-0.00401', '0.01066', '0.00203', '-0.00090', '-0.03916']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 69.116% (Updated)\n",
      "ğŸ“Š Train Accuracy: 69.116% | ğŸ† Best Train Accuracy: 69.116%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 69.116% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.17it/s, Test_acc=59.9, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 59.930% | ğŸ† Best Test Accuracy: 60.450%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.12it/s, Train_acc=70.1, Train_loss=1.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.30619', '2.31288', '2.31550', '2.30280', '2.30522', '2.32320', '2.30702', '2.30778', '2.30328', '2.29067', '2.28958', '2.28458', '2.29510'] | Gamma1 Grad: ['-0.02997', '-0.05745', '0.04713', '-0.02904', '-0.06871', '0.02420', '-0.01388', '-0.00333', '-0.00009', '-0.00318', '-0.00721', '0.01732', '-0.00159']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 70.148% (Updated)\n",
      "ğŸ“Š Train Accuracy: 70.148% | ğŸ† Best Train Accuracy: 70.148%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 70.148% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.12it/s, Test_acc=60.9, Test_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.870% | ğŸ† Best Test Accuracy: 60.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.04it/s, Train_acc=71.4, Train_loss=0.978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['2.29980', '2.30539', '2.29408', '2.30093', '2.28873', '2.31132', '2.30126', '2.31227', '2.29830', '2.29575', '2.29603', '2.29509', '2.29895'] | Gamma1 Grad: ['0.03326', '-0.00073', '-0.00284', '-0.00221', '-0.05428', '0.01585', '0.00727', '0.00091', '-0.01219', '0.01133', '0.01576', '0.00395', '0.05335']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 71.434% (Updated)\n",
      "ğŸ“Š Train Accuracy: 71.434% | ğŸ† Best Train Accuracy: 71.434%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 71.434% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.12it/s, Test_acc=61.1, Test_loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.150% | ğŸ† Best Test Accuracy: 61.150%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.84it/s, Train_acc=72, Train_loss=0.942]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.30225', '2.31114', '2.30496', '2.30849', '2.31341', '2.31424', '2.29846', '2.30955', '2.29880', '2.28700', '2.29377', '2.28857', '2.31441'] | Gamma1 Grad: ['0.01762', '-0.01127', '0.02500', '-0.03139', '-0.01136', '-0.01503', '0.01458', '0.00787', '0.01495', '0.00391', '-0.00074', '0.03163', '0.00202']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 72.036% (Updated)\n",
      "ğŸ“Š Train Accuracy: 72.036% | ğŸ† Best Train Accuracy: 72.036%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 72.036% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.22it/s, Test_acc=61.1, Test_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 61.120% | ğŸ† Best Test Accuracy: 61.150%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.51it/s, Train_acc=73.5, Train_loss=0.901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['2.29399', '2.30939', '2.29871', '2.29137', '2.30815', '2.30572', '2.29890', '2.30738', '2.29868', '2.29348', '2.29563', '2.28682', '2.30013'] | Gamma1 Grad: ['0.01718', '0.04184', '-0.01097', '0.04712', '0.00798', '-0.03138', '-0.00123', '-0.00932', '-0.00205', '-0.00969', '0.00961', '-0.01879', '0.06159']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 73.504% (Updated)\n",
      "ğŸ“Š Train Accuracy: 73.504% | ğŸ† Best Train Accuracy: 73.504%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 73.504% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.00it/s, Test_acc=61.4, Test_loss=1.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.430% | ğŸ† Best Test Accuracy: 61.430%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.96it/s, Train_acc=74.4, Train_loss=0.867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.30053', '2.30639', '2.30145', '2.30143', '2.29704', '2.30827', '2.30422', '2.31090', '2.29751', '2.29049', '2.29120', '2.28593', '2.30441'] | Gamma1 Grad: ['-0.01115', '0.00721', '-0.05039', '0.00081', '0.02189', '-0.00127', '-0.00800', '0.00167', '-0.00108', '-0.01838', '-0.00371', '0.03136', '-0.04273']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 74.418% (Updated)\n",
      "ğŸ“Š Train Accuracy: 74.418% | ğŸ† Best Train Accuracy: 74.418%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 74.418% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.53it/s, Test_acc=60.7, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 60.720% | ğŸ† Best Test Accuracy: 61.430%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.76it/s, Train_acc=75.5, Train_loss=0.829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.30787', '2.30620', '2.31357', '2.30855', '2.30167', '2.30574', '2.30234', '2.31594', '2.29918', '2.29436', '2.29986', '2.28933', '2.30054'] | Gamma1 Grad: ['0.03543', '0.02835', '-0.00693', '0.00629', '-0.00009', '-0.00480', '-0.01227', '-0.00535', '0.00508', '0.01494', '0.01862', '-0.00722', '-0.06868']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 75.500% (Updated)\n",
      "ğŸ“Š Train Accuracy: 75.500% | ğŸ† Best Train Accuracy: 75.500%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 75.500% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.35it/s, Test_acc=61.9, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.870% | ğŸ† Best Test Accuracy: 61.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.24it/s, Train_acc=76.5, Train_loss=0.796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.30563', '2.32110', '2.29620', '2.31170', '2.32114', '2.29578', '2.29463', '2.30017', '2.30099', '2.29373', '2.29658', '2.28690', '2.30512'] | Gamma1 Grad: ['-0.01921', '-0.01659', '0.00871', '-0.04182', '0.00728', '0.00099', '-0.01096', '0.00318', '-0.01110', '-0.00031', '-0.00601', '-0.00687', '-0.04899']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 76.450% (Updated)\n",
      "ğŸ“Š Train Accuracy: 76.450% | ğŸ† Best Train Accuracy: 76.450%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 76.450% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.73it/s, Test_acc=61.8, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 61.800% | ğŸ† Best Test Accuracy: 61.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.49it/s, Train_acc=77.1, Train_loss=0.771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.30116', '2.30638', '2.31348', '2.28157', '2.29849', '2.29889', '2.29647', '2.30649', '2.30909', '2.29734', '2.30143', '2.29431', '2.30533'] | Gamma1 Grad: ['-0.02952', '0.00581', '-0.02508', '0.00063', '-0.01702', '0.00286', '-0.02662', '-0.00173', '-0.01450', '-0.00526', '-0.00049', '-0.01027', '0.01532']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 77.074% (Updated)\n",
      "ğŸ“Š Train Accuracy: 77.074% | ğŸ† Best Train Accuracy: 77.074%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 77.074% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.39it/s, Test_acc=62.6, Test_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.600% | ğŸ† Best Test Accuracy: 62.600%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.01it/s, Train_acc=77.9, Train_loss=0.739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.29684', '2.30900', '2.29522', '2.29152', '2.29214', '2.29255', '2.28540', '2.29955', '2.30722', '2.29928', '2.30120', '2.28447', '2.29603'] | Gamma1 Grad: ['0.00435', '-0.02679', '0.01786', '-0.02411', '0.02450', '-0.00993', '-0.01590', '0.00811', '-0.01688', '-0.00088', '0.00763', '0.00373', '0.03015']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 77.908% (Updated)\n",
      "ğŸ“Š Train Accuracy: 77.908% | ğŸ† Best Train Accuracy: 77.908%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 77.908% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.20it/s, Test_acc=62.6, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.650% | ğŸ† Best Test Accuracy: 62.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 23.00it/s, Train_acc=78.8, Train_loss=0.712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00144 | Gamma1: ['2.30346', '2.30798', '2.30574', '2.29176', '2.31671', '2.31965', '2.31154', '2.31481', '2.28413', '2.29299', '2.29218', '2.28023', '2.30122'] | Gamma1 Grad: ['0.02114', '-0.03370', '0.06468', '0.00472', '0.04061', '0.04749', '0.02221', '-0.01823', '0.00328', '-0.00817', '0.00797', '-0.01596', '0.01042']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 78.766% (Updated)\n",
      "ğŸ“Š Train Accuracy: 78.766% | ğŸ† Best Train Accuracy: 78.766%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 78.766% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.11it/s, Test_acc=62.8, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 62.840% | ğŸ† Best Test Accuracy: 62.840%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.98it/s, Train_acc=79.5, Train_loss=0.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.29638', '2.29516', '2.31436', '2.30472', '2.29736', '2.31207', '2.28910', '2.31738', '2.30263', '2.30244', '2.30044', '2.29578', '2.29071'] | Gamma1 Grad: ['0.01429', '-0.00684', '0.03460', '-0.00764', '0.07634', '0.01890', '-0.00724', '0.00722', '-0.01386', '-0.00519', '-0.00602', '-0.03155', '0.03015']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 79.488% (Updated)\n",
      "ğŸ“Š Train Accuracy: 79.488% | ğŸ† Best Train Accuracy: 79.488%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 79.488% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.13it/s, Test_acc=63.7, Test_loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.680% | ğŸ† Best Test Accuracy: 63.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.03it/s, Train_acc=80.2, Train_loss=0.659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00139 | Gamma1: ['2.29286', '2.29701', '2.29723', '2.28168', '2.27767', '2.32766', '2.29768', '2.29706', '2.30966', '2.28996', '2.29128', '2.29787', '2.31462'] | Gamma1 Grad: ['0.09329', '-0.00571', '-0.06483', '-0.01963', '0.04862', '0.02063', '-0.02712', '-0.00979', '-0.00873', '0.00123', '0.00064', '0.00862', '0.01211']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 80.200% (Updated)\n",
      "ğŸ“Š Train Accuracy: 80.200% | ğŸ† Best Train Accuracy: 80.200%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 80.200% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.16it/s, Test_acc=63, Test_loss=1.53]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.000% | ğŸ† Best Test Accuracy: 63.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.40it/s, Train_acc=80.8, Train_loss=0.639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.30101', '2.29198', '2.29097', '2.30455', '2.30658', '2.31612', '2.30655', '2.30876', '2.29968', '2.29188', '2.28613', '2.28475', '2.30401'] | Gamma1 Grad: ['0.01968', '0.02063', '-0.03940', '0.08943', '0.00307', '-0.07245', '0.01661', '-0.01017', '-0.01626', '-0.00643', '-0.00875', '0.01506', '0.07420']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 80.750% (Updated)\n",
      "ğŸ“Š Train Accuracy: 80.750% | ğŸ† Best Train Accuracy: 80.750%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 80.750% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.16it/s, Test_acc=63.1, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.140% | ğŸ† Best Test Accuracy: 63.680%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.21it/s, Train_acc=81.9, Train_loss=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00133 | Gamma1: ['2.30842', '2.30202', '2.31289', '2.28890', '2.29348', '2.30453', '2.29817', '2.31163', '2.29049', '2.29596', '2.30102', '2.28868', '2.30029'] | Gamma1 Grad: ['-0.01481', '0.04445', '0.03337', '-0.00448', '0.08127', '0.01119', '0.00341', '0.01130', '-0.00389', '0.00800', '-0.00556', '-0.02109', '0.02605']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 81.942% (Updated)\n",
      "ğŸ“Š Train Accuracy: 81.942% | ğŸ† Best Train Accuracy: 81.942%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 81.942% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.18it/s, Test_acc=63.8, Test_loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 63.780% | ğŸ† Best Test Accuracy: 63.780%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.10it/s, Train_acc=82.3, Train_loss=0.59] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00129 | Gamma1: ['2.32785', '2.36822', '2.31433', '2.28462', '2.31788', '2.33005', '2.35005', '2.35369', '2.32211', '2.28662', '2.30023', '2.27841', '2.32194'] | Gamma1 Grad: ['0.00032', '0.01784', '-0.06867', '-0.03452', '0.03407', '0.00536', '-0.00858', '-0.01249', '-0.00228', '0.00971', '0.00581', '-0.03398', '-0.03253']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.316% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.316% | ğŸ† Best Train Accuracy: 82.316%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.316% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.19it/s, Test_acc=62.9, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.940% | ğŸ† Best Test Accuracy: 63.780%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.24it/s, Train_acc=82.8, Train_loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00125 | Gamma1: ['2.34143', '2.33211', '2.33722', '2.28635', '2.32009', '2.33485', '2.33890', '2.35634', '2.32191', '2.29827', '2.29917', '2.28185', '2.31432'] | Gamma1 Grad: ['-0.01402', '0.02721', '0.03543', '0.04717', '-0.15081', '0.03516', '-0.00721', '-0.03300', '0.02834', '0.00754', '-0.00015', '0.01789', '-0.01119']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.790% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.790% | ğŸ† Best Train Accuracy: 82.790%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.790% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.69it/s, Test_acc=64, Test_loss=1.52]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.010% | ğŸ† Best Test Accuracy: 64.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.18it/s, Train_acc=83.4, Train_loss=0.551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00120 | Gamma1: ['2.32835', '2.33760', '2.33140', '2.28733', '2.29475', '2.34208', '2.36179', '2.35933', '2.33239', '2.30886', '2.29830', '2.28068', '2.33531'] | Gamma1 Grad: ['0.03216', '0.03165', '0.01330', '0.01951', '0.00737', '-0.01866', '-0.00582', '0.02822', '-0.00638', '-0.01089', '0.00210', '-0.05803', '0.00725']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.432% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.432% | ğŸ† Best Train Accuracy: 83.432%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.432% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.81it/s, Test_acc=62.9, Test_loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 62.930% | ğŸ† Best Test Accuracy: 64.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.48it/s, Train_acc=84.1, Train_loss=0.529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00115 | Gamma1: ['2.32324', '2.35429', '2.32464', '2.27976', '2.31951', '2.33318', '2.36206', '2.35228', '2.33344', '2.30167', '2.31275', '2.27694', '2.33379'] | Gamma1 Grad: ['0.00835', '0.01879', '0.05146', '0.01075', '0.13559', '0.01229', '-0.01995', '0.02683', '-0.02680', '0.00476', '-0.00351', '0.00234', '0.01268']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 84.080% (Updated)\n",
      "ğŸ“Š Train Accuracy: 84.080% | ğŸ† Best Train Accuracy: 84.080%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 84.080% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.99it/s, Test_acc=64.1, Test_loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.110% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.96it/s, Train_acc=85.1, Train_loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00110 | Gamma1: ['2.34228', '2.32350', '2.34506', '2.30113', '2.31506', '2.33301', '2.35903', '2.36016', '2.34560', '2.30351', '2.30616', '2.28567', '2.31515'] | Gamma1 Grad: ['0.07265', '-0.08264', '0.01510', '0.03667', '-0.09264', '-0.02275', '-0.03931', '0.04520', '0.01565', '0.01342', '0.00915', '-0.01865', '-0.06844']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.072% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.072% | ğŸ† Best Train Accuracy: 85.072%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.072% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.21it/s, Test_acc=63.5, Test_loss=1.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.460% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.38it/s, Train_acc=85.2, Train_loss=0.494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00105 | Gamma1: ['2.33963', '2.32586', '2.33009', '2.33215', '2.27518', '2.36003', '2.36726', '2.36048', '2.32935', '2.31248', '2.31396', '2.28825', '2.32400'] | Gamma1 Grad: ['0.03402', '-0.06075', '0.03854', '-0.02643', '0.01486', '-0.02041', '0.01935', '-0.05366', '0.02361', '0.00272', '-0.01101', '0.00831', '0.03021']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.226% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.226% | ğŸ† Best Train Accuracy: 85.226%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.226% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.97it/s, Test_acc=63.9, Test_loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.920% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.75it/s, Train_acc=86, Train_loss=0.471]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00100 | Gamma1: ['2.33753', '2.34579', '2.31915', '2.29751', '2.29250', '2.32082', '2.35770', '2.36222', '2.32687', '2.31010', '2.30820', '2.30021', '2.31168'] | Gamma1 Grad: ['-0.05758', '-0.01232', '0.03409', '0.02060', '-0.05752', '0.01996', '-0.02736', '0.01137', '-0.01410', '0.00081', '0.00924', '-0.00918', '0.01042']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.992% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.992% | ğŸ† Best Train Accuracy: 85.992%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.992% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.12it/s, Test_acc=64, Test_loss=1.62]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.030% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 24.05it/s, Train_acc=86.2, Train_loss=0.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00094 | Gamma1: ['2.32553', '2.33847', '2.29180', '2.28050', '2.29037', '2.32413', '2.36038', '2.35702', '2.33670', '2.30453', '2.30507', '2.27260', '2.30436'] | Gamma1 Grad: ['-0.06606', '0.01653', '0.03017', '-0.05834', '-0.03008', '0.00904', '-0.01204', '0.01683', '-0.00205', '0.00052', '0.00378', '0.02822', '-0.00414']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.244% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.244% | ğŸ† Best Train Accuracy: 86.244%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.244% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.68it/s, Test_acc=64.1, Test_loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.060% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:32<00:00, 23.74it/s, Train_acc=86.4, Train_loss=0.451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00089 | Gamma1: ['2.32138', '2.33499', '2.30376', '2.26477', '2.26786', '2.33342', '2.36116', '2.37256', '2.33531', '2.30154', '2.29788', '2.28516', '2.31655'] | Gamma1 Grad: ['-0.05498', '-0.02548', '0.02084', '-0.00101', '0.06548', '0.03397', '-0.02062', '-0.01467', '0.02277', '0.00347', '0.01572', '-0.10981', '0.11022']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.440% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.440% | ğŸ† Best Train Accuracy: 86.440%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.440% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.03it/s, Test_acc=63.9, Test_loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.910% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.74it/s, Train_acc=87.1, Train_loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00083 | Gamma1: ['2.29691', '2.32208', '2.30020', '2.27401', '2.27825', '2.32554', '2.34648', '2.37954', '2.34359', '2.30782', '2.30356', '2.29514', '2.30897'] | Gamma1 Grad: ['-0.01040', '-0.06467', '-0.04596', '0.03871', '-0.05584', '0.01186', '0.00643', '0.02114', '0.02310', '0.00209', '0.01064', '0.00412', '0.00670']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.122% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.122% | ğŸ† Best Train Accuracy: 87.122%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.122% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.95it/s, Test_acc=63.3, Test_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.310% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:33<00:00, 23.31it/s, Train_acc=87.6, Train_loss=0.413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.31461', '2.32789', '2.31209', '2.28250', '2.28071', '2.31760', '2.34165', '2.35604', '2.32118', '2.30860', '2.30018', '2.30198', '2.30937'] | Gamma1 Grad: ['0.02545', '-0.02947', '-0.02771', '0.08208', '-0.04242', '0.05901', '-0.03606', '-0.03510', '0.00545', '-0.00028', '0.01006', '0.00831', '-0.05324']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.636% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.636% | ğŸ† Best Train Accuracy: 87.636%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.636% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.10it/s, Test_acc=64.1, Test_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.080% | ğŸ† Best Test Accuracy: 64.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 51 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.84it/s, Train_acc=87.9, Train_loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00072 | Gamma1: ['2.32528', '2.32765', '2.29718', '2.29858', '2.27523', '2.32600', '2.35145', '2.35429', '2.30684', '2.30837', '2.29568', '2.28802', '2.30706'] | Gamma1 Grad: ['-0.02944', '-0.04619', '0.02991', '0.00544', '0.00561', '0.01652', '-0.00115', '-0.00724', '0.01772', '-0.01248', '-0.00109', '0.00242', '-0.04217']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.924% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.924% | ğŸ† Best Train Accuracy: 87.924%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.924% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.63it/s, Test_acc=64.3, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.290% | ğŸ† Best Test Accuracy: 64.290%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 52 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.80it/s, Train_acc=88.2, Train_loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00066 | Gamma1: ['2.29547', '2.33227', '2.29612', '2.29858', '2.26872', '2.32218', '2.33890', '2.34768', '2.30743', '2.29401', '2.29925', '2.28243', '2.30012'] | Gamma1 Grad: ['-0.04654', '-0.02884', '-0.00551', '0.02103', '-0.00137', '-0.06803', '0.03314', '0.02942', '0.00326', '-0.00186', '-0.00830', '0.02237', '-0.00507']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.218% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.218% | ğŸ† Best Train Accuracy: 88.218%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.218% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.77it/s, Test_acc=64.3, Test_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.340% | ğŸ† Best Test Accuracy: 64.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 53 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.85it/s, Train_acc=88.6, Train_loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00061 | Gamma1: ['2.31563', '2.31213', '2.29820', '2.27888', '2.28257', '2.29775', '2.32731', '2.34370', '2.32228', '2.29093', '2.29986', '2.28570', '2.29821'] | Gamma1 Grad: ['0.00967', '-0.00705', '0.07466', '0.03526', '-0.01816', '-0.05257', '-0.01000', '0.01475', '0.01047', '0.00218', '-0.00109', '0.00402', '-0.01872']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.628% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.628% | ğŸ† Best Train Accuracy: 88.628%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.628% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 89.19it/s, Test_acc=64.1, Test_loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.080% | ğŸ† Best Test Accuracy: 64.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 54 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000), 8 (Î”=0.00000), 9 (Î”=0.00000), 10 (Î”=0.00000), 11 (Î”=0.00000), 12 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.43it/s, Train_acc=88.9, Train_loss=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00055 | Gamma1: ['2.31478', '2.32263', '2.29383', '2.27864', '2.25816', '2.31492', '2.30510', '2.34633', '2.32910', '2.30425', '2.31279', '2.28426', '2.31472'] | Gamma1 Grad: ['-0.04439', '-0.02351', '-0.05651', '0.00736', '0.03957', '-0.00451', '-0.04526', '0.04126', '-0.01064', '-0.01835', '0.00452', '-0.02594', '-0.00257']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.894% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.894% | ğŸ† Best Train Accuracy: 88.894%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.894% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.94it/s, Test_acc=64.1, Test_loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.120% | ğŸ† Best Test Accuracy: 64.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.78it/s, Train_acc=89.3, Train_loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00050 | Gamma1: ['2.31296', '2.32707', '2.29677', '2.29740', '2.25149', '2.31012', '2.35351', '2.34709', '2.32554', '2.30515', '2.30252', '2.26535', '2.30420'] | Gamma1 Grad: ['-0.03696', '-0.03130', '-0.00597', '0.05385', '0.04312', '0.00790', '0.02873', '0.03851', '-0.00927', '-0.01010', '-0.00685', '0.01297', '-0.01909']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.264% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.264% | ğŸ† Best Train Accuracy: 89.264%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.264% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.15it/s, Test_acc=63.7, Test_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.660% | ğŸ† Best Test Accuracy: 64.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.40it/s, Train_acc=89.8, Train_loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.31285', '2.30412', '2.31250', '2.28713', '2.26550', '2.31127', '2.35611', '2.35393', '2.32310', '2.30857', '2.31426', '2.27560', '2.28543'] | Gamma1 Grad: ['-0.09405', '-0.03274', '0.08151', '-0.07966', '0.00524', '0.01687', '0.02325', '-0.01894', '-0.01544', '-0.01194', '-0.00544', '0.01456', '0.10055']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.826% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.826% | ğŸ† Best Train Accuracy: 89.826%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.826% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.85it/s, Test_acc=63.8, Test_loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.760% | ğŸ† Best Test Accuracy: 64.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.47it/s, Train_acc=90.1, Train_loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00040 | Gamma1: ['2.32443', '2.33553', '2.30014', '2.28290', '2.26061', '2.28827', '2.35069', '2.35413', '2.32698', '2.30863', '2.29486', '2.27292', '2.29284'] | Gamma1 Grad: ['-0.01856', '0.01420', '0.00015', '-0.01968', '0.01031', '0.03365', '-0.00011', '0.00285', '-0.00982', '-0.00172', '0.00875', '0.00164', '0.03846']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.060% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.060% | ğŸ† Best Train Accuracy: 90.060%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.060% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.71it/s, Test_acc=64, Test_loss=1.77]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.040% | ğŸ† Best Test Accuracy: 64.340%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.71it/s, Train_acc=90.6, Train_loss=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00035 | Gamma1: ['2.33162', '2.34255', '2.30107', '2.29059', '2.28237', '2.28305', '2.33416', '2.33324', '2.32520', '2.30347', '2.28717', '2.28510', '2.29839'] | Gamma1 Grad: ['-0.00292', '0.02273', '-0.06514', '0.00987', '0.01777', '0.00648', '-0.11245', '0.02641', '0.02141', '0.00046', '0.00664', '-0.00739', '0.02823']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.590% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.590% | ğŸ† Best Train Accuracy: 90.590%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.590% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.03it/s, Test_acc=64.7, Test_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.650% | ğŸ† Best Test Accuracy: 64.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.75it/s, Train_acc=90.3, Train_loss=0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00030 | Gamma1: ['2.32838', '2.35562', '2.29529', '2.29046', '2.25591', '2.27903', '2.33377', '2.34901', '2.33382', '2.29247', '2.29085', '2.28686', '2.29575'] | Gamma1 Grad: ['-0.02334', '-0.03091', '-0.09029', '0.07499', '0.03064', '-0.07770', '-0.07133', '0.05078', '-0.00297', '0.00577', '-0.00742', '-0.00505', '0.01391']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 90.304% | ğŸ† Best Train Accuracy: 90.590%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.590% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.89it/s, Test_acc=65.4, Test_loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 65.390% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.63it/s, Train_acc=91, Train_loss=0.31]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00026 | Gamma1: ['2.32977', '2.36302', '2.29957', '2.27576', '2.28722', '2.29960', '2.35034', '2.36248', '2.35268', '2.29399', '2.28931', '2.27989', '2.27809'] | Gamma1 Grad: ['-0.06798', '-0.03339', '0.02229', '-0.04651', '-0.06672', '-0.03595', '-0.00269', '-0.01168', '-0.00301', '-0.00106', '-0.00091', '0.00321', '0.06017']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.008% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.008% | ğŸ† Best Train Accuracy: 91.008%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.008% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.65it/s, Test_acc=65, Test_loss=1.75]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.960% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.20it/s, Train_acc=91, Train_loss=0.304]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00022 | Gamma1: ['2.33338', '2.35686', '2.29556', '2.27392', '2.24966', '2.28770', '2.33705', '2.36734', '2.36706', '2.30324', '2.29438', '2.27749', '2.28001'] | Gamma1 Grad: ['-0.00088', '-0.00735', '0.02427', '0.00317', '-0.03482', '-0.02586', '-0.03008', '0.01460', '-0.00738', '0.01513', '0.00085', '-0.01773', '-0.02205']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 90.978% | ğŸ† Best Train Accuracy: 91.008%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.008% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.09it/s, Test_acc=63.9, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.930% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.82it/s, Train_acc=91.1, Train_loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.32363', '2.35478', '2.31044', '2.27556', '2.26337', '2.29105', '2.34086', '2.36833', '2.35390', '2.29264', '2.28723', '2.27884', '2.28138'] | Gamma1 Grad: ['-0.01681', '-0.02977', '-0.03146', '0.05971', '-0.09280', '0.03539', '-0.06153', '-0.05444', '0.01730', '-0.00910', '-0.00103', '0.00688', '-0.03345']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.066% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.066% | ğŸ† Best Train Accuracy: 91.066%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.066% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.21it/s, Test_acc=64.4, Test_loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.400% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.99it/s, Train_acc=91.5, Train_loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00016 | Gamma1: ['2.32619', '2.36293', '2.29083', '2.28556', '2.23874', '2.28911', '2.35190', '2.39095', '2.35101', '2.30064', '2.28575', '2.27779', '2.27525'] | Gamma1 Grad: ['-0.00712', '0.04316', '-0.04248', '-0.02763', '0.02632', '-0.02027', '-0.01346', '-0.04087', '0.00546', '-0.00339', '-0.00680', '0.02050', '0.00106']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.490% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.490% | ğŸ† Best Train Accuracy: 91.490%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.490% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.30it/s, Test_acc=65, Test_loss=1.81]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.020% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.57it/s, Train_acc=91.9, Train_loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00013 | Gamma1: ['2.30500', '2.34044', '2.28898', '2.25254', '2.24541', '2.29479', '2.36859', '2.36134', '2.34000', '2.31249', '2.28933', '2.28113', '2.28382'] | Gamma1 Grad: ['0.00972', '-0.00838', '-0.06277', '0.07191', '-0.06269', '0.00050', '0.04332', '0.00615', '0.00319', '-0.00133', '0.00074', '-0.01065', '-0.01218']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.920% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.920% | ğŸ† Best Train Accuracy: 91.920%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.920% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 78.93it/s, Test_acc=64, Test_loss=1.86]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.040% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 21.05it/s, Train_acc=91.7, Train_loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00011 | Gamma1: ['2.33090', '2.35324', '2.30244', '2.26946', '2.26001', '2.27953', '2.36641', '2.38007', '2.33736', '2.30473', '2.29026', '2.27764', '2.28912'] | Gamma1 Grad: ['-0.05222', '-0.04169', '0.04358', '-0.03559', '0.18422', '-0.01040', '0.01587', '0.02117', '-0.01048', '0.00453', '-0.00994', '0.01000', '0.00006']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 91.736% | ğŸ† Best Train Accuracy: 91.920%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.920% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.52it/s, Test_acc=64.2, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.250% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.45it/s, Train_acc=92.1, Train_loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.34200', '2.34329', '2.29970', '2.25878', '2.24636', '2.28955', '2.37364', '2.37356', '2.33367', '2.31029', '2.28749', '2.28145', '2.28964'] | Gamma1 Grad: ['0.07740', '0.04556', '-0.02306', '-0.00198', '-0.02823', '-0.01777', '-0.00628', '0.00287', '-0.00082', '0.00998', '0.00417', '-0.00920', '0.01066']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.132% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.132% | ğŸ† Best Train Accuracy: 92.132%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.132% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.49it/s, Test_acc=64.2, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.180% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.64it/s, Train_acc=92.3, Train_loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.32705', '2.33019', '2.31568', '2.25707', '2.25553', '2.28540', '2.34513', '2.36784', '2.34549', '2.30561', '2.29947', '2.27362', '2.27725'] | Gamma1 Grad: ['-0.00964', '0.01899', '0.02495', '0.04628', '0.03809', '0.00570', '0.01818', '-0.01477', '-0.01419', '-0.03288', '-0.00578', '0.07156', '0.03974']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.274% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.274% | ğŸ† Best Train Accuracy: 92.274%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.274% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.45it/s, Test_acc=64.8, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.840% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:37<00:00, 21.12it/s, Train_acc=92.7, Train_loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00006 | Gamma1: ['2.32709', '2.35894', '2.32287', '2.24284', '2.24981', '2.28553', '2.35296', '2.35879', '2.33154', '2.30777', '2.30261', '2.26805', '2.29578'] | Gamma1 Grad: ['0.01786', '0.01157', '0.01040', '-0.01725', '0.02633', '-0.00837', '0.02010', '-0.00997', '0.00723', '-0.00999', '-0.01520', '0.01874', '-0.00353']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.652% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.652% | ğŸ† Best Train Accuracy: 92.652%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.652% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.81it/s, Test_acc=64.7, Test_loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.730% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.14it/s, Train_acc=92.7, Train_loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['2.33338', '2.35255', '2.31085', '2.26273', '2.23459', '2.26745', '2.35820', '2.34057', '2.34142', '2.31753', '2.30384', '2.27321', '2.29437'] | Gamma1 Grad: ['0.01480', '0.00783', '-0.00105', '-0.01918', '0.04099', '-0.01903', '0.01475', '0.01735', '-0.00680', '0.00185', '0.00614', '0.00590', '0.00108']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.670% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.670% | ğŸ† Best Train Accuracy: 92.670%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.670% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.45it/s, Test_acc=65, Test_loss=1.8]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.050% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.73it/s, Train_acc=93, Train_loss=0.249]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.32520', '2.34043', '2.31059', '2.25487', '2.25077', '2.29033', '2.36642', '2.35316', '2.35228', '2.31151', '2.30395', '2.28343', '2.29155'] | Gamma1 Grad: ['-0.03174', '0.05513', '0.07368', '-0.09294', '0.05093', '0.05266', '-0.05250', '0.04494', '-0.03416', '-0.00725', '0.00012', '-0.01906', '0.02179']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.988% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.988% | ğŸ† Best Train Accuracy: 92.988%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.988% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.15it/s, Test_acc=65.2, Test_loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.240% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.85it/s, Train_acc=93, Train_loss=0.243]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.30038', '2.34673', '2.31822', '2.22509', '2.26279', '2.29643', '2.36332', '2.37763', '2.36082', '2.28051', '2.29412', '2.29241', '2.29935'] | Gamma1 Grad: ['-0.00179', '0.02014', '-0.02891', '-0.01788', '0.01228', '-0.00039', '0.00359', '0.00626', '-0.00003', '-0.00303', '-0.00724', '0.00191', '0.00610']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.990% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.990% | ğŸ† Best Train Accuracy: 92.990%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.990% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.04it/s, Test_acc=64.9, Test_loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.920% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.07it/s, Train_acc=93.2, Train_loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.31583', '2.34275', '2.30028', '2.24519', '2.24729', '2.29793', '2.37627', '2.35476', '2.31728', '2.30918', '2.29078', '2.26055', '2.29796'] | Gamma1 Grad: ['0.01579', '-0.03371', '-0.00638', '-0.01807', '0.01543', '-0.03250', '0.01788', '-0.02137', '-0.00561', '-0.00636', '0.00907', '0.00691', '0.07082']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.208% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.208% | ğŸ† Best Train Accuracy: 93.208%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.208% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.57it/s, Test_acc=64.2, Test_loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.250% | ğŸ† Best Test Accuracy: 65.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.74it/s, Train_acc=93.6, Train_loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.32879', '2.34249', '2.30264', '2.28714', '2.19476', '2.26755', '2.36014', '2.36931', '2.33766', '2.29467', '2.28160', '2.25403', '2.28584'] | Gamma1 Grad: ['0.01019', '0.05758', '-0.08747', '0.04074', '-0.13136', '0.01196', '0.05626', '-0.04442', '-0.01093', '0.01059', '0.01498', '-0.03781', '0.03332']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.636% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.636% | ğŸ† Best Train Accuracy: 93.636%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.636% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.51it/s, Test_acc=65.4, Test_loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 65.410% | ğŸ† Best Test Accuracy: 65.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.18it/s, Train_acc=93.3, Train_loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.33571', '2.33973', '2.27592', '2.28234', '2.23059', '2.24804', '2.35569', '2.37431', '2.33837', '2.28764', '2.28349', '2.25111', '2.28449'] | Gamma1 Grad: ['-0.00549', '0.01870', '0.00515', '-0.01438', '0.00435', '-0.00411', '0.00621', '0.02403', '-0.00841', '0.00429', '0.01115', '0.02039', '0.00305']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.330% | ğŸ† Best Train Accuracy: 93.636%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.636% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.68it/s, Test_acc=64.7, Test_loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.660% | ğŸ† Best Test Accuracy: 65.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.39it/s, Train_acc=93.6, Train_loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.33159', '2.34696', '2.31900', '2.30910', '2.21572', '2.26618', '2.35883', '2.34824', '2.34757', '2.29008', '2.29907', '2.28336', '2.28078'] | Gamma1 Grad: ['0.01342', '-0.01290', '-0.03780', '0.14875', '-0.14856', '0.08869', '-0.01371', '0.05324', '0.00838', '0.02340', '0.01484', '-0.02309', '-0.01613']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.644% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.644% | ğŸ† Best Train Accuracy: 93.644%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.644% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.23it/s, Test_acc=65.6, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 65.610% | ğŸ† Best Test Accuracy: 65.610%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.77it/s, Train_acc=93.8, Train_loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.32005', '2.34593', '2.26830', '2.27467', '2.21068', '2.26395', '2.37936', '2.37468', '2.34257', '2.31079', '2.31745', '2.27972', '2.28474'] | Gamma1 Grad: ['-0.01208', '0.00306', '0.02977', '0.01131', '0.11136', '-0.00502', '0.02156', '-0.01848', '0.00956', '0.00623', '-0.00558', '-0.03157', '0.03810']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.762% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.762% | ğŸ† Best Train Accuracy: 93.762%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.762% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.64it/s, Test_acc=65.2, Test_loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.210% | ğŸ† Best Test Accuracy: 65.610%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.55it/s, Train_acc=94, Train_loss=0.218]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00147 | Gamma1: ['2.31799', '2.33687', '2.26384', '2.24733', '2.23074', '2.24903', '2.32611', '2.35778', '2.33419', '2.29598', '2.32649', '2.28592', '2.26695'] | Gamma1 Grad: ['0.01389', '0.01429', '0.01851', '0.00474', '-0.04290', '-0.02214', '0.01024', '0.00757', '-0.00188', '-0.00000', '-0.00148', '0.00362', '-0.02136']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.998% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.998% | ğŸ† Best Train Accuracy: 93.998%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.998% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.99it/s, Test_acc=65.1, Test_loss=1.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.120% | ğŸ† Best Test Accuracy: 65.610%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.68it/s, Train_acc=93.9, Train_loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.32746', '2.32674', '2.26859', '2.20880', '2.21491', '2.25928', '2.32975', '2.37778', '2.34135', '2.28730', '2.28165', '2.27158', '2.27436'] | Gamma1 Grad: ['-0.00177', '-0.04151', '0.03572', '-0.10065', '0.04598', '0.02918', '-0.02210', '-0.04470', '-0.00054', '0.00743', '-0.00213', '0.02750', '0.02166']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.908% | ğŸ† Best Train Accuracy: 93.998%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.998% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 88.11it/s, Test_acc=65.7, Test_loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 65.660% | ğŸ† Best Test Accuracy: 65.660%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.26it/s, Train_acc=94.1, Train_loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00146 | Gamma1: ['2.31734', '2.34048', '2.28638', '2.24084', '2.24340', '2.25824', '2.33822', '2.35224', '2.32129', '2.30911', '2.28814', '2.30114', '2.26401'] | Gamma1 Grad: ['-0.00576', '0.02015', '-0.01787', '-0.03665', '0.12376', '-0.04281', '0.05933', '-0.02179', '-0.01159', '-0.00279', '-0.01174', '-0.01002', '-0.01007']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.080% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.080% | ğŸ† Best Train Accuracy: 94.080%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.080% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.14it/s, Test_acc=64.9, Test_loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.910% | ğŸ† Best Test Accuracy: 65.660%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.27it/s, Train_acc=96.9, Train_loss=0.127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00144 | Gamma1: ['2.29770', '2.35972', '2.29096', '2.28272', '2.20835', '2.27749', '2.34224', '2.32246', '2.32608', '2.31931', '2.30229', '2.28545', '2.29725'] | Gamma1 Grad: ['-0.00720', '0.00845', '0.02535', '-0.02387', '-0.00221', '-0.01147', '0.01595', '0.04287', '-0.00309', '-0.00134', '0.00620', '0.00228', '0.01259']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 96.942% (Updated)\n",
      "ğŸ“Š Train Accuracy: 96.942% | ğŸ† Best Train Accuracy: 96.942%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 96.942% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.61it/s, Test_acc=67.5, Test_loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.550% | ğŸ† Best Test Accuracy: 67.550%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.80it/s, Train_acc=97.8, Train_loss=0.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00143 | Gamma1: ['2.31857', '2.33599', '2.29763', '2.26034', '2.21448', '2.28187', '2.31509', '2.31698', '2.32202', '2.31505', '2.30349', '2.28964', '2.30075'] | Gamma1 Grad: ['0.03726', '0.01397', '0.00951', '-0.00111', '0.13215', '0.04911', '0.00692', '0.05077', '0.00679', '0.01457', '0.02105', '0.00456', '-0.00844']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.766% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.766% | ğŸ† Best Train Accuracy: 97.766%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.766% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.99it/s, Test_acc=68.2, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 68.160% | ğŸ† Best Test Accuracy: 68.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.01it/s, Train_acc=98.1, Train_loss=0.089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00142 | Gamma1: ['2.31702', '2.33087', '2.30353', '2.28110', '2.22656', '2.27739', '2.30904', '2.32510', '2.30181', '2.31820', '2.32434', '2.27456', '2.33456'] | Gamma1 Grad: ['-0.02129', '-0.00688', '0.05276', '0.00852', '0.05146', '0.05542', '-0.06120', '0.05149', '-0.00791', '-0.00724', '0.00288', '-0.01608', '-0.03944']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.140% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.140% | ğŸ† Best Train Accuracy: 98.140%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.140% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 87.58it/s, Test_acc=68, Test_loss=1.81]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.990% | ğŸ† Best Test Accuracy: 68.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.62it/s, Train_acc=98.4, Train_loss=0.083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00141 | Gamma1: ['2.30847', '2.32361', '2.29545', '2.25723', '2.22519', '2.26497', '2.30683', '2.33027', '2.29945', '2.30474', '2.32395', '2.28073', '2.30478'] | Gamma1 Grad: ['0.02010', '0.00708', '0.06519', '0.03070', '-0.03491', '-0.01876', '-0.03146', '0.01413', '0.00381', '0.01060', '0.00434', '0.00850', '0.02810']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.372% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.372% | ğŸ† Best Train Accuracy: 98.372%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.372% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.70it/s, Test_acc=68.1, Test_loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.100% | ğŸ† Best Test Accuracy: 68.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.86it/s, Train_acc=98.5, Train_loss=0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00139 | Gamma1: ['2.30368', '2.30738', '2.28053', '2.25535', '2.24440', '2.26485', '2.31653', '2.32349', '2.31722', '2.31834', '2.32639', '2.28449', '2.34184'] | Gamma1 Grad: ['-0.00425', '-0.04041', '-0.00348', '-0.00543', '-0.03103', '0.03845', '-0.00162', '-0.01855', '-0.00852', '-0.00812', '-0.00904', '0.00717', '0.00208']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.510% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.510% | ğŸ† Best Train Accuracy: 98.510%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.510% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 82.18it/s, Test_acc=68, Test_loss=1.86]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.010% | ğŸ† Best Test Accuracy: 68.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.53it/s, Train_acc=98.7, Train_loss=0.074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00138 | Gamma1: ['2.30289', '2.30088', '2.27160', '2.25757', '2.23732', '2.25587', '2.29372', '2.30147', '2.31279', '2.30373', '2.31226', '2.31288', '2.33196'] | Gamma1 Grad: ['0.00577', '0.00102', '0.00949', '0.01161', '-0.00098', '0.01017', '-0.00850', '-0.00243', '0.00241', '-0.00220', '0.00261', '0.00306', '0.00500']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.670% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.670% | ğŸ† Best Train Accuracy: 98.670%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.670% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.03it/s, Test_acc=68.1, Test_loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.110% | ğŸ† Best Test Accuracy: 68.160%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.55it/s, Train_acc=98.7, Train_loss=0.073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00136 | Gamma1: ['2.29281', '2.32725', '2.27269', '2.26162', '2.23766', '2.27752', '2.29088', '2.30912', '2.33040', '2.30666', '2.30695', '2.31230', '2.32090'] | Gamma1 Grad: ['0.00459', '-0.01081', '0.03419', '-0.00331', '0.01408', '0.00625', '0.05024', '-0.00443', '0.00493', '0.00243', '0.00020', '0.01891', '-0.00028']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.704% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.704% | ğŸ† Best Train Accuracy: 98.704%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.704% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.34it/s, Test_acc=68.6, Test_loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_VGG16_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 68.570% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.23it/s, Train_acc=98.8, Train_loss=0.069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00134 | Gamma1: ['2.28590', '2.34294', '2.28774', '2.27775', '2.23255', '2.24050', '2.29228', '2.30283', '2.31589', '2.30941', '2.31282', '2.30636', '2.31758'] | Gamma1 Grad: ['-0.07695', '-0.13703', '0.04202', '-0.04890', '0.10327', '-0.05885', '-0.07233', '0.05940', '0.00837', '-0.00506', '0.00292', '0.02481', '0.02955']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.764% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.764% | ğŸ† Best Train Accuracy: 98.764%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.764% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 81.55it/s, Test_acc=68.1, Test_loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.140% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.94it/s, Train_acc=98.8, Train_loss=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00133 | Gamma1: ['2.29100', '2.30964', '2.27982', '2.26329', '2.23399', '2.28104', '2.30989', '2.29369', '2.31075', '2.30066', '2.31629', '2.30462', '2.32205'] | Gamma1 Grad: ['0.02915', '-0.03200', '0.08891', '0.05708', '0.05303', '0.03112', '-0.05190', '-0.02546', '-0.00439', '-0.00392', '0.00179', '0.00040', '0.04160']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.824% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.824% | ğŸ† Best Train Accuracy: 98.824%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.824% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.23it/s, Test_acc=68.3, Test_loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.330% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.78it/s, Train_acc=98.9, Train_loss=0.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00131 | Gamma1: ['2.29494', '2.32273', '2.30471', '2.26221', '2.23685', '2.28613', '2.29620', '2.34189', '2.30069', '2.30786', '2.32786', '2.30387', '2.32917'] | Gamma1 Grad: ['-0.02424', '0.05975', '-0.01694', '-0.06712', '0.00722', '-0.05446', '-0.01568', '-0.02657', '-0.00285', '0.00856', '-0.00359', '0.00622', '-0.00962']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.938% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.938% | ğŸ† Best Train Accuracy: 98.938%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.938% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.06it/s, Test_acc=68.1, Test_loss=2.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.060% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.39it/s, Train_acc=99.1, Train_loss=0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00129 | Gamma1: ['2.29001', '2.31124', '2.31417', '2.24670', '2.23417', '2.28746', '2.30437', '2.33658', '2.30639', '2.30744', '2.31934', '2.31639', '2.33557'] | Gamma1 Grad: ['-0.01932', '0.04573', '-0.06092', '0.01753', '-0.00749', '-0.01728', '0.00294', '-0.01640', '0.00064', '-0.00539', '-0.00288', '0.00596', '-0.00113']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.062% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.062% | ğŸ† Best Train Accuracy: 99.062%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.062% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 10166 | TestAccHist: 0 | TrainLossHist: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 84.84it/s, Test_acc=68.2, Test_loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.180% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.14it/s, Train_acc=98.9, Train_loss=0.063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00127 | Gamma1: ['2.28829', '2.30672', '2.29501', '2.26271', '2.25336', '2.27079', '2.29248', '2.31504', '2.31632', '2.31608', '2.30238', '2.31286', '2.33916'] | Gamma1 Grad: ['-0.00119', '0.00458', '0.01303', '-0.01157', '-0.00095', '-0.00535', '-0.00075', '0.00280', '0.00071', '-0.00208', '-0.00348', '-0.00379', '-0.00879']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.946% | ğŸ† Best Train Accuracy: 99.062%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.062% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.50it/s, Test_acc=68, Test_loss=2.06]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.040% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.30it/s, Train_acc=99.1, Train_loss=0.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00125 | Gamma1: ['2.28655', '2.30068', '2.27696', '2.25065', '2.25987', '2.27665', '2.29004', '2.33351', '2.30037', '2.29146', '2.30635', '2.31043', '2.32979'] | Gamma1 Grad: ['-0.02186', '0.01235', '-0.00104', '-0.01572', '-0.02732', '-0.00301', '-0.04555', '-0.00770', '-0.00603', '-0.00082', '0.01125', '0.03205', '-0.00031']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.090% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.090% | ğŸ† Best Train Accuracy: 99.090%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.090% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.34it/s, Test_acc=68.3, Test_loss=2.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.300% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.25it/s, Train_acc=99, Train_loss=0.06]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00122 | Gamma1: ['2.29016', '2.30787', '2.27949', '2.24037', '2.27041', '2.26940', '2.29466', '2.31536', '2.31020', '2.30406', '2.30233', '2.31251', '2.31790'] | Gamma1 Grad: ['-0.03090', '0.05578', '-0.05198', '0.04254', '0.07893', '0.03900', '-0.04415', '-0.02034', '-0.00979', '0.00745', '-0.01088', '-0.02972', '0.00966']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.988% | ğŸ† Best Train Accuracy: 99.090%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.090% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.22it/s, Test_acc=68.2, Test_loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.230% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.70it/s, Train_acc=99.1, Train_loss=0.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00120 | Gamma1: ['2.31161', '2.31016', '2.29296', '2.25417', '2.25954', '2.27970', '2.29793', '2.31260', '2.30396', '2.30307', '2.30011', '2.29863', '2.32420'] | Gamma1 Grad: ['-0.02240', '-0.01105', '-0.00340', '0.04166', '-0.01841', '0.00872', '0.00850', '-0.01203', '-0.00042', '-0.00203', '-0.00514', '0.01144', '-0.00934']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.086% | ğŸ† Best Train Accuracy: 99.090%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.090% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.17it/s, Test_acc=68.1, Test_loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.080% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 22.34it/s, Train_acc=99.1, Train_loss=0.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00118 | Gamma1: ['2.29273', '2.29875', '2.28732', '2.25771', '2.25998', '2.29208', '2.30324', '2.33376', '2.31304', '2.29683', '2.30204', '2.28906', '2.33385'] | Gamma1 Grad: ['-0.09003', '0.01883', '0.04028', '-0.05760', '-0.16130', '0.01313', '-0.04674', '0.01729', '-0.00151', '0.00540', '-0.00718', '0.01934', '-0.04785']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.110% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.110% | ğŸ† Best Train Accuracy: 99.110%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.110% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 85.05it/s, Test_acc=68.5, Test_loss=2.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.460% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:34<00:00, 22.43it/s, Train_acc=99.2, Train_loss=0.057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00115 | Gamma1: ['2.30912', '2.30633', '2.25320', '2.26011', '2.25519', '2.25782', '2.30472', '2.32519', '2.30907', '2.29656', '2.31860', '2.29444', '2.32733'] | Gamma1 Grad: ['-0.00052', '-0.00323', '0.00623', '-0.00902', '-0.00226', '-0.00200', '-0.00155', '0.00084', '-0.00105', '0.00036', '0.00047', '-0.00046', '-0.00660']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.158% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.158% | ğŸ† Best Train Accuracy: 99.158%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.158% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.05it/s, Test_acc=68.5, Test_loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.460% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.44it/s, Train_acc=99.2, Train_loss=0.057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00113 | Gamma1: ['2.31724', '2.30759', '2.28135', '2.23477', '2.22368', '2.26529', '2.30234', '2.32334', '2.30746', '2.29969', '2.30579', '2.29523', '2.31705'] | Gamma1 Grad: ['0.00497', '0.00411', '0.00002', '0.00290', '-0.00122', '-0.00203', '-0.00174', '-0.00177', '-0.00138', '-0.00282', '-0.00130', '-0.00352', '-0.00015']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.188% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.188% | ğŸ† Best Train Accuracy: 99.188%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.188% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 80.41it/s, Test_acc=68.3, Test_loss=2.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.340% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:35<00:00, 21.83it/s, Train_acc=99.2, Train_loss=0.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00110 | Gamma1: ['2.30778', '2.30148', '2.29498', '2.24262', '2.24316', '2.25494', '2.29370', '2.31775', '2.30600', '2.30221', '2.30551', '2.28980', '2.32788'] | Gamma1 Grad: ['0.00050', '0.00008', '0.00039', '-0.00037', '-0.00042', '-0.00194', '0.00066', '-0.00011', '0.00016', '0.00002', '0.00010', '-0.00031', '0.00029']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.202% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.202% | ğŸ† Best Train Accuracy: 99.202%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.202% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 83.99it/s, Test_acc=68.4, Test_loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.440% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:36<00:00, 21.41it/s, Train_acc=99.2, Train_loss=0.055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00108 | Gamma1: ['2.29823', '2.30086', '2.28992', '2.26504', '2.24489', '2.25708', '2.29362', '2.32109', '2.30482', '2.29977', '2.30650', '2.29355', '2.32101'] | Gamma1 Grad: ['0.01109', '0.00096', '0.00140', '0.00008', '0.01220', '-0.01551', '-0.00233', '0.00571', '0.00485', '-0.00150', '-0.00068', '-0.00349', '-0.00545']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.198% | ğŸ† Best Train Accuracy: 99.202%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.202% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 86.13it/s, Test_acc=68, Test_loss=2.2]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.980% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\VGG16\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_VGG16_Adam.txt!\n",
      "\n",
      "Best Test Accuracy:  68.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 11. TRAIN MODEL WITH SHEDULAR | XXX ----------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Set Seed for Reproducibility BEFORE training starts\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)  \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(2)  \n",
    "\n",
    "# âœ… Training Loop\n",
    "num_epochs = 100 # Example: Set the total number of epochs\n",
    "for epoch in range(start_epoch, num_epochs):   # Runs training for 100 epochs\n",
    "\n",
    "    train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler, WARMUP_ACTIVATION_EPOCHS) # âœ… Pass required arguments\n",
    "\n",
    "    test(epoch)  # âœ… Test the model\n",
    "    tqdm.write(\"\")  # âœ… Clear leftover progress bar from test()\n",
    "\n",
    "\n",
    "print(\"Best Test Accuracy: \", best_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
