{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf2d2b9-9afc-4900-a264-d43cef54994b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Current working directory: C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\n",
      "âœ… sys.path updated:\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\python310.zip\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\DLLs\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\n",
      "   ğŸ“‚ \n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\Pythonwin\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\models\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\activation\n",
      "âœ… FFTGate imported successfully!\n",
      "âœ… FFTGate instance created successfully!\n",
      "âœ… FFTGate_SENet imported successfully!\n",
      "CIFAR100 Training Script Initialized...\n",
      "Using device: cuda\n",
      "Parsed learning rate: 0.001 (type: <class 'float'>)\n",
      "Formatted learning rate for filenames: 0_001\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Length of training dataset: 50000\n",
      "Length of testing dataset: 10000\n",
      "Number of classes in CIFAR-100: 100\n",
      "==> Building model..\n",
      "âœ… Found 8 FFTGate layers.\n",
      "âœ… Collected 8 trainable activation parameters.\n",
      "   ğŸ”¹ Layer 0: FFTGate()\n",
      "   ğŸ”¹ Layer 1: FFTGate()\n",
      "   ğŸ”¹ Layer 2: FFTGate()\n",
      "   ğŸ”¹ Layer 3: FFTGate()\n",
      "   ğŸ”¹ Layer 4: FFTGate()\n",
      "   ğŸ”¹ Layer 5: FFTGate()\n",
      "   ğŸ”¹ Layer 6: FFTGate()\n",
      "   ğŸ”¹ Layer 7: FFTGate()\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 1.A. IMPORTS LIBRARIES | XXX -----------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\"\"\"Train CIFAR100 with PyTorch.\"\"\"\n",
    "\n",
    "# Python 2/3 compatibility\n",
    "# from __future__ import print_function\n",
    "\n",
    "\n",
    "# âœ… Standard libraries\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch and related modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# torchvision for datasets and transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch_optimizer as torch_opt  # Use 'torch_opt' for torch_optimizer\n",
    "from timm.scheduler import CosineLRScheduler \n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Define currect working directory to ensure on right directory\n",
    "SENet18_PATH = r\"C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\"\n",
    "if os.getcwd() != SENet18_PATH:\n",
    "    os.chdir(SENet18_PATH)\n",
    "print(f\"âœ… Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# âœ… Define absolute paths\n",
    "PROJECT_PATH = SENet18_PATH\n",
    "MODELS_PATH = os.path.join(SENet18_PATH, \"models\")\n",
    "ACTIVATION_PATH = os.path.join(SENet18_PATH, \"activation\")\n",
    "# PAU_PATH = os.path.join(SENet18_PATH, \"pau\")\n",
    "\n",
    "# âœ… Ensure necessary paths are in sys.path\n",
    "for path in [PROJECT_PATH, MODELS_PATH, ACTIVATION_PATH]:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "# âœ… Print updated sys.path for debugging\n",
    "print(\"âœ… sys.path updated:\")\n",
    "for path in sys.path:\n",
    "    print(\"   ğŸ“‚\", path)\n",
    "\n",
    "# âœ… Import FFTGate (Check if the module exists)\n",
    "try:\n",
    "    from activation.FFTGate import FFTGate  # type: ignore\n",
    "    print(\"âœ… FFTGate imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ Import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'FFTGate.py' exists inside: {ACTIVATION_PATH}\")\n",
    "\n",
    "# âœ… Test if FFTGate is callable\n",
    "try:\n",
    "    activation_test = FFTGate()\n",
    "    print(\"âœ… FFTGate instance created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error while initializing FFTGate: {e}\")\n",
    "\n",
    "# âœ… Now import FFTGate_SENet (Ensure module exists inside models/)\n",
    "try:\n",
    "    from models.FFTGate_SENet import FFTGate_SENet  # type: ignore\n",
    "    print(\"âœ… FFTGate_SENet imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ FFTGate_SENet import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'FFTGate_SENet.py' exists inside: {MODELS_PATH}\")\n",
    "\n",
    "# from models.FFTGate_SENet import Block  # Import the Block class explicitly!\n",
    "from models.FFTGate_SENet import PreActBlock  # Ensure the correct path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 1.B. SEEDING FOR REPRODUCIBILITY | XXX -------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def set_seed_torch(seed):\n",
    "    torch.manual_seed(seed)                          \n",
    "\n",
    "\n",
    "\n",
    "def set_seed_main(seed):\n",
    "    random.seed(seed)                                ## Python's random module\n",
    "    np.random.seed(seed)                             ## NumPy's random module\n",
    "    torch.cuda.manual_seed(seed)                     ## PyTorch's random module for CUDA\n",
    "    torch.cuda.manual_seed_all(seed)                 ## Seed for all CUDA devices\n",
    "    torch.backends.cudnn.deterministic = True        ## Ensure deterministic behavior for CuDNN\n",
    "    torch.backends.cudnn.benchmark = False           ## Disable CuDNN's autotuning for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(0)   \n",
    "\n",
    "# Fixed main seed (model, CUDA, etc.)\n",
    "set_seed_main(0)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (Optional) Import Optimizers - Uncomment as needed\n",
    "# from Opt import opt\n",
    "# from diffGrad import diffGrad\n",
    "# from diffRGrad import diffRGrad, SdiffRGrad, BetaDiffRGrad, Beta12DiffRGrad, BetaDFCDiffRGrad\n",
    "# from RADAM import Radam, BetaRadam\n",
    "# from BetaAdam import BetaAdam, BetaAdam1, BetaAdam2, BetaAdam3, BetaAdam4, BetaAdam5, BetaAdam6, BetaAdam7, BetaAdam4A\n",
    "# from AdamRM import AdamRM, AdamRM1, AdamRM2, AdamRM3, AdamRM4, AdamRM5\n",
    "# from sadam import sadam\n",
    "# from SdiffGrad import SdiffGrad\n",
    "# from SRADAM import SRADAM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 1.D. SEARCH FOR FFTGate LAYERS| XXX ----------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def find_activations(module, activation_layers, activation_params):\n",
    "    \"\"\"Recursively find FFTGate layers and collect them into activation_layers and activation_params.\"\"\"\n",
    "    for layer in module.children():\n",
    "        if isinstance(layer, FFTGate):\n",
    "            activation_layers.append(layer)  # âœ… Store the entire layer\n",
    "            activation_params.append(layer.gamma1)  # âœ… Store only gamma1 for optimization\n",
    "        elif isinstance(layer, nn.Sequential) or isinstance(layer, nn.Module):  \n",
    "            find_activations(layer, activation_layers, activation_params)  # âœ… Recursively search\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 2. DEFINE MODEL Lr | XXX ---------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Main Execution (Placeholder)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"CIFAR100 Training Script Initialized...\")\n",
    "    # Add your training pipeline here\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Argument parser to get user inputs\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR100 Training')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "\n",
    "args, unknown = parser.parse_known_args()  # Avoids Jupyter argument issues\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Ensure lr is correctly parsed\n",
    "lr = args.lr  # Get learning rate from argparse\n",
    "lr_str = str(lr).replace('.', '_')  # Convert to string and replace '.' for filenames\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Parsed learning rate: {lr} (type: {type(lr)})\")\n",
    "print(f\"Formatted learning rate for filenames: {lr_str}\")\n",
    "\n",
    "# Initialize training variables\n",
    "best_acc = 0  # Best test accuracy\n",
    "start_epoch = 0  # Start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 3. LOAD DATASET | XXX ------------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "bs = 64 #set batch size\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Length of train and test datasets\n",
    "len_train = len(trainset)\n",
    "len_test = len(testset)\n",
    "print(f\"Length of training dataset: {len_train}\")\n",
    "print(f\"Length of testing dataset: {len_test}\")\n",
    "\n",
    "# âœ… Print number of classes\n",
    "num_classes = len(trainset.classes)\n",
    "print(f\"Number of classes in CIFAR-100: {num_classes}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 4. DYNAMIC REGULARIZATION| XXX ---------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def apply_dynamic_regularization(inputs, feature_activations, epoch,\n",
    "                                  prev_params, layer_index_map, batch_idx):\n",
    "\n",
    "\n",
    "    global activation_layers  # âœ… MUST be declared first, before using activation_layers | # Uses collected layers\n",
    "\n",
    "    if batch_idx == 0 and epoch <= 4:\n",
    "        print(f\"\\nğŸš¨ ENTERED apply_dynamic_regularization | Epoch={epoch} | Batch={batch_idx}\", flush=True)\n",
    "\n",
    "        # ğŸ§  Print all gamma1 stats in one line (once per batch)\n",
    "        all_layer_info = []\n",
    "        for idx, layer in enumerate(activation_layers):\n",
    "            param = getattr(layer, \"gamma1\")\n",
    "            all_layer_info.append(f\"Layer {idx}: ID={id(param)} | Mean={param.mean().item():.5f}\")\n",
    "        print(\"ğŸ§  GAMMA1 INFO:\", \" | \".join(all_layer_info), flush=True)\n",
    "\n",
    "    # âœ… Initialize gamma1 regularization accumulator\n",
    "    gamma1_reg = 0.0\n",
    "\n",
    "    # âœ… Compute batch std and define regularization strength\n",
    "    batch_std = torch.std(inputs) + 1e-6\n",
    "    regularization_strength = 0.05 if epoch < 40 else (0.01 if epoch < 60 else 0.005)\n",
    "\n",
    "\n",
    "    # âœ… Track layers where noise is injected (informative)\n",
    "    noisy_layers = []\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        if idx not in layer_index_map:\n",
    "            continue\n",
    "\n",
    "        prev_layer_params = prev_params[layer_index_map[idx]]\n",
    "        param_name = \"gamma1\"  # âœ… Only gamma1 is trainable\n",
    "        param = getattr(layer, param_name)\n",
    "        prev_param = prev_layer_params[param_name]\n",
    "\n",
    "\n",
    "        # âœ… Target based on input stats\n",
    "        target = compute_target(param_name, batch_std)\n",
    "\n",
    "        # âœ… Adaptive Target Regularization\n",
    "        gamma1_reg += regularization_strength * (param - target).pow(2).mean() * 1.2\n",
    "\n",
    "        # âœ… Adaptive Cohesion Regularization \n",
    "        cohesion = (param - prev_param).pow(2)  \n",
    "        gamma1_reg += 0.005 * cohesion.mean()  \n",
    "\n",
    "\n",
    "        # âœ… Adaptive Noise Regularization\n",
    "        epoch_AddNoise = 50\n",
    "        if epoch > epoch_AddNoise:\n",
    "            param_variation = torch.abs(param - prev_param).mean()\n",
    "            if param_variation < 0.015:  \n",
    "                noise = (0.001 + 0.0004 * batch_std.item()) * torch.randn_like(param)\n",
    "                penalty = (param - (prev_param + noise)).pow(2).sum()\n",
    "                gamma1_reg += 0.00015 * penalty                  \n",
    "                noisy_layers.append(f\"{idx} (Î”={param_variation.item():.5f})\") # Collect index and variation\n",
    "\n",
    "    # âœ… Print noise summary for first few epochs\n",
    "    if batch_idx == 0 and epoch <= (epoch_AddNoise+4) and noisy_layers:\n",
    "        print(f\"ğŸ”¥ Stable Noise Injected | Epoch {epoch} | Batch {batch_idx} | Layers: \" + \", \".join(noisy_layers), flush=True)\n",
    "    mags = feature_activations.abs().mean(dim=(0, 2, 3))\n",
    "    m = mags / mags.sum()\n",
    "    gamma1_reg += 0.005 * (-(m * torch.log(m + 1e-6)).sum())\n",
    "\n",
    "    return gamma1_reg\n",
    "\n",
    "\n",
    "def compute_target(param_name, batch_std):\n",
    "    if param_name == \"gamma1\":\n",
    "        return 2.0 + 0.2 * batch_std.item()    \n",
    "    \n",
    "    raise ValueError(f\"Unknown param {param_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 5. INITIALIZE MODEL | XXX --------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "#net = Elliott_VGG('VGG16'); net1 = 'Elliott_VGG16'\n",
    "#net = GELU_MobileNet(); net1 = 'GELU_MobileNet'\n",
    "#net = GELU_SENet18(); net1 = 'GELU_SENet18'\n",
    "#net = PDELU_ResNet50(); net1 = 'PDELU_ResNet50'\n",
    "# net = Sigmoid_GoogLeNet(); net1 = 'Sigmoid_GoogLeNet'\n",
    "#net = GELU_DenseNet121(); net1 = 'GELU_DenseNet121'\n",
    "# net = ReLU_VGG('VGG16'); net1 = 'ReLU_VGG16'\n",
    "# net = MY_VGG4('VGG16'); net1 = 'MY_VGG16'\n",
    "# net = MY_MobileNet4(num_classes=10); net1 = 'MY_MobileNet4'\n",
    "# net = MY_SENet4(num_classes=10); net1 = 'MY_SENet4'\n",
    "net = FFTGate_SENet(PreActBlock, [2, 2, 2, 2], num_classes=100); net1 = 'FFTGate_SENet'\n",
    "\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9); optimizer1 = 'SGDM5'\n",
    "#optimizer = optim.Adagrad(net.parameters()); optimizer1 = 'AdaGrad'\n",
    "#optimizer = optim.Adadelta(net.parameters()); optimizer1 = 'AdaDelta'\n",
    "#optimizer = optim.RMSprop(net.parameters()); optimizer1 = 'RMSprop'\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr); optimizer1 = 'Adam'\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr, amsgrad=True); optimizer1 = 'amsgrad'\n",
    "#optimizer = diffGrad(net.parameters(), lr=args.lr); optimizer1 = 'diffGrad'\n",
    "#optimizer = Radam(net.parameters(), lr=args.lr); optimizer1 = 'Radam'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 6. INITIALIZE ACTIVATION PARAMETERS, OPTIMIZERS & SCHEDULERS | XXX ---------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "# âœ… Step 1: Collect Activation Parameters from ALL Layers (Ensure Compatibility with DataParallel)\n",
    "if isinstance(net, torch.nn.DataParallel):\n",
    "    model_layers = [\n",
    "        net.module.conv1, net.module.bn1, \n",
    "        *net.module.layer1, *net.module.layer2, \n",
    "        *net.module.layer3, *net.module.layer4\n",
    "    ]\n",
    "else:\n",
    "    model_layers = [\n",
    "        net.conv1, net.bn1, \n",
    "        *net.layer1, *net.layer2, \n",
    "        *net.layer3, *net.layer4\n",
    "    ]\n",
    "\n",
    "\n",
    "# âœ… Step 2: Recursively search for FFTGate layers\n",
    "activation_params = []\n",
    "activation_layers = []  # âœ… Define an empty list to store FFTGate layers\n",
    "\n",
    "\n",
    "# ğŸ” Correctly populate activation_layers and activation_params\n",
    "for layer in model_layers:\n",
    "    find_activations(layer, activation_layers, activation_params)\n",
    "\n",
    "\n",
    "# âœ… Step 3: Print collected activation layers and parameters\n",
    "if activation_layers and activation_params:\n",
    "    print(f\"âœ… Found {len(activation_layers)} FFTGate layers.\")\n",
    "    print(f\"âœ… Collected {len(activation_params)} trainable activation parameters.\")\n",
    "    \n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        print(f\"   ğŸ”¹ Layer {idx}: {layer}\")\n",
    "\n",
    "elif activation_layers and not activation_params:\n",
    "    print(f\"âš  Warning: Found {len(activation_layers)} FFTGate layers, but no trainable parameters were collected.\")\n",
    "\n",
    "elif activation_params and not activation_layers:\n",
    "    print(f\"âš  Warning: Collected {len(activation_params)} activation parameters, but no FFTGate layers were recorded.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš  Warning: No FFTGate layers or activation parameters found! Skipping activation optimizer.\")\n",
    "    activation_optimizers = None  # Prevents crash\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 4: Define Unfreeze Epoch\n",
    "unfreeze_activation_epoch = 1  # âœ… Change this value if needed changed from 1 to 0 temporary\n",
    "# unfreeze_activation_epoch = 10  # âœ… Delay unfreezing until epoch 10\n",
    "\n",
    "\n",
    "# âœ…Step 5: Define the warm-up epoch value\n",
    "# WARMUP_ACTIVATION_EPOCHS = 5  # The number of epochs for warm-up\n",
    "WARMUP_ACTIVATION_EPOCHS = 0  # The number of epochs for warm-up\n",
    "\n",
    "\n",
    "# âœ… Step 6: Initially Freeze Activation Parameters\n",
    "for param in activation_params:\n",
    "    param.requires_grad = False  # ğŸš« Keep frozen before the unfreeze epoch\n",
    "\n",
    "\n",
    "# âœ… Step 7: Initialize Activation Optimizers (Using AdamW for Better Weight Decay)\n",
    "activation_optimizers = {\n",
    "    \"gamma1\": torch.optim.AdamW(activation_params, lr=0.0015, weight_decay=1e-6)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 8: Initialize Activation Schedulers with Warm Restarts (Per Parameter Type)\n",
    "activation_schedulers = {\n",
    "    \"gamma1\": CosineAnnealingWarmRestarts(\n",
    "        activation_optimizers[\"gamma1\"],\n",
    "        T_0=10,\n",
    "        T_mult=2,\n",
    "        eta_min=1e-5  # âœ… recommended safer modification\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 7. INITIALIZE MAIN OPTIMIZER SCHEDULER | XXX -------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Step 6: Define MultiStepLR for Main Optimizer\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "main_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 8. MODEL CHECK POINT | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists('checkpoint'):\n",
    "    os.makedirs('checkpoint')\n",
    "\n",
    "if not os.path.exists('Results'):\n",
    "    os.makedirs('Results')\n",
    "\n",
    "# Construct checkpoint path\n",
    "checkpoint_path = f'./checkpoint/CIFAR10_B{bs}_LR{lr}_{net1}_{optimizer1}.t7'\n",
    "\n",
    "# Resume checkpoint only if file exists\n",
    "if args.resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 9. DEFINE TRAIN LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Training\n",
    "\n",
    "def train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler , WARMUP_ACTIVATION_EPOCHS):\n",
    "    global train_loss_history, best_train_acc, prev_params, recent_test_acc, gamma1_history, activation_layers, test_acc_history  # ğŸŸ¢ğŸŸ¢ğŸŸ¢\n",
    "\n",
    "    if epoch == 0:\n",
    "        train_loss_history = []\n",
    "        best_train_acc = 0.0\n",
    "        recent_test_acc = 0.0\n",
    "        gamma1_history = {}        # âœ… Initialize history \n",
    "        test_acc_history = []      # âœ… test accuracy history\n",
    "\n",
    "\n",
    "    prev_params = {}\n",
    "    layer_index_map = {idx: idx for idx in range(len(activation_layers))}\n",
    "\n",
    "    # âœ… Cache previous gamma1 values\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        prev_params[idx] = {\"gamma1\": layer.gamma1.clone().detach()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_accuracy = 0.0\n",
    "\n",
    "    # âœ… Initialize log history\n",
    "    log_history = []\n",
    "\n",
    "    # âœ… Define path to store Training log\n",
    "    save_paths = {\n",
    "       \n",
    "        \"log_history\": f\"C:\\\\Users\\\\emeka\\\\Research\\\\ModelCUDA\\\\Big_Data_Journal\\\\Comparison\\\\Code\\\\Paper\\\\github2\\\\CIFAR100\\\\SENet18\\\\Results\\\\FFTGate\\\\FFTGate_training_logs.txt\"  # âœ… Training log_history \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 1: Unfreeze Activation Parameters (Only Once Per Epoch)\n",
    "    if epoch == unfreeze_activation_epoch:\n",
    "        print(\"\\nğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\")\n",
    "        for layer in activation_layers:  \n",
    "            layer.gamma1.requires_grad = True  # âœ… Only gamma1 is trainable\n",
    "        print(\"âœ… Activation Parameters Unfrozen! ğŸš€\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 2: Gradual Warm-up for Activation Learning Rates (AFTER Unfreezing)\n",
    "    warmup_start = unfreeze_activation_epoch  # ğŸ”¹ Start warm-up when unfreezing happens\n",
    "    warmup_end = unfreeze_activation_epoch + WARMUP_ACTIVATION_EPOCHS  # ğŸ”¹ End warm-up period\n",
    "\n",
    "    # âœ… Adjust learning rates **only** during the warm-up phase\n",
    "    if warmup_start <= epoch < warmup_end:\n",
    "        warmup_factor = (epoch - warmup_start + 1) / WARMUP_ACTIVATION_EPOCHS  \n",
    "\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            for param_group in act_scheduler.optimizer.param_groups:\n",
    "                if \"initial_lr\" not in param_group:\n",
    "                    param_group[\"initial_lr\"] = param_group[\"lr\"]  # ğŸ”¹ Store initial LR\n",
    "                param_group[\"lr\"] = param_group[\"initial_lr\"] * warmup_factor  # ğŸ”¹ Scale LR\n",
    "\n",
    "        # âœ… Debugging output to track warm-up process\n",
    "        print(f\"ğŸ”¥ Warm-up Epoch {epoch}: Scaling LR by {warmup_factor:.3f}\")\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            print(f\"  ğŸ”¹ {name} LR: {act_scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    activation_history = []  # ğŸ”´ Initialize empty history at start of epoch (outside batch loop)\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Training Loop\n",
    "    with tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch}\") as progress:\n",
    "        for batch_idx, (inputs, targets) in progress:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # zero_grad activation parameter\n",
    "            for opt in activation_optimizers.values():\n",
    "                opt.zero_grad()\n",
    "\n",
    "\n",
    "            # âœ… Forward Pass\n",
    "            # outputs = net(inputs, epoch=epoch, train_accuracy=train_accuracy, targets=targets)\n",
    "            outputs = net(inputs, epoch=epoch)  \n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "\n",
    "            # Feature Extraction | Collect feature activations directly from the network\n",
    "            x = inputs.to(device)\n",
    "            x = net.module.conv1(x) if isinstance(net, torch.nn.DataParallel) else net.conv1(x)\n",
    "            x = net.module.bn1(x) if isinstance(net, torch.nn.DataParallel) else net.bn1(x)\n",
    "            x = net.module.activation(x, epoch=epoch) if isinstance(net, torch.nn.DataParallel) else net.activation(x, epoch=epoch)\n",
    "\n",
    "\n",
    "            features = (\n",
    "                [net.module.layer1, net.module.layer2, net.module.layer3, net.module.layer4]\n",
    "                if isinstance(net, torch.nn.DataParallel)\n",
    "                else [net.layer1, net.layer2, net.layer3, net.layer4]\n",
    "            )\n",
    "            for group in features:\n",
    "                for block in group:\n",
    "                    x = block(x, epoch=epoch) if isinstance(block, PreActBlock) else block(x)\n",
    "\n",
    "\n",
    "            feature_activations = x  # Ensures gradients flow properly\n",
    "\n",
    "            # âœ… Collect Activation History | âœ… Per-layer mean activations\n",
    "            batch_means = [layer.saved_output.mean().item() for layer in activation_layers]\n",
    "            activation_history.extend(batch_means)  \n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Apply Decay strategy to history for each activation layer\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    if isinstance(layer, FFTGate):\n",
    "                        layer.decay_spectral_history(epoch, num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Compute Training Accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            train_accuracy = 100. * correct / total if total > 0 else 0.0  # Compute training accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Call Regularization Function for the Activation Parameter\n",
    "            if epoch > 0:\n",
    "                gamma1_reg = apply_dynamic_regularization(\n",
    "                    inputs, feature_activations, epoch,\n",
    "                    prev_params, layer_index_map, batch_idx\n",
    "                )\n",
    "                loss += gamma1_reg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "       \n",
    "\n",
    "            # âœ… ğŸ¯ Adaptive Gradient Clipping of gamma1 \n",
    "            for layer in activation_layers:\n",
    "                torch.nn.utils.clip_grad_norm_([layer.gamma1], max_norm=0.7) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Apply Optimizer Step for Model Parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # âœ… Apply Optimizer Steps for Activation Parameters (Only if Unfrozen)\n",
    "            if epoch >= unfreeze_activation_epoch:\n",
    "                for opt in activation_optimizers.values():\n",
    "                    opt.step()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Clamping of gamma1 (Applied AFTER Optimizer Step)\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    layer.gamma1.clamp_(0.1, 6.0)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Update progress bar\n",
    "            progress.set_postfix(Train_loss=round(train_loss / (batch_idx + 1), 3),\n",
    "                                 Train_acc=train_accuracy)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step the main optimizer scheduler (ONLY for model parameters)\n",
    "    main_scheduler.step()\n",
    "\n",
    "    # âœ… Step the activation parameter schedulers (ONLY for activation parameters) | Epoch-wise stepping\n",
    "    if epoch >= unfreeze_activation_epoch:\n",
    "        for name, act_scheduler in activation_schedulers.items():  \n",
    "            act_scheduler.step()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Ensure Activation Learning Rate doesn't drop too low (Prevent vanishing updates)\n",
    "    for opt in activation_optimizers.values():\n",
    "        for group in opt.param_groups:\n",
    "            group['lr'] = max(group['lr'], 1e-5)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    # âœ… ONLY update prev_params here AFTER all updates | âœ… Update prev_params AFTER training epoch\n",
    "    for idx, layer in enumerate(features):\n",
    "        if isinstance(layer, FFTGate): \n",
    "            prev_params[idx] = {\n",
    "                \"gamma1\": layer.gamma1.clone().detach()  \n",
    "            }\n",
    " \n",
    "\n",
    "\n",
    "    # âœ… Logging Parameters & Gradients\n",
    "    last_batch_grads = {\"Gamma1 Grad\": []}\n",
    "    current_params = {\"Gamma1\": []}\n",
    "\n",
    "    # âœ… Iterate over the previously collected activation layers\n",
    "    for idx, layer in enumerate(activation_layers):  # Use activation_layers instead of features\n",
    "        if layer.gamma1.grad is not None:\n",
    "            grad_value = f\"{layer.gamma1.grad.item():.5f}\"\n",
    "        else:\n",
    "            grad_value = \"None\"\n",
    "\n",
    "        param_value = f\"{layer.gamma1.item():.5f}\"\n",
    "\n",
    "        last_batch_grads[\"Gamma1 Grad\"].append(grad_value)\n",
    "        current_params[\"Gamma1\"].append(param_value)\n",
    "\n",
    "    # âœ… Build log message (showing params and gradients for ALL layers, cleaned and rounded)\n",
    "    log_msg = (\n",
    "        f\"Epoch {epoch}: M_Optimizer LR => {optimizer.param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1 LR => {activation_optimizers['gamma1'].param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1: {current_params['Gamma1']} | \"\n",
    "        f\"Gamma1 Grad: {last_batch_grads['Gamma1 Grad']}\"\n",
    "    )\n",
    "\n",
    "    log_history.append(log_msg)\n",
    "    print(log_msg)  # âœ… Prints only once per epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (Clear old logs)\n",
    "    if epoch == 0:  # âœ… Only clear at the start of training\n",
    "        with open(save_paths[\"log_history\"], \"w\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Save logs once per epoch (Append new logs)\n",
    "    if log_history:\n",
    "        with open(save_paths[\"log_history\"], \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_history) + \"\\n\")         # âœ… Ensure each entry is on a new line\n",
    "        print(f\"ğŸ“œ Logs saved to {save_paths['log_history']}!\")  # âœ… Only prints once per epoch\n",
    "    else:\n",
    "        print(\"âš  No logs to save!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final training accuracy for the epoch\n",
    "    final_train_loss = train_loss / len(trainloader)\n",
    "    final_train_acc = 100. * correct / total\n",
    "\n",
    "    # Append to history\n",
    "    train_loss_history.append(final_train_loss)\n",
    "    test_acc_history.append(final_train_acc)  # Track test/train accuracy across epochs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save training results (without affecting best accuracy tracking)\n",
    "    train_results_path = f'./Results/CIFAR100_Train_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "    # âœ… Clear the log file at the start of training (Epoch 0)\n",
    "    if epoch == 0 and os.path.exists(train_results_path):\n",
    "        with open(train_results_path, 'w') as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs only once\n",
    "\n",
    "    # âœ… Append new training results for each epoch\n",
    "    with open(train_results_path, 'a') as f:\n",
    "        f.write(f\"Epoch {epoch} | Train Loss: {final_train_loss:.3f} | Train Acc: {final_train_acc:.3f}%\\n\")\n",
    "\n",
    "    if final_train_acc > best_train_acc:\n",
    "        best_train_acc = final_train_acc  # âœ… Update best training accuracy\n",
    "        print(f\"ğŸ† New Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "    # âœ… Append the best training accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):  # Only log once at the final epoch\n",
    "        with open(train_results_path, 'a') as f:\n",
    "            f.write(f\"\\nğŸ† Best Training Accuracy: {best_train_acc:.3f}%\\n\")  \n",
    "\n",
    "    # âœ… Print both Final and Best Training Accuracy\n",
    "    print(f\"ğŸ“Š Train Accuracy: {final_train_acc:.3f}% | ğŸ† Best Train Accuracy: {best_train_acc:.3f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"ğŸ“œ Training logs saved to {train_results_path}!\")\n",
    "    print(f\"ğŸ† Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"ğŸ“ Sizes â†’ ActivationHist: {len(activation_history)} | TestAccHist: {len(test_acc_history)} | TrainLossHist: {len(train_loss_history)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # return final_train_loss, final_train_acc, feature_activations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 10. DEFINE TEST LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(epoch, save_results=True):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and optionally saves the results.\n",
    "    \n",
    "    Args:\n",
    "    - epoch (int): The current epoch number.\n",
    "    - save_results (bool): Whether to save results to a file.\n",
    "\n",
    "    Returns:\n",
    "    - acc (float): Test accuracy percentage.\n",
    "    \"\"\"\n",
    "    global best_acc, val_accuracy \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # âœ… Ensure activation function parameters are clamped before evaluation\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(testloader), total=len(testloader), desc=f\"Testing Epoch {epoch}\") as progress:\n",
    "            for batch_idx, (inputs, targets) in progress:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                # outputs = net(inputs, epoch=0)  # Pass epoch=0 to fix TypeError!\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                # âœ… Pass validation accuracy to activation function\n",
    "                val_accuracy = 100. * correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "                # âœ… Update progress bar with loss & accuracy\n",
    "                progress.set_postfix(Test_loss=round(test_loss / (batch_idx + 1), 3),\n",
    "                                     Test_acc=round(val_accuracy, 3))\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final test accuracy\n",
    "    final_test_loss = test_loss / len(testloader)\n",
    "    final_test_acc = 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Ensure \"Results\" folder exists (just like training logs)\n",
    "    results_dir = os.path.join(PROJECT_PATH, \"Results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # âœ… Define log file path for test results\n",
    "    test_results_path = os.path.join(results_dir, f'CIFAR100_Test_B{bs}_LR{lr}_{net1}_{optimizer1}.txt')\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (clear old logs)\n",
    "    if epoch == 0:\n",
    "        with open(test_results_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Append new test results for each epoch (same style as training)\n",
    "    with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Epoch {epoch} | Test Loss: {final_test_loss:.3f} | Test Acc: {final_test_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save checkpoint if accuracy improves (does NOT interfere with logging)\n",
    "    if final_test_acc > best_acc:\n",
    "        print('ğŸ† Saving best model...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': final_test_acc,  # âœ… Ensures the best test accuracy is saved in checkpoint\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Ensure checkpoint directory exists\n",
    "        checkpoint_dir = \"checkpoint\"\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "        # âœ… Format learning rate properly before saving filename\n",
    "        lr_str = str(lr).replace('.', '_')\n",
    "        checkpoint_path = f'./checkpoint/CIFAR100_B{bs}_LR{lr_str}_{net1}_{optimizer1}.t7'\n",
    "        torch.save(state, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "        best_acc = final_test_acc  # âœ… Update best accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Append the best test accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):\n",
    "        with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\nğŸ† Best Test Accuracy: {best_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Print both Final and Best Test Accuracy (always executed)\n",
    "    print(f\"ğŸ“Š Test Accuracy: {final_test_acc:.3f}% | ğŸ† Best Test Accuracy: {best_acc:.3f}%\")\n",
    "    print(f\"ğŸ“œ Test logs saved to {test_results_path}!\")\n",
    "\n",
    "\n",
    "    global recent_test_acc\n",
    "    recent_test_acc = final_test_acc  # Capture latest test accuracy for next train() call | Store latest test accuracy\n",
    "\n",
    "    return final_test_acc  # âœ… Return the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9719a6-5565-4265-9d11-40c31ca8e7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:38<00:00, 20.45it/s, Train_acc=11.6, Train_loss=3.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000'] | Gamma1 Grad: ['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 11.624% (Updated)\n",
      "ğŸ“Š Train Accuracy: 11.624% | ğŸ† Best Train Accuracy: 11.624%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 11.624% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 6256 | TestAccHist: 1 | TrainLossHist: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.45it/s, Test_acc=20.2, Test_loss=3.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 20.190% | ğŸ† Best Test Accuracy: 20.190%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n",
      "\n",
      "ğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\n",
      "âœ… Activation Parameters Unfrozen! ğŸš€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=1 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2408987072352 | Mean=1.50000 | Layer 1: ID=2408955071600 | Mean=1.50000 | Layer 2: ID=2408955069040 | Mean=1.50000 | Layer 3: ID=2409452503744 | Mean=1.50000 | Layer 4: ID=2409452505904 | Mean=1.50000 | Layer 5: ID=2409452507904 | Mean=1.50000 | Layer 6: ID=2409452509904 | Mean=1.50000 | Layer 7: ID=2409452511904 | Mean=1.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.15it/s, Train_acc=26.7, Train_loss=2.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.23135', '2.23472', '2.24969', '2.22539', '2.22614', '2.22918', '2.19716', '2.21382'] | Gamma1 Grad: ['0.01181', '-0.01615', '0.00013', '0.00877', '0.01392', '0.01021', '0.01733', '-0.03287']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 26.688% (Updated)\n",
      "ğŸ“Š Train Accuracy: 26.688% | ğŸ† Best Train Accuracy: 26.688%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 26.688% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 56.15it/s, Test_acc=32.3, Test_loss=2.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 32.300% | ğŸ† Best Test Accuracy: 32.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=2 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2408987072352 | Mean=2.23135 | Layer 1: ID=2408955071600 | Mean=2.23472 | Layer 2: ID=2408955069040 | Mean=2.24969 | Layer 3: ID=2409452503744 | Mean=2.22539 | Layer 4: ID=2409452505904 | Mean=2.22614 | Layer 5: ID=2409452507904 | Mean=2.22918 | Layer 6: ID=2409452509904 | Mean=2.19716 | Layer 7: ID=2409452511904 | Mean=2.21382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.91it/s, Train_acc=37.7, Train_loss=2.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.29190', '2.28902', '2.30266', '2.28281', '2.28884', '2.28561', '2.25052', '2.30513'] | Gamma1 Grad: ['0.01038', '-0.01218', '-0.02824', '-0.01060', '0.01808', '0.00258', '0.00516', '-0.06469']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 37.652% (Updated)\n",
      "ğŸ“Š Train Accuracy: 37.652% | ğŸ† Best Train Accuracy: 37.652%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 37.652% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.07it/s, Test_acc=40.9, Test_loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 40.930% | ğŸ† Best Test Accuracy: 40.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=3 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2408987072352 | Mean=2.29190 | Layer 1: ID=2408955071600 | Mean=2.28902 | Layer 2: ID=2408955069040 | Mean=2.30266 | Layer 3: ID=2409452503744 | Mean=2.28281 | Layer 4: ID=2409452505904 | Mean=2.28884 | Layer 5: ID=2409452507904 | Mean=2.28561 | Layer 6: ID=2409452509904 | Mean=2.25052 | Layer 7: ID=2409452511904 | Mean=2.30513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.08it/s, Train_acc=45, Train_loss=2.04]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00119 | Gamma1: ['2.30476', '2.29521', '2.30815', '2.28298', '2.28203', '2.29525', '2.26641', '2.32035'] | Gamma1 Grad: ['-0.00307', '-0.00995', '-0.01307', '0.01703', '-0.00174', '-0.03785', '0.01883', '0.04925']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 45.042% (Updated)\n",
      "ğŸ“Š Train Accuracy: 45.042% | ğŸ† Best Train Accuracy: 45.042%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 45.042% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.12it/s, Test_acc=47.7, Test_loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 47.660% | ğŸ† Best Test Accuracy: 47.660%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=4 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2408987072352 | Mean=2.30476 | Layer 1: ID=2408955071600 | Mean=2.29521 | Layer 2: ID=2408955069040 | Mean=2.30815 | Layer 3: ID=2409452503744 | Mean=2.28298 | Layer 4: ID=2409452505904 | Mean=2.28203 | Layer 5: ID=2409452507904 | Mean=2.29525 | Layer 6: ID=2409452509904 | Mean=2.26641 | Layer 7: ID=2409452511904 | Mean=2.32035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.47it/s, Train_acc=50.5, Train_loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00099 | Gamma1: ['2.28766', '2.30016', '2.30130', '2.30580', '2.28967', '2.29750', '2.26309', '2.33443'] | Gamma1 Grad: ['0.01737', '-0.00924', '-0.02796', '-0.02377', '-0.00966', '-0.00578', '0.04670', '-0.10372']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 50.550% (Updated)\n",
      "ğŸ“Š Train Accuracy: 50.550% | ğŸ† Best Train Accuracy: 50.550%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 50.550% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.93it/s, Test_acc=51.8, Test_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 51.780% | ğŸ† Best Test Accuracy: 51.780%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.18it/s, Train_acc=55.3, Train_loss=1.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00076 | Gamma1: ['2.30451', '2.30218', '2.31114', '2.30510', '2.29858', '2.30036', '2.26204', '2.35891'] | Gamma1 Grad: ['0.00353', '-0.00302', '-0.01099', '-0.01016', '-0.02090', '0.01397', '-0.00623', '0.00524']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 55.324% (Updated)\n",
      "ğŸ“Š Train Accuracy: 55.324% | ğŸ† Best Train Accuracy: 55.324%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 55.324% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.90it/s, Test_acc=53.4, Test_loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 53.400% | ğŸ† Best Test Accuracy: 53.400%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.56it/s, Train_acc=58.5, Train_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.30112', '2.30557', '2.31016', '2.29333', '2.29493', '2.29589', '2.26091', '2.34849'] | Gamma1 Grad: ['0.02227', '-0.01413', '0.00639', '0.00311', '0.00031', '0.03353', '0.05216', '-0.02170']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 58.488% (Updated)\n",
      "ğŸ“Š Train Accuracy: 58.488% | ğŸ† Best Train Accuracy: 58.488%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 58.488% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.55it/s, Test_acc=58, Test_loss=1.51]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 57.970% | ğŸ† Best Test Accuracy: 57.970%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.22it/s, Train_acc=61.8, Train_loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00032 | Gamma1: ['2.29430', '2.29916', '2.30369', '2.30390', '2.29886', '2.29676', '2.26701', '2.35967'] | Gamma1 Grad: ['0.05424', '0.00992', '0.01245', '0.01659', '0.01962', '-0.03114', '-0.00601', '0.04277']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 61.762% (Updated)\n",
      "ğŸ“Š Train Accuracy: 61.762% | ğŸ† Best Train Accuracy: 61.762%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 61.762% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.80it/s, Test_acc=58.8, Test_loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 58.820% | ğŸ† Best Test Accuracy: 58.820%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.38it/s, Train_acc=64.7, Train_loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['2.30194', '2.29780', '2.30246', '2.29349', '2.29488', '2.30105', '2.27490', '2.36075'] | Gamma1 Grad: ['0.00853', '-0.01064', '-0.04699', '-0.00063', '-0.00074', '0.07095', '-0.03131', '0.09380']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 64.724% (Updated)\n",
      "ğŸ“Š Train Accuracy: 64.724% | ğŸ† Best Train Accuracy: 64.724%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 64.724% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.11it/s, Test_acc=60.1, Test_loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 60.100% | ğŸ† Best Test Accuracy: 60.100%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.19it/s, Train_acc=67.4, Train_loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['2.28478', '2.29354', '2.30030', '2.30010', '2.29731', '2.29791', '2.27302', '2.36455'] | Gamma1 Grad: ['-0.01632', '-0.01633', '-0.03703', '-0.09060', '-0.01002', '-0.06581', '-0.05650', '-0.08478']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 67.430% (Updated)\n",
      "ğŸ“Š Train Accuracy: 67.430% | ğŸ† Best Train Accuracy: 67.430%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 67.430% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.63it/s, Test_acc=61.9, Test_loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 61.870% | ğŸ† Best Test Accuracy: 61.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.31it/s, Train_acc=69.6, Train_loss=1.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.29547', '2.28941', '2.30211', '2.30046', '2.29758', '2.30214', '2.27360', '2.36702'] | Gamma1 Grad: ['0.01114', '0.00853', '0.00931', '-0.02165', '-0.05000', '-0.05436', '0.08866', '-0.05502']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 69.566% (Updated)\n",
      "ğŸ“Š Train Accuracy: 69.566% | ğŸ† Best Train Accuracy: 69.566%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 69.566% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 6256 | TestAccHist: 11 | TrainLossHist: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.21it/s, Test_acc=61.7, Test_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 61.730% | ğŸ† Best Test Accuracy: 61.870%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.43it/s, Train_acc=71.5, Train_loss=0.964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.29848', '2.30107', '2.30840', '2.27763', '2.29165', '2.30311', '2.27683', '2.36735'] | Gamma1 Grad: ['-0.01276', '-0.00846', '-0.01646', '-0.02674', '-0.03052', '0.05389', '-0.03313', '-0.04097']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 71.506% (Updated)\n",
      "ğŸ“Š Train Accuracy: 71.506% | ğŸ† Best Train Accuracy: 71.506%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 71.506% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 56.90it/s, Test_acc=64.2, Test_loss=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 64.230% | ğŸ† Best Test Accuracy: 64.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.29it/s, Train_acc=73.5, Train_loss=0.894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.30142', '2.29066', '2.29709', '2.31265', '2.29726', '2.30403', '2.28602', '2.39772'] | Gamma1 Grad: ['-0.03046', '-0.01163', '0.13467', '0.03040', '-0.04869', '0.05259', '-0.05856', '-0.02545']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 73.486% (Updated)\n",
      "ğŸ“Š Train Accuracy: 73.486% | ğŸ† Best Train Accuracy: 73.486%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 73.486% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.66it/s, Test_acc=63.5, Test_loss=1.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.540% | ğŸ† Best Test Accuracy: 64.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:49<00:00, 15.67it/s, Train_acc=75.6, Train_loss=0.825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.28582', '2.30568', '2.29387', '2.29752', '2.29181', '2.29525', '2.27099', '2.38915'] | Gamma1 Grad: ['-0.00814', '-0.01831', '-0.04000', '-0.00064', '-0.01582', '-0.06367', '0.14765', '0.02658']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 75.556% (Updated)\n",
      "ğŸ“Š Train Accuracy: 75.556% | ğŸ† Best Train Accuracy: 75.556%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 75.556% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 60.57it/s, Test_acc=63.6, Test_loss=1.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 63.600% | ğŸ† Best Test Accuracy: 64.230%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.12it/s, Train_acc=77, Train_loss=0.766]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.31171', '2.30335', '2.28676', '2.29571', '2.30579', '2.31061', '2.28121', '2.39380'] | Gamma1 Grad: ['0.00151', '-0.01425', '0.00605', '-0.10287', '-0.04555', '-0.05187', '0.19337', '0.13101']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 76.994% (Updated)\n",
      "ğŸ“Š Train Accuracy: 76.994% | ğŸ† Best Train Accuracy: 76.994%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 76.994% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.12it/s, Test_acc=65.2, Test_loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 65.170% | ğŸ† Best Test Accuracy: 65.170%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.88it/s, Train_acc=78.6, Train_loss=0.711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00128 | Gamma1: ['2.29647', '2.28919', '2.29182', '2.29640', '2.27538', '2.31943', '2.28142', '2.40391'] | Gamma1 Grad: ['0.00528', '0.00155', '-0.07663', '0.09684', '-0.00756', '0.05261', '-0.05348', '-0.05881']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 78.622% (Updated)\n",
      "ğŸ“Š Train Accuracy: 78.622% | ğŸ† Best Train Accuracy: 78.622%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 78.622% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.84it/s, Test_acc=64.6, Test_loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.600% | ğŸ† Best Test Accuracy: 65.170%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.33it/s, Train_acc=80.4, Train_loss=0.65] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00119 | Gamma1: ['2.28872', '2.30777', '2.29237', '2.29101', '2.27733', '2.29914', '2.28275', '2.39846'] | Gamma1 Grad: ['0.04284', '-0.00043', '0.04685', '-0.00089', '0.04073', '0.07049', '0.31133', '-0.19721']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 80.376% (Updated)\n",
      "ğŸ“Š Train Accuracy: 80.376% | ğŸ† Best Train Accuracy: 80.376%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 80.376% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.18it/s, Test_acc=65.1, Test_loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.140% | ğŸ† Best Test Accuracy: 65.170%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.43it/s, Train_acc=81.4, Train_loss=0.613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00109 | Gamma1: ['2.29173', '2.29824', '2.28376', '2.29165', '2.30011', '2.31884', '2.27427', '2.38852'] | Gamma1 Grad: ['-0.11444', '0.10067', '0.12813', '0.38109', '-0.04828', '0.07571', '0.06038', '-0.04106']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 81.438% (Updated)\n",
      "ğŸ“Š Train Accuracy: 81.438% | ğŸ† Best Train Accuracy: 81.438%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 81.438% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 60.71it/s, Test_acc=64, Test_loss=1.54]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 64.000% | ğŸ† Best Test Accuracy: 65.170%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.38it/s, Train_acc=82.3, Train_loss=0.58] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00099 | Gamma1: ['2.29326', '2.29765', '2.29603', '2.29692', '2.29476', '2.30136', '2.27489', '2.38669'] | Gamma1 Grad: ['-0.06963', '0.03610', '-0.03974', '0.08015', '-0.00184', '0.01474', '0.03423', '-0.00988']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 82.282% (Updated)\n",
      "ğŸ“Š Train Accuracy: 82.282% | ğŸ† Best Train Accuracy: 82.282%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 82.282% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.39it/s, Test_acc=66, Test_loss=1.46]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 66.010% | ğŸ† Best Test Accuracy: 66.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.48it/s, Train_acc=83.7, Train_loss=0.535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['2.28568', '2.29896', '2.27632', '2.28830', '2.30499', '2.29320', '2.29365', '2.39184'] | Gamma1 Grad: ['0.08268', '0.01971', '-0.00590', '0.10635', '-0.08443', '0.04372', '-0.22090', '-0.00841']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.672% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.672% | ğŸ† Best Train Accuracy: 83.672%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.672% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.26it/s, Test_acc=65.8, Test_loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.760% | ğŸ† Best Test Accuracy: 66.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.59it/s, Train_acc=84.9, Train_loss=0.503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00076 | Gamma1: ['2.29575', '2.28748', '2.29723', '2.29952', '2.31445', '2.30101', '2.27094', '2.39391'] | Gamma1 Grad: ['-0.00926', '-0.03187', '0.05612', '-0.09820', '0.10487', '0.05979', '-0.01384', '0.06953']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 84.858% (Updated)\n",
      "ğŸ“Š Train Accuracy: 84.858% | ğŸ† Best Train Accuracy: 84.858%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 84.858% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 6256 | TestAccHist: 21 | TrainLossHist: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 56.91it/s, Test_acc=65.4, Test_loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.440% | ğŸ† Best Test Accuracy: 66.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.26it/s, Train_acc=85.4, Train_loss=0.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00064 | Gamma1: ['2.29138', '2.29720', '2.29236', '2.30776', '2.30225', '2.30446', '2.27830', '2.40510'] | Gamma1 Grad: ['-0.07788', '-0.02842', '0.04068', '-0.01970', '-0.03993', '-0.00690', '0.11138', '-0.03918']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.436% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.436% | ğŸ† Best Train Accuracy: 85.436%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.436% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.94it/s, Test_acc=66.3, Test_loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 66.330% | ğŸ† Best Test Accuracy: 66.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.16it/s, Train_acc=86.3, Train_loss=0.449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.28846', '2.29447', '2.28077', '2.29389', '2.29049', '2.30157', '2.27442', '2.38731'] | Gamma1 Grad: ['-0.00541', '-0.03874', '-0.01509', '0.05644', '0.03600', '-0.00279', '0.00095', '0.15346']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.310% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.310% | ğŸ† Best Train Accuracy: 86.310%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.310% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.22it/s, Test_acc=65.9, Test_loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.890% | ğŸ† Best Test Accuracy: 66.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.22it/s, Train_acc=87, Train_loss=0.425]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00042 | Gamma1: ['2.27797', '2.28925', '2.29475', '2.29483', '2.29393', '2.30180', '2.27693', '2.39034'] | Gamma1 Grad: ['0.01024', '-0.05911', '0.01848', '-0.03218', '0.12553', '0.03172', '-0.14405', '-0.11935']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.036% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.036% | ğŸ† Best Train Accuracy: 87.036%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.036% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.47it/s, Test_acc=65.9, Test_loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.920% | ğŸ† Best Test Accuracy: 66.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.43it/s, Train_acc=87.8, Train_loss=0.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00032 | Gamma1: ['2.29205', '2.30369', '2.28876', '2.29320', '2.28456', '2.29305', '2.28412', '2.39903'] | Gamma1 Grad: ['-0.04049', '-0.06998', '0.07050', '-0.10597', '0.13565', '-0.17648', '0.02476', '0.06020']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 87.846% (Updated)\n",
      "ğŸ“Š Train Accuracy: 87.846% | ğŸ† Best Train Accuracy: 87.846%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 87.846% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.98it/s, Test_acc=66.2, Test_loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.200% | ğŸ† Best Test Accuracy: 66.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.19it/s, Train_acc=88, Train_loss=0.391]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00023 | Gamma1: ['2.28800', '2.29797', '2.30094', '2.30642', '2.29841', '2.30425', '2.29061', '2.38923'] | Gamma1 Grad: ['-0.00836', '0.03383', '-0.01881', '-0.02341', '-0.04966', '-0.00690', '0.26999', '-0.22427']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.040% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.040% | ğŸ† Best Train Accuracy: 88.040%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.040% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 60.32it/s, Test_acc=66, Test_loss=1.73]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.020% | ğŸ† Best Test Accuracy: 66.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.61it/s, Train_acc=88.9, Train_loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['2.28479', '2.29551', '2.30513', '2.29343', '2.28940', '2.29135', '2.28111', '2.41367'] | Gamma1 Grad: ['-0.08195', '-0.06722', '0.05698', '0.07460', '0.08287', '0.02277', '-0.04601', '0.17298']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.886% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.886% | ğŸ† Best Train Accuracy: 88.886%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.886% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.23it/s, Test_acc=66, Test_loss=1.84]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.040% | ğŸ† Best Test Accuracy: 66.330%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.37it/s, Train_acc=89.5, Train_loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.28964', '2.28836', '2.30086', '2.29587', '2.30212', '2.30865', '2.28029', '2.40884'] | Gamma1 Grad: ['-0.01641', '-0.03088', '0.00859', '-0.02863', '0.00670', '0.08039', '-0.18198', '-0.00989']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.482% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.482% | ğŸ† Best Train Accuracy: 89.482%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.482% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.15it/s, Test_acc=67, Test_loss=1.73]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 66.970% | ğŸ† Best Test Accuracy: 66.970%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:49<00:00, 15.70it/s, Train_acc=89.8, Train_loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['2.28925', '2.28992', '2.28552', '2.30244', '2.28828', '2.29687', '2.28941', '2.41916'] | Gamma1 Grad: ['-0.12293', '-0.02920', '-0.05987', '-0.26535', '-0.26014', '0.15549', '0.13766', '-0.00397']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.788% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.788% | ğŸ† Best Train Accuracy: 89.788%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.788% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 60.38it/s, Test_acc=66.3, Test_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.300% | ğŸ† Best Test Accuracy: 66.970%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:49<00:00, 15.79it/s, Train_acc=89.9, Train_loss=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00002 | Gamma1: ['2.28627', '2.28362', '2.29536', '2.29930', '2.30133', '2.29562', '2.28678', '2.40618'] | Gamma1 Grad: ['0.06213', '0.06681', '-0.02099', '-0.17420', '-0.01405', '-0.08756', '-0.10990', '-0.04007']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.934% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.934% | ğŸ† Best Train Accuracy: 89.934%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.934% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 64.15it/s, Test_acc=66, Test_loss=1.82]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 65.950% | ğŸ† Best Test Accuracy: 66.970%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.57it/s, Train_acc=90.6, Train_loss=0.315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.29670', '2.29668', '2.29011', '2.30569', '2.29409', '2.30715', '2.28320', '2.40674'] | Gamma1 Grad: ['0.02893', '0.01096', '-0.02060', '0.02857', '-0.02313', '0.01649', '-0.01506', '-0.02237']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.596% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.596% | ğŸ† Best Train Accuracy: 90.596%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.596% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 6256 | TestAccHist: 31 | TrainLossHist: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.86it/s, Test_acc=66.8, Test_loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.750% | ğŸ† Best Test Accuracy: 66.970%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:49<00:00, 15.83it/s, Train_acc=90.9, Train_loss=0.309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.27816', '2.27813', '2.30089', '2.29588', '2.27862', '2.31050', '2.30489', '2.42354'] | Gamma1 Grad: ['-0.02984', '0.03481', '0.05117', '0.06801', '0.03104', '-0.00982', '0.02948', '0.00074']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.866% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.866% | ğŸ† Best Train Accuracy: 90.866%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.866% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 63.93it/s, Test_acc=66.9, Test_loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.930% | ğŸ† Best Test Accuracy: 66.970%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.50it/s, Train_acc=91.2, Train_loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.28164', '2.27073', '2.30406', '2.28809', '2.31317', '2.29908', '2.30466', '2.42663'] | Gamma1 Grad: ['-0.03321', '0.07320', '-0.19334', '0.11394', '0.05094', '-0.16699', '0.03976', '0.08675']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.244% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.244% | ğŸ† Best Train Accuracy: 91.244%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.244% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 62.07it/s, Test_acc=66.6, Test_loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.560% | ğŸ† Best Test Accuracy: 66.970%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:48<00:00, 16.15it/s, Train_acc=91.5, Train_loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.27374', '2.29152', '2.29776', '2.29971', '2.29352', '2.28146', '2.28871', '2.41991'] | Gamma1 Grad: ['0.00606', '-0.01771', '-0.01613', '-0.05174', '-0.04824', '-0.08234', '-0.02789', '0.17043']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.490% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.490% | ğŸ† Best Train Accuracy: 91.490%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.490% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 65.37it/s, Test_acc=67.4, Test_loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.360% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.55it/s, Train_acc=91.4, Train_loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.29519', '2.29818', '2.29836', '2.32093', '2.29632', '2.29981', '2.29069', '2.41753'] | Gamma1 Grad: ['0.03010', '-0.01946', '-0.06941', '-0.01799', '0.01117', '-0.06399', '0.07794', '-0.11550']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 91.416% | ğŸ† Best Train Accuracy: 91.490%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.490% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 60.12it/s, Test_acc=66.9, Test_loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.880% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:48<00:00, 15.98it/s, Train_acc=92.1, Train_loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00144 | Gamma1: ['2.28768', '2.28922', '2.29302', '2.29266', '2.28510', '2.29648', '2.27458', '2.40425'] | Gamma1 Grad: ['-0.04107', '-0.01752', '0.01357', '0.00357', '0.04433', '0.05332', '-0.02812', '-0.03658']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.102% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.102% | ğŸ† Best Train Accuracy: 92.102%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.102% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.67it/s, Test_acc=66.9, Test_loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.880% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.48it/s, Train_acc=92.2, Train_loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.29807', '2.31220', '2.28756', '2.28419', '2.29254', '2.29523', '2.27822', '2.43225'] | Gamma1 Grad: ['0.00334', '-0.01921', '0.04313', '-0.01304', '-0.00253', '0.07262', '-0.06197', '-0.09053']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.176% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.176% | ğŸ† Best Train Accuracy: 92.176%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.176% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.64it/s, Test_acc=66.5, Test_loss=2.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.540% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.45it/s, Train_acc=92.2, Train_loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00139 | Gamma1: ['2.29834', '2.29346', '2.29499', '2.30280', '2.29340', '2.29276', '2.28227', '2.40877'] | Gamma1 Grad: ['-0.01586', '0.04481', '0.07478', '-0.24398', '-0.18239', '-0.28176', '0.16882', '-0.02661']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 92.174% | ğŸ† Best Train Accuracy: 92.176%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.176% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 64.79it/s, Test_acc=67.1, Test_loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.130% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:48<00:00, 16.11it/s, Train_acc=92.4, Train_loss=0.262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.29254', '2.30247', '2.28889', '2.28686', '2.28558', '2.29877', '2.29149', '2.41230'] | Gamma1 Grad: ['0.01325', '0.00681', '0.03960', '0.02357', '-0.03796', '0.02904', '-0.05636', '-0.09636']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.400% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.400% | ğŸ† Best Train Accuracy: 92.400%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.400% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 62.50it/s, Test_acc=66.8, Test_loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.850% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.63it/s, Train_acc=92.8, Train_loss=0.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00132 | Gamma1: ['2.27627', '2.29476', '2.28194', '2.30042', '2.31622', '2.29704', '2.30639', '2.42119'] | Gamma1 Grad: ['0.02146', '-0.00863', '0.01104', '0.03454', '0.01149', '-0.04597', '-0.02225', '0.05684']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.784% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.784% | ğŸ† Best Train Accuracy: 92.784%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.784% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 63.38it/s, Test_acc=66.8, Test_loss=2.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.830% | ğŸ† Best Test Accuracy: 67.360%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:48<00:00, 16.26it/s, Train_acc=93, Train_loss=0.245]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00128 | Gamma1: ['2.28311', '2.28337', '2.28104', '2.30661', '2.29582', '2.28021', '2.30628', '2.59886'] | Gamma1 Grad: ['-0.05449', '-0.01897', '-0.02894', '0.07574', '0.21311', '0.08550', '0.09140', '-0.19572']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.040% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.040% | ğŸ† Best Train Accuracy: 93.040%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.040% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 6256 | TestAccHist: 41 | TrainLossHist: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 64.21it/s, Test_acc=67.9, Test_loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 67.890% | ğŸ† Best Test Accuracy: 67.890%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.43it/s, Train_acc=92.7, Train_loss=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00124 | Gamma1: ['2.28380', '2.29117', '2.29697', '2.30196', '2.25325', '2.32359', '2.30608', '2.64806'] | Gamma1 Grad: ['-0.07814', '-0.03500', '-0.27012', '-0.04670', '0.10114', '-0.04411', '0.22313', '-0.37177']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 92.662% | ğŸ† Best Train Accuracy: 93.040%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.040% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 62.41it/s, Test_acc=67.1, Test_loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.130% | ğŸ† Best Test Accuracy: 67.890%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:49<00:00, 15.84it/s, Train_acc=93.3, Train_loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00119 | Gamma1: ['2.27601', '2.28793', '2.30665', '2.28792', '2.30522', '2.30067', '2.31124', '2.67038'] | Gamma1 Grad: ['-0.05503', '-0.04272', '0.16877', '-0.06095', '0.56321', '0.18918', '0.29023', '-0.04196']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.332% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.332% | ğŸ† Best Train Accuracy: 93.332%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.332% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 64.18it/s, Test_acc=67, Test_loss=2.04]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.040% | ğŸ† Best Test Accuracy: 67.890%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:49<00:00, 15.94it/s, Train_acc=93, Train_loss=0.248]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00114 | Gamma1: ['2.29386', '2.29274', '2.27729', '2.31109', '2.30980', '2.30599', '2.30104', '2.67689'] | Gamma1 Grad: ['0.03345', '-0.00467', '-0.07372', '-0.05959', '-0.04405', '-0.11633', '0.22083', '0.12041']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.026% | ğŸ† Best Train Accuracy: 93.332%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.332% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 63.71it/s, Test_acc=67.5, Test_loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.540% | ğŸ† Best Test Accuracy: 67.890%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.58it/s, Train_acc=93.8, Train_loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00109 | Gamma1: ['2.28403', '2.28560', '2.29792', '2.31179', '2.29375', '2.29113', '2.29254', '2.67679'] | Gamma1 Grad: ['-0.02894', '-0.03409', '0.04176', '-0.17672', '0.70000', '-0.09885', '0.02763', '-0.02700']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.774% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.774% | ğŸ† Best Train Accuracy: 93.774%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.774% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 61.35it/s, Test_acc=67.2, Test_loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.160% | ğŸ† Best Test Accuracy: 67.890%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:49<00:00, 15.91it/s, Train_acc=93.7, Train_loss=0.225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00104 | Gamma1: ['2.25425', '2.30091', '2.30245', '2.28331', '2.28058', '2.30842', '2.28624', '2.69643'] | Gamma1 Grad: ['-0.04022', '0.03194', '0.15327', '-0.05597', '-0.16138', '0.29006', '0.09420', '-0.24505']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.720% | ğŸ† Best Train Accuracy: 93.774%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.774% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 62.27it/s, Test_acc=66.6, Test_loss=2.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.570% | ğŸ† Best Test Accuracy: 67.890%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.53it/s, Train_acc=93.6, Train_loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00099 | Gamma1: ['2.28346', '2.28015', '2.27180', '2.29068', '2.29813', '2.29810', '2.29428', '2.73006'] | Gamma1 Grad: ['-0.02361', '-0.00521', '-0.02407', '-0.13792', '-0.01345', '-0.22120', '-0.05730', '0.22985']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.634% | ğŸ† Best Train Accuracy: 93.774%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.774% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 60.48it/s, Test_acc=68.3, Test_loss=2.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 68.300% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:49<00:00, 15.86it/s, Train_acc=94.1, Train_loss=0.216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00093 | Gamma1: ['2.28561', '2.28092', '2.29439', '2.29450', '2.28317', '2.29601', '2.28538', '2.70725'] | Gamma1 Grad: ['0.20431', '0.03482', '0.34388', '0.70000', '0.61140', '-0.18755', '0.11688', '-0.00364']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.066% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.066% | ğŸ† Best Train Accuracy: 94.066%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.066% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 60.94it/s, Test_acc=67, Test_loss=2.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.030% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.42it/s, Train_acc=93.7, Train_loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['2.25880', '2.31365', '2.30180', '2.31359', '2.30383', '2.32848', '2.30321', '2.71355'] | Gamma1 Grad: ['0.02806', '-0.00196', '0.02838', '-0.04110', '0.02067', '0.12293', '-0.12395', '0.04536']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 93.686% | ğŸ† Best Train Accuracy: 94.066%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.066% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 62.13it/s, Test_acc=66.9, Test_loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.870% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.47it/s, Train_acc=94.1, Train_loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00081 | Gamma1: ['2.27092', '2.29635', '2.31200', '2.30744', '2.28272', '2.30852', '2.28980', '2.73321'] | Gamma1 Grad: ['-0.06664', '0.08028', '0.02190', '-0.02759', '0.17730', '0.02548', '-0.47956', '0.29640']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.074% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.074% | ğŸ† Best Train Accuracy: 94.074%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.074% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 62.72it/s, Test_acc=66.9, Test_loss=2.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.920% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.25it/s, Train_acc=94.2, Train_loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00076 | Gamma1: ['2.29794', '2.28845', '2.27839', '2.32310', '2.29647', '2.29209', '2.31324', '2.71380'] | Gamma1 Grad: ['0.06078', '-0.01254', '0.03159', '-0.17074', '0.01505', '-0.01014', '-0.11938', '0.21344']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.154% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.154% | ğŸ† Best Train Accuracy: 94.154%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.154% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 6256 | TestAccHist: 51 | TrainLossHist: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 56.38it/s, Test_acc=67, Test_loss=2.22]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.980% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 51 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:53<00:00, 14.75it/s, Train_acc=94.4, Train_loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00070 | Gamma1: ['2.28461', '2.29257', '2.27469', '2.31231', '2.26422', '2.29101', '2.31442', '2.73194'] | Gamma1 Grad: ['-0.04082', '-0.01204', '0.04732', '-0.04718', '0.04221', '0.09406', '-0.13158', '-0.13668']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.410% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.410% | ğŸ† Best Train Accuracy: 94.410%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.410% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.41it/s, Test_acc=67.2, Test_loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.150% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 52 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:53<00:00, 14.50it/s, Train_acc=94.6, Train_loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00064 | Gamma1: ['2.30088', '2.29257', '2.29894', '2.30161', '2.28471', '2.30309', '2.32942', '2.72522'] | Gamma1 Grad: ['-0.03410', '-0.06791', '0.16588', '-0.18270', '-0.07299', '-0.02799', '0.56618', '-0.06002']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.564% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.564% | ğŸ† Best Train Accuracy: 94.564%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.564% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 54.98it/s, Test_acc=66.8, Test_loss=2.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.820% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 53 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.78it/s, Train_acc=94.3, Train_loss=0.213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00058 | Gamma1: ['2.29149', '2.30887', '2.30672', '2.28280', '2.27942', '2.29802', '2.32239', '2.73997'] | Gamma1 Grad: ['-0.01712', '0.05188', '-0.09013', '0.12994', '-0.08629', '-0.05308', '0.38832', '0.02826']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 94.304% | ğŸ† Best Train Accuracy: 94.564%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.564% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.52it/s, Test_acc=67.1, Test_loss=2.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.110% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54:   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 54 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.98it/s, Train_acc=94.6, Train_loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.27915', '2.29840', '2.29748', '2.31058', '2.30496', '2.31106', '2.33183', '2.72989'] | Gamma1 Grad: ['-0.02396', '-0.02502', '0.09262', '0.03451', '-0.13227', '-0.04947', '0.23380', '0.10928']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 94.556% | ğŸ† Best Train Accuracy: 94.564%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.564% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 56.35it/s, Test_acc=67.1, Test_loss=2.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.070% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.82it/s, Train_acc=94.6, Train_loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00047 | Gamma1: ['2.25597', '2.30864', '2.30189', '2.30742', '2.30121', '2.29695', '2.31108', '2.72862'] | Gamma1 Grad: ['0.01086', '-0.00209', '-0.01489', '0.10449', '-0.04307', '0.00235', '-0.06276', '0.00351']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 94.562% | ğŸ† Best Train Accuracy: 94.564%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.564% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 54.97it/s, Test_acc=66.9, Test_loss=2.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 66.860% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:53<00:00, 14.73it/s, Train_acc=94.6, Train_loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00042 | Gamma1: ['2.27990', '2.26325', '2.30094', '2.32015', '2.30679', '2.29608', '2.30869', '2.73964'] | Gamma1 Grad: ['0.03436', '-0.34674', '-0.70000', '-0.70000', '-0.23396', '-0.35036', '0.13289', '-0.10449']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.586% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.586% | ğŸ† Best Train Accuracy: 94.586%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.586% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.77it/s, Test_acc=67.8, Test_loss=2.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.780% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.83it/s, Train_acc=94, Train_loss=0.216]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00037 | Gamma1: ['2.28330', '2.28572', '2.28131', '2.31902', '2.29882', '2.29028', '2.33037', '2.75364'] | Gamma1 Grad: ['-0.00491', '0.00187', '-0.00259', '0.00000', '0.02199', '0.00135', '-0.03858', '0.02294']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 94.020% | ğŸ† Best Train Accuracy: 94.586%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.586% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.21it/s, Test_acc=67.9, Test_loss=2.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.860% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 15.03it/s, Train_acc=94.8, Train_loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00032 | Gamma1: ['2.26612', '2.30072', '2.27423', '2.29641', '2.28917', '2.28498', '2.31739', '2.75566'] | Gamma1 Grad: ['-0.01383', '-0.00046', '0.00776', '-0.00497', '0.04309', '-0.01962', '-0.05725', '-0.00099']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.750% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.750% | ğŸ† Best Train Accuracy: 94.750%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.750% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.46it/s, Test_acc=67, Test_loss=2.36]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.020% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.83it/s, Train_acc=95.2, Train_loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00027 | Gamma1: ['2.27358', '2.29336', '2.26232', '2.30920', '2.28836', '2.28386', '2.31561', '2.75217'] | Gamma1 Grad: ['0.02220', '0.01699', '-0.09862', '-0.06884', '-0.22488', '-0.21186', '0.23203', '-0.07243']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 95.230% (Updated)\n",
      "ğŸ“Š Train Accuracy: 95.230% | ğŸ† Best Train Accuracy: 95.230%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.230% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.36it/s, Test_acc=67.5, Test_loss=2.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.540% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.20it/s, Train_acc=94.9, Train_loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00023 | Gamma1: ['2.27467', '2.30955', '2.27753', '2.31574', '2.31092', '2.26888', '2.31759', '2.83317'] | Gamma1 Grad: ['0.05218', '0.02545', '0.06871', '-0.00354', '0.06628', '0.13120', '-0.18725', '0.14105']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 94.944% | ğŸ† Best Train Accuracy: 95.230%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.230% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 6256 | TestAccHist: 61 | TrainLossHist: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.74it/s, Test_acc=67, Test_loss=2.33]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.000% | ğŸ† Best Test Accuracy: 68.300%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:53<00:00, 14.63it/s, Train_acc=95, Train_loss=0.188]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.26205', '2.31424', '2.28750', '2.32250', '2.31708', '2.27680', '2.34118', '2.86659'] | Gamma1 Grad: ['0.01713', '0.02380', '0.01599', '0.13201', '-0.08344', '0.02400', '-0.01843', '0.15263']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.000% | ğŸ† Best Train Accuracy: 95.230%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.230% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.77it/s, Test_acc=68.5, Test_loss=2.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 68.460% | ğŸ† Best Test Accuracy: 68.460%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 15.01it/s, Train_acc=95.1, Train_loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['2.25080', '2.30479', '2.27805', '2.30921', '2.31355', '2.27495', '2.36080', '2.89569'] | Gamma1 Grad: ['-0.00341', '-0.00383', '-0.00232', '0.00251', '0.01130', '0.01029', '0.01138', '0.01298']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.124% | ğŸ† Best Train Accuracy: 95.230%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.230% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.90it/s, Test_acc=67.7, Test_loss=2.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.680% | ğŸ† Best Test Accuracy: 68.460%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.13it/s, Train_acc=95, Train_loss=0.19]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00012 | Gamma1: ['2.25101', '2.30182', '2.27410', '2.32859', '2.30801', '2.29113', '2.34297', '2.89773'] | Gamma1 Grad: ['0.03779', '-0.00450', '-0.04470', '-0.04556', '0.06817', '-0.01684', '0.05290', '0.08679']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 94.970% | ğŸ† Best Train Accuracy: 95.230%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.230% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.16it/s, Test_acc=67.9, Test_loss=2.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.910% | ğŸ† Best Test Accuracy: 68.460%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.84it/s, Train_acc=95.4, Train_loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.27720', '2.31740', '2.26579', '2.31823', '2.29180', '2.28566', '2.33727', '2.90512'] | Gamma1 Grad: ['0.06608', '0.02451', '0.20875', '0.04063', '0.09734', '-0.12431', '-0.05693', '0.10248']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 95.414% (Updated)\n",
      "ğŸ“Š Train Accuracy: 95.414% | ğŸ† Best Train Accuracy: 95.414%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.414% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 60.48it/s, Test_acc=67.1, Test_loss=2.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.120% | ğŸ† Best Test Accuracy: 68.460%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.13it/s, Train_acc=95, Train_loss=0.189]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.27944', '2.30680', '2.25658', '2.32168', '2.31011', '2.27840', '2.35267', '2.94004'] | Gamma1 Grad: ['0.03049', '-0.08347', '0.02070', '-0.00557', '-0.33501', '-0.19860', '0.23026', '0.04164']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.022% | ğŸ† Best Train Accuracy: 95.414%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.414% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 55.39it/s, Test_acc=68.4, Test_loss=2.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.410% | ğŸ† Best Test Accuracy: 68.460%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 15.01it/s, Train_acc=95.2, Train_loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['2.27928', '2.31909', '2.24935', '2.32393', '2.30741', '2.30025', '2.37150', '2.95096'] | Gamma1 Grad: ['0.02003', '0.06485', '-0.08149', '-0.01709', '0.14410', '-0.14886', '-0.07797', '-0.03176']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.196% | ğŸ† Best Train Accuracy: 95.414%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.414% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.50it/s, Test_acc=67.8, Test_loss=2.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.820% | ğŸ† Best Test Accuracy: 68.460%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.21it/s, Train_acc=95.3, Train_loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00003 | Gamma1: ['2.27043', '2.32869', '2.27211', '2.31898', '2.32114', '2.28565', '2.37266', '2.96864'] | Gamma1 Grad: ['-0.00771', '0.00748', '0.02577', '0.00978', '-0.00091', '0.00926', '0.01636', '0.00484']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.278% | ğŸ† Best Train Accuracy: 95.414%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.414% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 54.67it/s, Test_acc=68.6, Test_loss=2.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 68.570% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.83it/s, Train_acc=95.6, Train_loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00002 | Gamma1: ['2.27363', '2.31052', '2.26313', '2.31745', '2.31002', '2.28993', '2.37628', '2.96947'] | Gamma1 Grad: ['-0.01260', '-0.03429', '-0.08174', '0.07820', '-0.36829', '-0.25750', '-0.23014', '-0.04917']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 95.630% (Updated)\n",
      "ğŸ“Š Train Accuracy: 95.630% | ğŸ† Best Train Accuracy: 95.630%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.630% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.82it/s, Test_acc=67.7, Test_loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.650% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.98it/s, Train_acc=95.4, Train_loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00001 | Gamma1: ['2.26548', '2.28774', '2.28615', '2.31960', '2.32994', '2.27764', '2.37254', '2.97675'] | Gamma1 Grad: ['0.16564', '0.00710', '0.07976', '-0.21048', '-0.27202', '0.03859', '0.31545', '-0.18557']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.420% | ğŸ† Best Train Accuracy: 95.630%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.630% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 60.64it/s, Test_acc=68.2, Test_loss=2.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.180% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.79it/s, Train_acc=95.2, Train_loss=0.18] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.27883', '2.26893', '2.26662', '2.32025', '2.32393', '2.29335', '2.36376', '2.99350'] | Gamma1 Grad: ['-0.02486', '0.03178', '0.00262', '-0.02323', '-0.05787', '0.04487', '0.01499', '-0.04085']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.202% | ğŸ† Best Train Accuracy: 95.630%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.630% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 6256 | TestAccHist: 71 | TrainLossHist: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.53it/s, Test_acc=67.9, Test_loss=2.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.890% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.18it/s, Train_acc=95.6, Train_loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.29616', '2.30773', '2.26633', '2.32524', '2.24836', '2.31391', '2.36243', '3.00574'] | Gamma1 Grad: ['-0.01913', '0.01138', '0.02035', '0.02257', '0.21197', '0.08631', '-0.00898', '-0.05645']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.582% | ğŸ† Best Train Accuracy: 95.630%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.630% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.80it/s, Test_acc=67.5, Test_loss=2.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.540% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.97it/s, Train_acc=95.3, Train_loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.21238', '2.27141', '2.24915', '2.31953', '2.32122', '2.28661', '2.39009', '3.03089'] | Gamma1 Grad: ['0.02926', '-0.08343', '0.04949', '0.17481', '-0.18446', '0.19063', '-0.13697', '0.25205']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.314% | ğŸ† Best Train Accuracy: 95.630%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.630% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.08it/s, Test_acc=68.5, Test_loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.550% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.33it/s, Train_acc=95.5, Train_loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.28491', '2.30214', '2.28841', '2.31301', '2.31180', '2.25988', '2.36146', '3.04725'] | Gamma1 Grad: ['-0.01522', '0.02687', '0.00369', '-0.01508', '0.01335', '-0.14476', '0.11964', '-0.08655']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.516% | ğŸ† Best Train Accuracy: 95.630%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.630% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.11it/s, Test_acc=68.3, Test_loss=2.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.260% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.05it/s, Train_acc=95.7, Train_loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.27192', '2.32183', '2.30281', '2.32217', '2.31965', '2.28385', '2.37027', '3.01057'] | Gamma1 Grad: ['0.02057', '0.03285', '-0.02133', '0.02887', '-0.10494', '-0.04625', '-0.09580', '0.09777']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 95.716% (Updated)\n",
      "ğŸ“Š Train Accuracy: 95.716% | ğŸ† Best Train Accuracy: 95.716%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.716% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 55.80it/s, Test_acc=67.4, Test_loss=2.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.430% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.15it/s, Train_acc=95.5, Train_loss=0.177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.25396', '2.28157', '2.30103', '2.30841', '2.29698', '2.28510', '2.37959', '3.04749'] | Gamma1 Grad: ['0.00112', '0.04286', '-0.05130', '0.07835', '0.11428', '0.11477', '0.03372', '0.03631']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.458% | ğŸ† Best Train Accuracy: 95.716%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.716% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.50it/s, Test_acc=67.7, Test_loss=2.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 67.670% | ğŸ† Best Test Accuracy: 68.570%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.86it/s, Train_acc=95.8, Train_loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.25490', '2.29633', '2.27377', '2.31851', '2.31537', '2.28331', '2.35471', '3.02678'] | Gamma1 Grad: ['0.11053', '-0.13113', '0.12111', '0.34367', '-0.70000', '-0.60577', '-0.10413', '-0.23464']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 95.848% (Updated)\n",
      "ğŸ“Š Train Accuracy: 95.848% | ğŸ† Best Train Accuracy: 95.848%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.848% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.56it/s, Test_acc=68.8, Test_loss=2.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 68.790% | ğŸ† Best Test Accuracy: 68.790%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.92it/s, Train_acc=95.6, Train_loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00147 | Gamma1: ['2.27667', '2.31059', '2.25659', '2.33907', '2.31230', '2.27002', '2.34993', '3.06308'] | Gamma1 Grad: ['-0.04388', '0.05959', '-0.10533', '0.26596', '-0.13651', '0.16007', '-0.13397', '0.19085']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.598% | ğŸ† Best Train Accuracy: 95.848%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.848% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 60.06it/s, Test_acc=68.6, Test_loss=2.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.580% | ğŸ† Best Test Accuracy: 68.790%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:50<00:00, 15.36it/s, Train_acc=95.9, Train_loss=0.164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.26214', '2.30186', '2.28647', '2.29986', '2.33630', '2.23384', '2.38016', '2.98894'] | Gamma1 Grad: ['0.00260', '-0.04983', '-0.03475', '-0.06791', '0.09034', '-0.01566', '0.19625', '-0.27753']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 95.930% (Updated)\n",
      "ğŸ“Š Train Accuracy: 95.930% | ğŸ† Best Train Accuracy: 95.930%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.930% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 54.88it/s, Test_acc=68, Test_loss=2.62]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.030% | ğŸ† Best Test Accuracy: 68.790%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.09it/s, Train_acc=95.8, Train_loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00145 | Gamma1: ['2.25563', '2.27133', '2.28938', '2.32227', '2.32552', '2.26699', '2.38558', '3.05554'] | Gamma1 Grad: ['0.00700', '0.02955', '-0.00346', '0.00562', '0.01304', '-0.04774', '-0.03497', '-0.02780']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 95.760% | ğŸ† Best Train Accuracy: 95.930%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.930% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.62it/s, Test_acc=68.2, Test_loss=2.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 68.200% | ğŸ† Best Test Accuracy: 68.790%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.17it/s, Train_acc=97.8, Train_loss=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00144 | Gamma1: ['2.27623', '2.30121', '2.27211', '2.33442', '2.29509', '2.25054', '2.37844', '2.93319'] | Gamma1 Grad: ['-0.00044', '-0.02117', '0.10106', '-0.09231', '-0.18373', '-0.14333', '0.09465', '0.06542']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.802% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.802% | ğŸ† Best Train Accuracy: 97.802%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.802% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 6256 | TestAccHist: 81 | TrainLossHist: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.66it/s, Test_acc=70, Test_loss=2.44]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 70.010% | ğŸ† Best Test Accuracy: 70.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.78it/s, Train_acc=98.7, Train_loss=0.072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00143 | Gamma1: ['2.29932', '2.29686', '2.27543', '2.31411', '2.28697', '2.26774', '2.35754', '2.80362'] | Gamma1 Grad: ['-0.01103', '-0.03064', '0.03449', '-0.04400', '0.02447', '0.07806', '0.06196', '0.04895']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.720% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.720% | ğŸ† Best Train Accuracy: 98.720%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.720% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.55it/s, Test_acc=70.7, Test_loss=2.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 70.660% | ğŸ† Best Test Accuracy: 70.660%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.91it/s, Train_acc=99, Train_loss=0.061]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00142 | Gamma1: ['2.28220', '2.29071', '2.26794', '2.30907', '2.27713', '2.26132', '2.33647', '2.69479'] | Gamma1 Grad: ['0.07111', '0.03795', '-0.11365', '0.01247', '-0.00330', '-0.14583', '-0.00591', '0.10904']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.034% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.034% | ğŸ† Best Train Accuracy: 99.034%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.034% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.72it/s, Test_acc=70.9, Test_loss=2.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 70.910% | ğŸ† Best Test Accuracy: 70.910%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.87it/s, Train_acc=99.1, Train_loss=0.058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00141 | Gamma1: ['2.28610', '2.30051', '2.26161', '2.31414', '2.27145', '2.24392', '2.31096', '2.66518'] | Gamma1 Grad: ['0.00516', '0.00035', '0.00208', '-0.00131', '-0.01183', '-0.00348', '-0.00341', '0.00694']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.076% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.076% | ğŸ† Best Train Accuracy: 99.076%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.076% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.60it/s, Test_acc=71, Test_loss=2.41]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 71.010% | ğŸ† Best Test Accuracy: 71.010%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.88it/s, Train_acc=99.3, Train_loss=0.053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00139 | Gamma1: ['2.29220', '2.29400', '2.27651', '2.30386', '2.28910', '2.26768', '2.28615', '2.60882'] | Gamma1 Grad: ['-0.00249', '0.00881', '0.02506', '-0.00126', '0.02163', '-0.05209', '-0.01808', '0.00595']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.312% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.312% | ğŸ† Best Train Accuracy: 99.312%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.312% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.51it/s, Test_acc=71.3, Test_loss=2.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 71.350% | ğŸ† Best Test Accuracy: 71.350%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.77it/s, Train_acc=99.4, Train_loss=0.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00137 | Gamma1: ['2.27381', '2.29375', '2.30011', '2.30179', '2.29386', '2.23293', '2.31030', '2.54686'] | Gamma1 Grad: ['0.02075', '0.11249', '0.04520', '-0.15960', '-0.14497', '0.02209', '0.06352', '0.11035']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.398% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.398% | ğŸ† Best Train Accuracy: 99.398%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.398% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.71it/s, Test_acc=71.5, Test_loss=2.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 71.470% | ğŸ† Best Test Accuracy: 71.470%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.97it/s, Train_acc=99.5, Train_loss=0.048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00136 | Gamma1: ['2.28063', '2.28306', '2.29746', '2.32076', '2.27174', '2.23028', '2.30236', '2.49404'] | Gamma1 Grad: ['-0.01133', '-0.04604', '0.00033', '-0.06590', '-0.04335', '0.05530', '-0.08348', '-0.05142']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.454% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.454% | ğŸ† Best Train Accuracy: 99.454%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.454% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.58it/s, Test_acc=71.1, Test_loss=2.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 71.110% | ğŸ† Best Test Accuracy: 71.470%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.89it/s, Train_acc=99.5, Train_loss=0.048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00134 | Gamma1: ['2.28771', '2.30779', '2.27724', '2.31183', '2.24681', '2.26341', '2.31685', '2.49179'] | Gamma1 Grad: ['0.01584', '0.01088', '-0.00166', '-0.00983', '-0.00703', '-0.00881', '0.02040', '-0.00079']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.468% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.468% | ğŸ† Best Train Accuracy: 99.468%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.468% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 59.06it/s, Test_acc=70.9, Test_loss=2.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 70.920% | ğŸ† Best Test Accuracy: 71.470%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.79it/s, Train_acc=99.6, Train_loss=0.045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00132 | Gamma1: ['2.30917', '2.29313', '2.28843', '2.31068', '2.27515', '2.27923', '2.28329', '2.46118'] | Gamma1 Grad: ['0.00391', '0.02662', '-0.13249', '0.16473', '0.14300', '-0.07485', '-0.00509', '-0.09143']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.552% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.552% | ğŸ† Best Train Accuracy: 99.552%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.552% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.90it/s, Test_acc=70.9, Test_loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 70.890% | ğŸ† Best Test Accuracy: 71.470%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.85it/s, Train_acc=99.6, Train_loss=0.044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00130 | Gamma1: ['2.29291', '2.28670', '2.29789', '2.29794', '2.24864', '2.26512', '2.27526', '2.42524'] | Gamma1 Grad: ['0.05717', '0.01436', '-0.02121', '-0.03556', '0.13409', '0.01513', '0.05493', '-0.06075']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.616% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.616% | ğŸ† Best Train Accuracy: 99.616%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.616% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.57it/s, Test_acc=71.5, Test_loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 71.470% | ğŸ† Best Test Accuracy: 71.470%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.86it/s, Train_acc=99.6, Train_loss=0.044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00128 | Gamma1: ['2.30983', '2.28007', '2.28396', '2.28076', '2.24326', '2.30055', '2.28203', '2.44509'] | Gamma1 Grad: ['0.00047', '-0.00002', '-0.00007', '-0.00075', '-0.00094', '0.00119', '0.00031', '0.00280']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.564% | ğŸ† Best Train Accuracy: 99.616%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.616% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 6256 | TestAccHist: 91 | TrainLossHist: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.69it/s, Test_acc=71.4, Test_loss=2.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 71.370% | ğŸ† Best Test Accuracy: 71.470%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:51<00:00, 15.32it/s, Train_acc=99.6, Train_loss=0.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00126 | Gamma1: ['2.31362', '2.30108', '2.28019', '2.27379', '2.27965', '2.29680', '2.28967', '2.43628'] | Gamma1 Grad: ['-0.01115', '-0.05359', '0.02869', '0.09362', '-0.12081', '0.16210', '0.12712', '-0.05167']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.642% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.642% | ğŸ† Best Train Accuracy: 99.642%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.642% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 56.58it/s, Test_acc=71.8, Test_loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 71.820% | ğŸ† Best Test Accuracy: 71.820%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:53<00:00, 14.63it/s, Train_acc=99.7, Train_loss=0.042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00124 | Gamma1: ['2.31933', '2.31258', '2.28757', '2.31009', '2.27103', '2.29223', '2.28496', '2.42321'] | Gamma1 Grad: ['0.00175', '0.00529', '0.00301', '0.01135', '-0.00820', '0.01918', '-0.02805', '0.00587']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.678% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.678% | ğŸ† Best Train Accuracy: 99.678%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.678% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 60.72it/s, Test_acc=71.5, Test_loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 71.550% | ğŸ† Best Test Accuracy: 71.820%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.85it/s, Train_acc=99.7, Train_loss=0.041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00122 | Gamma1: ['2.31785', '2.30849', '2.25581', '2.27801', '2.26230', '2.28795', '2.27783', '2.42496'] | Gamma1 Grad: ['0.00047', '0.00054', '-0.00150', '-0.00098', '0.00170', '-0.00164', '-0.00355', '0.00469']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.682% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.682% | ğŸ† Best Train Accuracy: 99.682%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.682% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 53.49it/s, Test_acc=71.5, Test_loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 71.500% | ğŸ† Best Test Accuracy: 71.820%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.77it/s, Train_acc=99.7, Train_loss=0.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00119 | Gamma1: ['2.29818', '2.30777', '2.27949', '2.28474', '2.27684', '2.28813', '2.28047', '2.41179'] | Gamma1 Grad: ['0.00759', '0.00401', '0.00200', '0.00987', '0.01838', '-0.02313', '0.02124', '0.06997']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.708% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.708% | ğŸ† Best Train Accuracy: 99.708%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.708% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 56.63it/s, Test_acc=71.4, Test_loss=2.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 71.420% | ğŸ† Best Test Accuracy: 71.820%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:53<00:00, 14.62it/s, Train_acc=99.8, Train_loss=0.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00117 | Gamma1: ['2.29394', '2.30002', '2.28631', '2.29900', '2.26056', '2.27584', '2.28429', '2.38673'] | Gamma1 Grad: ['0.00120', '0.00106', '-0.00096', '-0.00046', '0.00008', '0.00048', '-0.00294', '-0.00060']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.750% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.750% | ğŸ† Best Train Accuracy: 99.750%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.750% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 57.72it/s, Test_acc=71.3, Test_loss=2.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 71.330% | ğŸ† Best Test Accuracy: 71.820%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.77it/s, Train_acc=99.7, Train_loss=0.041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00114 | Gamma1: ['2.27695', '2.29377', '2.26715', '2.29523', '2.26426', '2.25892', '2.29337', '2.38440'] | Gamma1 Grad: ['0.03205', '-0.01091', '-0.04592', '-0.02322', '-0.03023', '-0.07075', '-0.00809', '0.02738']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.680% | ğŸ† Best Train Accuracy: 99.750%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.750% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 58.67it/s, Test_acc=71.9, Test_loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 71.860% | ğŸ† Best Test Accuracy: 71.860%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.78it/s, Train_acc=99.8, Train_loss=0.038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00112 | Gamma1: ['2.29144', '2.28781', '2.31117', '2.29549', '2.27485', '2.27600', '2.28298', '2.41384'] | Gamma1 Grad: ['-0.01059', '-0.00419', '0.02029', '-0.00614', '-0.00684', '-0.01544', '0.01742', '-0.00384']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.792% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.792% | ğŸ† Best Train Accuracy: 99.792%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.792% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 56.67it/s, Test_acc=71.8, Test_loss=2.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 71.790% | ğŸ† Best Test Accuracy: 71.860%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:53<00:00, 14.66it/s, Train_acc=99.8, Train_loss=0.039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00109 | Gamma1: ['2.31308', '2.28539', '2.28281', '2.29932', '2.28823', '2.28852', '2.27143', '2.39445'] | Gamma1 Grad: ['0.00092', '0.00265', '-0.00280', '-0.00127', '-0.00013', '0.00927', '-0.00238', '-0.00238']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.774% | ğŸ† Best Train Accuracy: 99.792%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.792% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 56.46it/s, Test_acc=72.1, Test_loss=2.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR100_B64_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 72.140% | ğŸ† Best Test Accuracy: 72.140%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:52<00:00, 14.91it/s, Train_acc=99.8, Train_loss=0.038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00107 | Gamma1: ['2.30085', '2.31547', '2.30008', '2.28590', '2.28295', '2.29952', '2.27631', '2.38983'] | Gamma1 Grad: ['-0.00278', '0.00038', '0.00027', '0.00063', '0.00560', '0.00171', '0.00507', '0.00543']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.774% | ğŸ† Best Train Accuracy: 99.792%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR100_Train_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.792% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:02<00:00, 56.88it/s, Test_acc=72, Test_loss=2.46]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 71.990% | ğŸ† Best Test Accuracy: 72.140%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR100\\SENet18\\Results\\CIFAR100_Test_B64_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n",
      "Best Test Accuracy:  72.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 11. TRAIN MODEL WITH SHEDULAR | XXX ----------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Set Seed for Reproducibility BEFORE training starts\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(0)\n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(0)  \n",
    "\n",
    "\n",
    "\n",
    "# âœ… Training Loop\n",
    "num_epochs = 100 # Example: Set the total number of epochs\n",
    "for epoch in range(start_epoch, num_epochs):   # Runs training for 100 epochs\n",
    "\n",
    "    train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler, WARMUP_ACTIVATION_EPOCHS) # âœ… Pass required arguments\n",
    "\n",
    "    test(epoch)  # âœ… Test the model\n",
    "    tqdm.write(\"\")  # âœ… Clear leftover progress bar from test()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best Test Accuracy: \", best_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
