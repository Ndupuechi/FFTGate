Epoch 0: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000'] | Gamma1 Grad: ['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']
Epoch 1: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.23135', '2.23472', '2.24969', '2.22539', '2.22614', '2.22918', '2.19716', '2.21382'] | Gamma1 Grad: ['0.01181', '-0.01615', '0.00013', '0.00877', '0.01392', '0.01021', '0.01733', '-0.03287']
Epoch 2: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.29190', '2.28902', '2.30266', '2.28281', '2.28884', '2.28561', '2.25052', '2.30513'] | Gamma1 Grad: ['0.01038', '-0.01218', '-0.02824', '-0.01060', '0.01808', '0.00258', '0.00516', '-0.06469']
Epoch 3: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00119 | Gamma1: ['2.30476', '2.29521', '2.30815', '2.28298', '2.28203', '2.29525', '2.26641', '2.32035'] | Gamma1 Grad: ['-0.00307', '-0.00995', '-0.01307', '0.01703', '-0.00174', '-0.03785', '0.01883', '0.04925']
Epoch 4: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00099 | Gamma1: ['2.28766', '2.30016', '2.30130', '2.30580', '2.28967', '2.29750', '2.26309', '2.33443'] | Gamma1 Grad: ['0.01737', '-0.00924', '-0.02796', '-0.02377', '-0.00966', '-0.00578', '0.04670', '-0.10372']
Epoch 5: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00076 | Gamma1: ['2.30451', '2.30218', '2.31114', '2.30510', '2.29858', '2.30036', '2.26204', '2.35891'] | Gamma1 Grad: ['0.00353', '-0.00302', '-0.01099', '-0.01016', '-0.02090', '0.01397', '-0.00623', '0.00524']
Epoch 6: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.30112', '2.30557', '2.31016', '2.29333', '2.29493', '2.29589', '2.26091', '2.34849'] | Gamma1 Grad: ['0.02227', '-0.01413', '0.00639', '0.00311', '0.00031', '0.03353', '0.05216', '-0.02170']
Epoch 7: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00032 | Gamma1: ['2.29430', '2.29916', '2.30369', '2.30390', '2.29886', '2.29676', '2.26701', '2.35967'] | Gamma1 Grad: ['0.05424', '0.00992', '0.01245', '0.01659', '0.01962', '-0.03114', '-0.00601', '0.04277']
Epoch 8: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['2.30194', '2.29780', '2.30246', '2.29349', '2.29488', '2.30105', '2.27490', '2.36075'] | Gamma1 Grad: ['0.00853', '-0.01064', '-0.04699', '-0.00063', '-0.00074', '0.07095', '-0.03131', '0.09380']
Epoch 9: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['2.28478', '2.29354', '2.30030', '2.30010', '2.29731', '2.29791', '2.27302', '2.36455'] | Gamma1 Grad: ['-0.01632', '-0.01633', '-0.03703', '-0.09060', '-0.01002', '-0.06581', '-0.05650', '-0.08478']
Epoch 10: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.29547', '2.28941', '2.30211', '2.30046', '2.29758', '2.30214', '2.27360', '2.36702'] | Gamma1 Grad: ['0.01114', '0.00853', '0.00931', '-0.02165', '-0.05000', '-0.05436', '0.08866', '-0.05502']
Epoch 11: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.29848', '2.30107', '2.30840', '2.27763', '2.29165', '2.30311', '2.27683', '2.36735'] | Gamma1 Grad: ['-0.01276', '-0.00846', '-0.01646', '-0.02674', '-0.03052', '0.05389', '-0.03313', '-0.04097']
Epoch 12: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.30142', '2.29066', '2.29709', '2.31265', '2.29726', '2.30403', '2.28602', '2.39772'] | Gamma1 Grad: ['-0.03046', '-0.01163', '0.13467', '0.03040', '-0.04869', '0.05259', '-0.05856', '-0.02545']
Epoch 13: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.28582', '2.30568', '2.29387', '2.29752', '2.29181', '2.29525', '2.27099', '2.38915'] | Gamma1 Grad: ['-0.00814', '-0.01831', '-0.04000', '-0.00064', '-0.01582', '-0.06367', '0.14765', '0.02658']
Epoch 14: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.31171', '2.30335', '2.28676', '2.29571', '2.30579', '2.31061', '2.28121', '2.39380'] | Gamma1 Grad: ['0.00151', '-0.01425', '0.00605', '-0.10287', '-0.04555', '-0.05187', '0.19337', '0.13101']
Epoch 15: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00128 | Gamma1: ['2.29647', '2.28919', '2.29182', '2.29640', '2.27538', '2.31943', '2.28142', '2.40391'] | Gamma1 Grad: ['0.00528', '0.00155', '-0.07663', '0.09684', '-0.00756', '0.05261', '-0.05348', '-0.05881']
Epoch 16: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00119 | Gamma1: ['2.28872', '2.30777', '2.29237', '2.29101', '2.27733', '2.29914', '2.28275', '2.39846'] | Gamma1 Grad: ['0.04284', '-0.00043', '0.04685', '-0.00089', '0.04073', '0.07049', '0.31133', '-0.19721']
Epoch 17: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00109 | Gamma1: ['2.29173', '2.29824', '2.28376', '2.29165', '2.30011', '2.31884', '2.27427', '2.38852'] | Gamma1 Grad: ['-0.11444', '0.10067', '0.12813', '0.38109', '-0.04828', '0.07571', '0.06038', '-0.04106']
Epoch 18: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00099 | Gamma1: ['2.29326', '2.29765', '2.29603', '2.29692', '2.29476', '2.30136', '2.27489', '2.38669'] | Gamma1 Grad: ['-0.06963', '0.03610', '-0.03974', '0.08015', '-0.00184', '0.01474', '0.03423', '-0.00988']
Epoch 19: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['2.28568', '2.29896', '2.27632', '2.28830', '2.30499', '2.29320', '2.29365', '2.39184'] | Gamma1 Grad: ['0.08268', '0.01971', '-0.00590', '0.10635', '-0.08443', '0.04372', '-0.22090', '-0.00841']
Epoch 20: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00076 | Gamma1: ['2.29575', '2.28748', '2.29723', '2.29952', '2.31445', '2.30101', '2.27094', '2.39391'] | Gamma1 Grad: ['-0.00926', '-0.03187', '0.05612', '-0.09820', '0.10487', '0.05979', '-0.01384', '0.06953']
Epoch 21: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00064 | Gamma1: ['2.29138', '2.29720', '2.29236', '2.30776', '2.30225', '2.30446', '2.27830', '2.40510'] | Gamma1 Grad: ['-0.07788', '-0.02842', '0.04068', '-0.01970', '-0.03993', '-0.00690', '0.11138', '-0.03918']
Epoch 22: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.28846', '2.29447', '2.28077', '2.29389', '2.29049', '2.30157', '2.27442', '2.38731'] | Gamma1 Grad: ['-0.00541', '-0.03874', '-0.01509', '0.05644', '0.03600', '-0.00279', '0.00095', '0.15346']
Epoch 23: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00042 | Gamma1: ['2.27797', '2.28925', '2.29475', '2.29483', '2.29393', '2.30180', '2.27693', '2.39034'] | Gamma1 Grad: ['0.01024', '-0.05911', '0.01848', '-0.03218', '0.12553', '0.03172', '-0.14405', '-0.11935']
Epoch 24: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00032 | Gamma1: ['2.29205', '2.30369', '2.28876', '2.29320', '2.28456', '2.29305', '2.28412', '2.39903'] | Gamma1 Grad: ['-0.04049', '-0.06998', '0.07050', '-0.10597', '0.13565', '-0.17648', '0.02476', '0.06020']
Epoch 25: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00023 | Gamma1: ['2.28800', '2.29797', '2.30094', '2.30642', '2.29841', '2.30425', '2.29061', '2.38923'] | Gamma1 Grad: ['-0.00836', '0.03383', '-0.01881', '-0.02341', '-0.04966', '-0.00690', '0.26999', '-0.22427']
Epoch 26: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['2.28479', '2.29551', '2.30513', '2.29343', '2.28940', '2.29135', '2.28111', '2.41367'] | Gamma1 Grad: ['-0.08195', '-0.06722', '0.05698', '0.07460', '0.08287', '0.02277', '-0.04601', '0.17298']
Epoch 27: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.28964', '2.28836', '2.30086', '2.29587', '2.30212', '2.30865', '2.28029', '2.40884'] | Gamma1 Grad: ['-0.01641', '-0.03088', '0.00859', '-0.02863', '0.00670', '0.08039', '-0.18198', '-0.00989']
Epoch 28: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['2.28925', '2.28992', '2.28552', '2.30244', '2.28828', '2.29687', '2.28941', '2.41916'] | Gamma1 Grad: ['-0.12293', '-0.02920', '-0.05987', '-0.26535', '-0.26014', '0.15549', '0.13766', '-0.00397']
Epoch 29: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00002 | Gamma1: ['2.28627', '2.28362', '2.29536', '2.29930', '2.30133', '2.29562', '2.28678', '2.40618'] | Gamma1 Grad: ['0.06213', '0.06681', '-0.02099', '-0.17420', '-0.01405', '-0.08756', '-0.10990', '-0.04007']
Epoch 30: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.29670', '2.29668', '2.29011', '2.30569', '2.29409', '2.30715', '2.28320', '2.40674'] | Gamma1 Grad: ['0.02893', '0.01096', '-0.02060', '0.02857', '-0.02313', '0.01649', '-0.01506', '-0.02237']
Epoch 31: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.27816', '2.27813', '2.30089', '2.29588', '2.27862', '2.31050', '2.30489', '2.42354'] | Gamma1 Grad: ['-0.02984', '0.03481', '0.05117', '0.06801', '0.03104', '-0.00982', '0.02948', '0.00074']
Epoch 32: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.28164', '2.27073', '2.30406', '2.28809', '2.31317', '2.29908', '2.30466', '2.42663'] | Gamma1 Grad: ['-0.03321', '0.07320', '-0.19334', '0.11394', '0.05094', '-0.16699', '0.03976', '0.08675']
Epoch 33: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.27374', '2.29152', '2.29776', '2.29971', '2.29352', '2.28146', '2.28871', '2.41991'] | Gamma1 Grad: ['0.00606', '-0.01771', '-0.01613', '-0.05174', '-0.04824', '-0.08234', '-0.02789', '0.17043']
Epoch 34: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.29519', '2.29818', '2.29836', '2.32093', '2.29632', '2.29981', '2.29069', '2.41753'] | Gamma1 Grad: ['0.03010', '-0.01946', '-0.06941', '-0.01799', '0.01117', '-0.06399', '0.07794', '-0.11550']
Epoch 35: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00144 | Gamma1: ['2.28768', '2.28922', '2.29302', '2.29266', '2.28510', '2.29648', '2.27458', '2.40425'] | Gamma1 Grad: ['-0.04107', '-0.01752', '0.01357', '0.00357', '0.04433', '0.05332', '-0.02812', '-0.03658']
Epoch 36: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['2.29807', '2.31220', '2.28756', '2.28419', '2.29254', '2.29523', '2.27822', '2.43225'] | Gamma1 Grad: ['0.00334', '-0.01921', '0.04313', '-0.01304', '-0.00253', '0.07262', '-0.06197', '-0.09053']
Epoch 37: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00139 | Gamma1: ['2.29834', '2.29346', '2.29499', '2.30280', '2.29340', '2.29276', '2.28227', '2.40877'] | Gamma1 Grad: ['-0.01586', '0.04481', '0.07478', '-0.24398', '-0.18239', '-0.28176', '0.16882', '-0.02661']
Epoch 38: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.29254', '2.30247', '2.28889', '2.28686', '2.28558', '2.29877', '2.29149', '2.41230'] | Gamma1 Grad: ['0.01325', '0.00681', '0.03960', '0.02357', '-0.03796', '0.02904', '-0.05636', '-0.09636']
Epoch 39: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00132 | Gamma1: ['2.27627', '2.29476', '2.28194', '2.30042', '2.31622', '2.29704', '2.30639', '2.42119'] | Gamma1 Grad: ['0.02146', '-0.00863', '0.01104', '0.03454', '0.01149', '-0.04597', '-0.02225', '0.05684']
Epoch 40: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00128 | Gamma1: ['2.28311', '2.28337', '2.28104', '2.30661', '2.29582', '2.28021', '2.30628', '2.59886'] | Gamma1 Grad: ['-0.05449', '-0.01897', '-0.02894', '0.07574', '0.21311', '0.08550', '0.09140', '-0.19572']
Epoch 41: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00124 | Gamma1: ['2.28380', '2.29117', '2.29697', '2.30196', '2.25325', '2.32359', '2.30608', '2.64806'] | Gamma1 Grad: ['-0.07814', '-0.03500', '-0.27012', '-0.04670', '0.10114', '-0.04411', '0.22313', '-0.37177']
Epoch 42: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00119 | Gamma1: ['2.27601', '2.28793', '2.30665', '2.28792', '2.30522', '2.30067', '2.31124', '2.67038'] | Gamma1 Grad: ['-0.05503', '-0.04272', '0.16877', '-0.06095', '0.56321', '0.18918', '0.29023', '-0.04196']
Epoch 43: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00114 | Gamma1: ['2.29386', '2.29274', '2.27729', '2.31109', '2.30980', '2.30599', '2.30104', '2.67689'] | Gamma1 Grad: ['0.03345', '-0.00467', '-0.07372', '-0.05959', '-0.04405', '-0.11633', '0.22083', '0.12041']
Epoch 44: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00109 | Gamma1: ['2.28403', '2.28560', '2.29792', '2.31179', '2.29375', '2.29113', '2.29254', '2.67679'] | Gamma1 Grad: ['-0.02894', '-0.03409', '0.04176', '-0.17672', '0.70000', '-0.09885', '0.02763', '-0.02700']
Epoch 45: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00104 | Gamma1: ['2.25425', '2.30091', '2.30245', '2.28331', '2.28058', '2.30842', '2.28624', '2.69643'] | Gamma1 Grad: ['-0.04022', '0.03194', '0.15327', '-0.05597', '-0.16138', '0.29006', '0.09420', '-0.24505']
Epoch 46: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00099 | Gamma1: ['2.28346', '2.28015', '2.27180', '2.29068', '2.29813', '2.29810', '2.29428', '2.73006'] | Gamma1 Grad: ['-0.02361', '-0.00521', '-0.02407', '-0.13792', '-0.01345', '-0.22120', '-0.05730', '0.22985']
Epoch 47: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00093 | Gamma1: ['2.28561', '2.28092', '2.29439', '2.29450', '2.28317', '2.29601', '2.28538', '2.70725'] | Gamma1 Grad: ['0.20431', '0.03482', '0.34388', '0.70000', '0.61140', '-0.18755', '0.11688', '-0.00364']
Epoch 48: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['2.25880', '2.31365', '2.30180', '2.31359', '2.30383', '2.32848', '2.30321', '2.71355'] | Gamma1 Grad: ['0.02806', '-0.00196', '0.02838', '-0.04110', '0.02067', '0.12293', '-0.12395', '0.04536']
Epoch 49: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00081 | Gamma1: ['2.27092', '2.29635', '2.31200', '2.30744', '2.28272', '2.30852', '2.28980', '2.73321'] | Gamma1 Grad: ['-0.06664', '0.08028', '0.02190', '-0.02759', '0.17730', '0.02548', '-0.47956', '0.29640']
Epoch 50: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00076 | Gamma1: ['2.29794', '2.28845', '2.27839', '2.32310', '2.29647', '2.29209', '2.31324', '2.71380'] | Gamma1 Grad: ['0.06078', '-0.01254', '0.03159', '-0.17074', '0.01505', '-0.01014', '-0.11938', '0.21344']
Epoch 51: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00070 | Gamma1: ['2.28461', '2.29257', '2.27469', '2.31231', '2.26422', '2.29101', '2.31442', '2.73194'] | Gamma1 Grad: ['-0.04082', '-0.01204', '0.04732', '-0.04718', '0.04221', '0.09406', '-0.13158', '-0.13668']
Epoch 52: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00064 | Gamma1: ['2.30088', '2.29257', '2.29894', '2.30161', '2.28471', '2.30309', '2.32942', '2.72522'] | Gamma1 Grad: ['-0.03410', '-0.06791', '0.16588', '-0.18270', '-0.07299', '-0.02799', '0.56618', '-0.06002']
Epoch 53: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00058 | Gamma1: ['2.29149', '2.30887', '2.30672', '2.28280', '2.27942', '2.29802', '2.32239', '2.73997'] | Gamma1 Grad: ['-0.01712', '0.05188', '-0.09013', '0.12994', '-0.08629', '-0.05308', '0.38832', '0.02826']
Epoch 54: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.27915', '2.29840', '2.29748', '2.31058', '2.30496', '2.31106', '2.33183', '2.72989'] | Gamma1 Grad: ['-0.02396', '-0.02502', '0.09262', '0.03451', '-0.13227', '-0.04947', '0.23380', '0.10928']
Epoch 55: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00047 | Gamma1: ['2.25597', '2.30864', '2.30189', '2.30742', '2.30121', '2.29695', '2.31108', '2.72862'] | Gamma1 Grad: ['0.01086', '-0.00209', '-0.01489', '0.10449', '-0.04307', '0.00235', '-0.06276', '0.00351']
Epoch 56: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00042 | Gamma1: ['2.27990', '2.26325', '2.30094', '2.32015', '2.30679', '2.29608', '2.30869', '2.73964'] | Gamma1 Grad: ['0.03436', '-0.34674', '-0.70000', '-0.70000', '-0.23396', '-0.35036', '0.13289', '-0.10449']
Epoch 57: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00037 | Gamma1: ['2.28330', '2.28572', '2.28131', '2.31902', '2.29882', '2.29028', '2.33037', '2.75364'] | Gamma1 Grad: ['-0.00491', '0.00187', '-0.00259', '0.00000', '0.02199', '0.00135', '-0.03858', '0.02294']
Epoch 58: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00032 | Gamma1: ['2.26612', '2.30072', '2.27423', '2.29641', '2.28917', '2.28498', '2.31739', '2.75566'] | Gamma1 Grad: ['-0.01383', '-0.00046', '0.00776', '-0.00497', '0.04309', '-0.01962', '-0.05725', '-0.00099']
Epoch 59: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00027 | Gamma1: ['2.27358', '2.29336', '2.26232', '2.30920', '2.28836', '2.28386', '2.31561', '2.75217'] | Gamma1 Grad: ['0.02220', '0.01699', '-0.09862', '-0.06884', '-0.22488', '-0.21186', '0.23203', '-0.07243']
Epoch 60: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00023 | Gamma1: ['2.27467', '2.30955', '2.27753', '2.31574', '2.31092', '2.26888', '2.31759', '2.83317'] | Gamma1 Grad: ['0.05218', '0.02545', '0.06871', '-0.00354', '0.06628', '0.13120', '-0.18725', '0.14105']
Epoch 61: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.26205', '2.31424', '2.28750', '2.32250', '2.31708', '2.27680', '2.34118', '2.86659'] | Gamma1 Grad: ['0.01713', '0.02380', '0.01599', '0.13201', '-0.08344', '0.02400', '-0.01843', '0.15263']
Epoch 62: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['2.25080', '2.30479', '2.27805', '2.30921', '2.31355', '2.27495', '2.36080', '2.89569'] | Gamma1 Grad: ['-0.00341', '-0.00383', '-0.00232', '0.00251', '0.01130', '0.01029', '0.01138', '0.01298']
Epoch 63: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00012 | Gamma1: ['2.25101', '2.30182', '2.27410', '2.32859', '2.30801', '2.29113', '2.34297', '2.89773'] | Gamma1 Grad: ['0.03779', '-0.00450', '-0.04470', '-0.04556', '0.06817', '-0.01684', '0.05290', '0.08679']
Epoch 64: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['2.27720', '2.31740', '2.26579', '2.31823', '2.29180', '2.28566', '2.33727', '2.90512'] | Gamma1 Grad: ['0.06608', '0.02451', '0.20875', '0.04063', '0.09734', '-0.12431', '-0.05693', '0.10248']
Epoch 65: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.27944', '2.30680', '2.25658', '2.32168', '2.31011', '2.27840', '2.35267', '2.94004'] | Gamma1 Grad: ['0.03049', '-0.08347', '0.02070', '-0.00557', '-0.33501', '-0.19860', '0.23026', '0.04164']
Epoch 66: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['2.27928', '2.31909', '2.24935', '2.32393', '2.30741', '2.30025', '2.37150', '2.95096'] | Gamma1 Grad: ['0.02003', '0.06485', '-0.08149', '-0.01709', '0.14410', '-0.14886', '-0.07797', '-0.03176']
Epoch 67: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00003 | Gamma1: ['2.27043', '2.32869', '2.27211', '2.31898', '2.32114', '2.28565', '2.37266', '2.96864'] | Gamma1 Grad: ['-0.00771', '0.00748', '0.02577', '0.00978', '-0.00091', '0.00926', '0.01636', '0.00484']
Epoch 68: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00002 | Gamma1: ['2.27363', '2.31052', '2.26313', '2.31745', '2.31002', '2.28993', '2.37628', '2.96947'] | Gamma1 Grad: ['-0.01260', '-0.03429', '-0.08174', '0.07820', '-0.36829', '-0.25750', '-0.23014', '-0.04917']
Epoch 69: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00001 | Gamma1: ['2.26548', '2.28774', '2.28615', '2.31960', '2.32994', '2.27764', '2.37254', '2.97675'] | Gamma1 Grad: ['0.16564', '0.00710', '0.07976', '-0.21048', '-0.27202', '0.03859', '0.31545', '-0.18557']
Epoch 70: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.27883', '2.26893', '2.26662', '2.32025', '2.32393', '2.29335', '2.36376', '2.99350'] | Gamma1 Grad: ['-0.02486', '0.03178', '0.00262', '-0.02323', '-0.05787', '0.04487', '0.01499', '-0.04085']
Epoch 71: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.29616', '2.30773', '2.26633', '2.32524', '2.24836', '2.31391', '2.36243', '3.00574'] | Gamma1 Grad: ['-0.01913', '0.01138', '0.02035', '0.02257', '0.21197', '0.08631', '-0.00898', '-0.05645']
Epoch 72: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['2.21238', '2.27141', '2.24915', '2.31953', '2.32122', '2.28661', '2.39009', '3.03089'] | Gamma1 Grad: ['0.02926', '-0.08343', '0.04949', '0.17481', '-0.18446', '0.19063', '-0.13697', '0.25205']
Epoch 73: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.28491', '2.30214', '2.28841', '2.31301', '2.31180', '2.25988', '2.36146', '3.04725'] | Gamma1 Grad: ['-0.01522', '0.02687', '0.00369', '-0.01508', '0.01335', '-0.14476', '0.11964', '-0.08655']
Epoch 74: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.27192', '2.32183', '2.30281', '2.32217', '2.31965', '2.28385', '2.37027', '3.01057'] | Gamma1 Grad: ['0.02057', '0.03285', '-0.02133', '0.02887', '-0.10494', '-0.04625', '-0.09580', '0.09777']
Epoch 75: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['2.25396', '2.28157', '2.30103', '2.30841', '2.29698', '2.28510', '2.37959', '3.04749'] | Gamma1 Grad: ['0.00112', '0.04286', '-0.05130', '0.07835', '0.11428', '0.11477', '0.03372', '0.03631']
Epoch 76: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['2.25490', '2.29633', '2.27377', '2.31851', '2.31537', '2.28331', '2.35471', '3.02678'] | Gamma1 Grad: ['0.11053', '-0.13113', '0.12111', '0.34367', '-0.70000', '-0.60577', '-0.10413', '-0.23464']
Epoch 77: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00147 | Gamma1: ['2.27667', '2.31059', '2.25659', '2.33907', '2.31230', '2.27002', '2.34993', '3.06308'] | Gamma1 Grad: ['-0.04388', '0.05959', '-0.10533', '0.26596', '-0.13651', '0.16007', '-0.13397', '0.19085']
Epoch 78: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['2.26214', '2.30186', '2.28647', '2.29986', '2.33630', '2.23384', '2.38016', '2.98894'] | Gamma1 Grad: ['0.00260', '-0.04983', '-0.03475', '-0.06791', '0.09034', '-0.01566', '0.19625', '-0.27753']
Epoch 79: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00145 | Gamma1: ['2.25563', '2.27133', '2.28938', '2.32227', '2.32552', '2.26699', '2.38558', '3.05554'] | Gamma1 Grad: ['0.00700', '0.02955', '-0.00346', '0.00562', '0.01304', '-0.04774', '-0.03497', '-0.02780']
Epoch 80: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00144 | Gamma1: ['2.27623', '2.30121', '2.27211', '2.33442', '2.29509', '2.25054', '2.37844', '2.93319'] | Gamma1 Grad: ['-0.00044', '-0.02117', '0.10106', '-0.09231', '-0.18373', '-0.14333', '0.09465', '0.06542']
Epoch 81: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00143 | Gamma1: ['2.29932', '2.29686', '2.27543', '2.31411', '2.28697', '2.26774', '2.35754', '2.80362'] | Gamma1 Grad: ['-0.01103', '-0.03064', '0.03449', '-0.04400', '0.02447', '0.07806', '0.06196', '0.04895']
Epoch 82: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00142 | Gamma1: ['2.28220', '2.29071', '2.26794', '2.30907', '2.27713', '2.26132', '2.33647', '2.69479'] | Gamma1 Grad: ['0.07111', '0.03795', '-0.11365', '0.01247', '-0.00330', '-0.14583', '-0.00591', '0.10904']
Epoch 83: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00141 | Gamma1: ['2.28610', '2.30051', '2.26161', '2.31414', '2.27145', '2.24392', '2.31096', '2.66518'] | Gamma1 Grad: ['0.00516', '0.00035', '0.00208', '-0.00131', '-0.01183', '-0.00348', '-0.00341', '0.00694']
Epoch 84: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00139 | Gamma1: ['2.29220', '2.29400', '2.27651', '2.30386', '2.28910', '2.26768', '2.28615', '2.60882'] | Gamma1 Grad: ['-0.00249', '0.00881', '0.02506', '-0.00126', '0.02163', '-0.05209', '-0.01808', '0.00595']
Epoch 85: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00137 | Gamma1: ['2.27381', '2.29375', '2.30011', '2.30179', '2.29386', '2.23293', '2.31030', '2.54686'] | Gamma1 Grad: ['0.02075', '0.11249', '0.04520', '-0.15960', '-0.14497', '0.02209', '0.06352', '0.11035']
Epoch 86: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00136 | Gamma1: ['2.28063', '2.28306', '2.29746', '2.32076', '2.27174', '2.23028', '2.30236', '2.49404'] | Gamma1 Grad: ['-0.01133', '-0.04604', '0.00033', '-0.06590', '-0.04335', '0.05530', '-0.08348', '-0.05142']
Epoch 87: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00134 | Gamma1: ['2.28771', '2.30779', '2.27724', '2.31183', '2.24681', '2.26341', '2.31685', '2.49179'] | Gamma1 Grad: ['0.01584', '0.01088', '-0.00166', '-0.00983', '-0.00703', '-0.00881', '0.02040', '-0.00079']
Epoch 88: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00132 | Gamma1: ['2.30917', '2.29313', '2.28843', '2.31068', '2.27515', '2.27923', '2.28329', '2.46118'] | Gamma1 Grad: ['0.00391', '0.02662', '-0.13249', '0.16473', '0.14300', '-0.07485', '-0.00509', '-0.09143']
Epoch 89: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00130 | Gamma1: ['2.29291', '2.28670', '2.29789', '2.29794', '2.24864', '2.26512', '2.27526', '2.42524'] | Gamma1 Grad: ['0.05717', '0.01436', '-0.02121', '-0.03556', '0.13409', '0.01513', '0.05493', '-0.06075']
Epoch 90: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00128 | Gamma1: ['2.30983', '2.28007', '2.28396', '2.28076', '2.24326', '2.30055', '2.28203', '2.44509'] | Gamma1 Grad: ['0.00047', '-0.00002', '-0.00007', '-0.00075', '-0.00094', '0.00119', '0.00031', '0.00280']
Epoch 91: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00126 | Gamma1: ['2.31362', '2.30108', '2.28019', '2.27379', '2.27965', '2.29680', '2.28967', '2.43628'] | Gamma1 Grad: ['-0.01115', '-0.05359', '0.02869', '0.09362', '-0.12081', '0.16210', '0.12712', '-0.05167']
Epoch 92: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00124 | Gamma1: ['2.31933', '2.31258', '2.28757', '2.31009', '2.27103', '2.29223', '2.28496', '2.42321'] | Gamma1 Grad: ['0.00175', '0.00529', '0.00301', '0.01135', '-0.00820', '0.01918', '-0.02805', '0.00587']
Epoch 93: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00122 | Gamma1: ['2.31785', '2.30849', '2.25581', '2.27801', '2.26230', '2.28795', '2.27783', '2.42496'] | Gamma1 Grad: ['0.00047', '0.00054', '-0.00150', '-0.00098', '0.00170', '-0.00164', '-0.00355', '0.00469']
Epoch 94: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00119 | Gamma1: ['2.29818', '2.30777', '2.27949', '2.28474', '2.27684', '2.28813', '2.28047', '2.41179'] | Gamma1 Grad: ['0.00759', '0.00401', '0.00200', '0.00987', '0.01838', '-0.02313', '0.02124', '0.06997']
Epoch 95: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00117 | Gamma1: ['2.29394', '2.30002', '2.28631', '2.29900', '2.26056', '2.27584', '2.28429', '2.38673'] | Gamma1 Grad: ['0.00120', '0.00106', '-0.00096', '-0.00046', '0.00008', '0.00048', '-0.00294', '-0.00060']
Epoch 96: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00114 | Gamma1: ['2.27695', '2.29377', '2.26715', '2.29523', '2.26426', '2.25892', '2.29337', '2.38440'] | Gamma1 Grad: ['0.03205', '-0.01091', '-0.04592', '-0.02322', '-0.03023', '-0.07075', '-0.00809', '0.02738']
Epoch 97: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00112 | Gamma1: ['2.29144', '2.28781', '2.31117', '2.29549', '2.27485', '2.27600', '2.28298', '2.41384'] | Gamma1 Grad: ['-0.01059', '-0.00419', '0.02029', '-0.00614', '-0.00684', '-0.01544', '0.01742', '-0.00384']
Epoch 98: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00109 | Gamma1: ['2.31308', '2.28539', '2.28281', '2.29932', '2.28823', '2.28852', '2.27143', '2.39445'] | Gamma1 Grad: ['0.00092', '0.00265', '-0.00280', '-0.00127', '-0.00013', '0.00927', '-0.00238', '-0.00238']
Epoch 99: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00107 | Gamma1: ['2.30085', '2.31547', '2.30008', '2.28590', '2.28295', '2.29952', '2.27631', '2.38983'] | Gamma1 Grad: ['-0.00278', '0.00038', '0.00027', '0.00063', '0.00560', '0.00171', '0.00507', '0.00543']
