Epoch 0: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000'] | Gamma1 Grad: ['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']
Epoch 1: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00244 | Gamma1: ['2.19609', '2.19957', '2.20496', '2.19457', '2.19599', '2.19489', '2.18250', '2.18916'] | Gamma1 Grad: ['-0.00741', '-0.00281', '-0.00944', '-0.00480', '-0.00889', '-0.00556', '0.01464', '0.00021']
Epoch 2: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00226 | Gamma1: ['2.27312', '2.28002', '2.28873', '2.27233', '2.27663', '2.27561', '2.26207', '2.27252'] | Gamma1 Grad: ['-0.00027', '0.00302', '-0.01786', '-0.00605', '-0.00113', '0.00459', '-0.00012', '-0.00406']
Epoch 3: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00199 | Gamma1: ['2.28008', '2.28817', '2.29141', '2.28098', '2.28192', '2.28244', '2.27652', '2.27946'] | Gamma1 Grad: ['0.00804', '0.00395', '0.00299', '-0.00312', '-0.00219', '0.00354', '0.01091', '0.00928']
Epoch 4: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00164 | Gamma1: ['2.28074', '2.28604', '2.29322', '2.28037', '2.28040', '2.28595', '2.28205', '2.28513'] | Gamma1 Grad: ['-0.00733', '-0.01056', '-0.00721', '0.01363', '0.00784', '0.02062', '0.01390', '0.00696']
Epoch 5: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00126 | Gamma1: ['2.28334', '2.28527', '2.28913', '2.28144', '2.28266', '2.28366', '2.27867', '2.28933'] | Gamma1 Grad: ['0.00177', '-0.00519', '0.00671', '0.00553', '-0.00068', '-0.00337', '-0.00040', '-0.00648']
Epoch 6: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['2.27825', '2.28825', '2.28779', '2.28547', '2.27982', '2.28033', '2.27613', '2.28761'] | Gamma1 Grad: ['0.00515', '0.01551', '0.00307', '-0.00367', '-0.00096', '-0.00031', '-0.00853', '0.00850']
Epoch 7: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.27936', '2.28374', '2.28654', '2.28120', '2.28462', '2.28579', '2.27868', '2.29001'] | Gamma1 Grad: ['0.00187', '-0.00263', '0.01164', '-0.00639', '-0.00050', '0.00509', '-0.00346', '-0.00269']
Epoch 8: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00025 | Gamma1: ['2.28196', '2.28303', '2.28299', '2.28612', '2.28155', '2.28463', '2.28338', '2.28710'] | Gamma1 Grad: ['-0.00154', '0.00523', '0.00692', '0.00286', '0.00573', '0.00461', '-0.01028', '0.00470']
Epoch 9: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.28137', '2.28205', '2.28846', '2.28746', '2.28315', '2.28545', '2.28348', '2.28595'] | Gamma1 Grad: ['0.00312', '0.00161', '0.01041', '0.00188', '0.00334', '-0.00261', '0.00245', '-0.00756']
Epoch 10: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.27771', '2.28436', '2.28436', '2.28620', '2.28023', '2.28514', '2.28103', '2.28607'] | Gamma1 Grad: ['0.00480', '-0.00359', '-0.02357', '-0.00363', '-0.00348', '-0.00778', '0.00237', '-0.00295']
Epoch 11: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00248 | Gamma1: ['2.28457', '2.27897', '2.28453', '2.28708', '2.28494', '2.28294', '2.27934', '2.28511'] | Gamma1 Grad: ['0.00180', '0.00214', '0.00157', '0.00852', '-0.00183', '-0.00017', '0.01457', '0.00003']
Epoch 12: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00244 | Gamma1: ['2.29209', '2.27762', '2.28290', '2.27979', '2.28070', '2.28605', '2.27970', '2.29053'] | Gamma1 Grad: ['0.00194', '0.00584', '0.01141', '0.00501', '-0.00091', '0.00351', '-0.00492', '0.00562']
Epoch 13: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00236 | Gamma1: ['2.27824', '2.28580', '2.27971', '2.29071', '2.28554', '2.29150', '2.27709', '2.29661'] | Gamma1 Grad: ['-0.00046', '0.01614', '0.00435', '-0.01313', '0.01261', '-0.00665', '-0.01336', '-0.00057']
Epoch 14: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00226 | Gamma1: ['2.27521', '2.28582', '2.27547', '2.27263', '2.27694', '2.29963', '2.28744', '2.27519'] | Gamma1 Grad: ['-0.00061', '-0.00166', '-0.00774', '-0.00382', '0.01071', '-0.00903', '0.00744', '0.00608']
Epoch 15: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00214 | Gamma1: ['2.26945', '2.26893', '2.27901', '2.29139', '2.27904', '2.28695', '2.27519', '2.27903'] | Gamma1 Grad: ['0.00147', '0.00487', '-0.01087', '0.01925', '-0.00031', '0.00539', '0.02254', '-0.00490']
Epoch 16: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00199 | Gamma1: ['2.26911', '2.28694', '2.28658', '2.27196', '2.28088', '2.28862', '2.28194', '2.28021'] | Gamma1 Grad: ['-0.00294', '-0.00268', '0.00700', '-0.00731', '0.00599', '-0.00327', '0.01450', '0.00015']
Epoch 17: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00182 | Gamma1: ['2.27818', '2.28334', '2.27623', '2.28158', '2.27920', '2.27823', '2.29415', '2.28394'] | Gamma1 Grad: ['0.00147', '-0.00107', '-0.00853', '-0.00208', '-0.00270', '-0.01146', '0.00162', '0.00325']
Epoch 18: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00164 | Gamma1: ['2.28546', '2.28862', '2.27686', '2.28171', '2.27021', '2.28984', '2.27461', '2.29269'] | Gamma1 Grad: ['0.00171', '0.00130', '-0.00558', '-0.00262', '0.00057', '-0.00278', '0.01315', '0.00805']
Epoch 19: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00145 | Gamma1: ['2.27897', '2.29088', '2.29131', '2.28956', '2.28821', '2.28523', '2.28661', '2.29359'] | Gamma1 Grad: ['0.00750', '0.00008', '0.00458', '-0.00487', '-0.01697', '0.01593', '0.02820', '-0.00785']
Epoch 20: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00126 | Gamma1: ['2.27608', '2.28292', '2.29604', '2.28900', '2.28783', '2.28567', '2.28677', '2.28266'] | Gamma1 Grad: ['-0.00616', '-0.00325', '0.00514', '-0.00481', '0.00981', '0.02519', '-0.01494', '0.00281']
Epoch 21: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00106 | Gamma1: ['2.27615', '2.28306', '2.28207', '2.29115', '2.28421', '2.28545', '2.28971', '2.27726'] | Gamma1 Grad: ['-0.00821', '-0.00566', '0.00688', '-0.00589', '-0.01389', '-0.00033', '0.00085', '-0.00192']
Epoch 22: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['2.28159', '2.27872', '2.28257', '2.28209', '2.28349', '2.28364', '2.28202', '2.28007'] | Gamma1 Grad: ['0.00031', '-0.00354', '0.00298', '-0.00798', '-0.00066', '0.00166', '0.00057', '-0.00612']
Epoch 23: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00069 | Gamma1: ['2.28306', '2.28131', '2.28559', '2.27898', '2.28654', '2.28446', '2.28175', '2.28373'] | Gamma1 Grad: ['0.01074', '0.00621', '-0.01574', '0.00864', '-0.00840', '0.00459', '-0.00457', '0.00329']
Epoch 24: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.28160', '2.27961', '2.28263', '2.28548', '2.28271', '2.29673', '2.28226', '2.28153'] | Gamma1 Grad: ['-0.00248', '0.00431', '0.04474', '-0.03670', '-0.01473', '-0.02535', '0.01124', '-0.01103']
Epoch 25: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00037 | Gamma1: ['2.28387', '2.27928', '2.27804', '2.28317', '2.28469', '2.28618', '2.28091', '2.29069'] | Gamma1 Grad: ['-0.00677', '-0.00690', '0.02268', '0.02219', '-0.00365', '0.00441', '-0.01214', '0.00237']
Epoch 26: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00025 | Gamma1: ['2.27892', '2.27652', '2.27588', '2.27866', '2.28298', '2.28430', '2.28834', '2.28515'] | Gamma1 Grad: ['0.00356', '0.00742', '0.02103', '0.00452', '0.02247', '-0.02597', '0.00800', '0.00149']
Epoch 27: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['2.28724', '2.27641', '2.28516', '2.28699', '2.27742', '2.27915', '2.28344', '2.28257'] | Gamma1 Grad: ['-0.00790', '-0.01574', '-0.01035', '0.01048', '0.00599', '-0.00430', '0.00971', '-0.00064']
Epoch 28: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.28467', '2.28206', '2.28064', '2.27966', '2.28192', '2.28632', '2.28464', '2.28709'] | Gamma1 Grad: ['-0.00283', '-0.01668', '-0.04683', '-0.00255', '-0.02173', '0.01956', '0.01028', '-0.00398']
Epoch 29: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00003 | Gamma1: ['2.28380', '2.28892', '2.28288', '2.27508', '2.28295', '2.28632', '2.28987', '2.28229'] | Gamma1 Grad: ['-0.00052', '-0.00124', '-0.00262', '0.01534', '-0.01629', '-0.01079', '0.00998', '-0.00507']
Epoch 30: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.27217', '2.27609', '2.27885', '2.28536', '2.28559', '2.28430', '2.28388', '2.28056'] | Gamma1 Grad: ['-0.00293', '-0.00458', '-0.00023', '-0.00813', '0.01143', '0.00300', '0.00380', '0.00465']
Epoch 31: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.26936', '2.28426', '2.27744', '2.28326', '2.29729', '2.28617', '2.29708', '2.28914'] | Gamma1 Grad: ['-0.01726', '-0.00051', '-0.01302', '0.02857', '-0.00106', '-0.00395', '-0.02709', '-0.00941']
Epoch 32: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00248 | Gamma1: ['2.27392', '2.27635', '2.26759', '2.28506', '2.30258', '2.26849', '2.29084', '2.28322'] | Gamma1 Grad: ['0.01028', '0.00091', '0.00888', '0.03617', '0.00289', '-0.03689', '0.00148', '-0.00965']
Epoch 33: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00247 | Gamma1: ['2.28208', '2.27918', '2.26963', '2.27774', '2.29028', '2.27433', '2.28136', '2.27174'] | Gamma1 Grad: ['0.00385', '0.00234', '0.00188', '-0.00603', '-0.03025', '0.00947', '-0.00298', '0.00203']
Epoch 34: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00244 | Gamma1: ['2.28753', '2.28948', '2.28246', '2.28132', '2.25792', '2.27404', '2.27943', '2.27135'] | Gamma1 Grad: ['0.00273', '0.00258', '0.00547', '-0.00181', '-0.00464', '0.00205', '-0.00130', '-0.00050']
Epoch 35: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00241 | Gamma1: ['2.27851', '2.27595', '2.27604', '2.29238', '2.28160', '2.27694', '2.28180', '2.27755'] | Gamma1 Grad: ['0.00011', '-0.00554', '0.00456', '-0.00441', '0.03653', '-0.00327', '-0.00215', '0.00804']
Epoch 36: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00236 | Gamma1: ['2.28297', '2.28216', '2.29484', '2.26531', '2.30974', '2.28330', '2.27903', '2.28132'] | Gamma1 Grad: ['0.00278', '0.01152', '0.01869', '0.02351', '-0.03471', '-0.01925', '0.02218', '-0.00148']
Epoch 37: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00232 | Gamma1: ['2.27349', '2.28602', '2.28124', '2.28454', '2.29693', '2.29798', '2.29459', '2.28544'] | Gamma1 Grad: ['0.00780', '0.00679', '0.02755', '0.02800', '0.00717', '-0.00197', '0.00599', '-0.01156']
Epoch 38: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00226 | Gamma1: ['2.28501', '2.28201', '2.28215', '2.28487', '2.27557', '2.28266', '2.28864', '2.29291'] | Gamma1 Grad: ['0.00154', '-0.00348', '-0.00492', '0.01710', '0.00731', '0.00914', '-0.00023', '0.00214']
Epoch 39: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00220 | Gamma1: ['2.27618', '2.28962', '2.27638', '2.27979', '2.30678', '2.27761', '2.29085', '2.26799'] | Gamma1 Grad: ['-0.00514', '-0.00610', '0.02867', '-0.00984', '-0.02613', '0.01753', '0.01331', '-0.00089']
Epoch 40: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00214 | Gamma1: ['2.27217', '2.27911', '2.28713', '2.29071', '2.30012', '2.29142', '2.30250', '2.28823'] | Gamma1 Grad: ['-0.00094', '-0.00269', '0.00099', '0.00743', '-0.01120', '0.01205', '-0.00271', '0.00138']
Epoch 41: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00206 | Gamma1: ['2.25465', '2.26918', '2.29424', '2.29572', '2.29925', '2.29382', '2.30833', '2.25259'] | Gamma1 Grad: ['-0.00065', '0.00205', '-0.00295', '0.01518', '0.01110', '-0.00043', '-0.00347', '0.00082']
Epoch 42: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00199 | Gamma1: ['2.26897', '2.26985', '2.29653', '2.27228', '2.27420', '2.29337', '2.30099', '2.26747'] | Gamma1 Grad: ['0.00145', '0.00232', '-0.00149', '-0.00932', '0.00365', '0.00156', '-0.00749', '0.00088']
Epoch 43: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00191 | Gamma1: ['2.24837', '2.27053', '2.29257', '2.26392', '2.27337', '2.27959', '2.30932', '2.30129'] | Gamma1 Grad: ['-0.01574', '0.00180', '0.01123', '-0.01284', '-0.05683', '0.01131', '-0.00071', '0.00687']
Epoch 44: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00182 | Gamma1: ['2.26048', '2.24302', '2.27007', '2.27741', '2.28222', '2.29031', '2.32352', '2.28448'] | Gamma1 Grad: ['0.00248', '-0.00141', '0.00179', '-0.00185', '-0.00638', '-0.00363', '0.00240', '0.00049']
Epoch 45: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00173 | Gamma1: ['2.29080', '2.25370', '2.28630', '2.28300', '2.28269', '2.30930', '2.30834', '2.28281'] | Gamma1 Grad: ['-0.00285', '-0.00215', '-0.01104', '0.00421', '0.00721', '0.00378', '0.00417', '-0.00076']
Epoch 46: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00164 | Gamma1: ['2.26747', '2.25711', '2.26291', '2.28728', '2.28187', '2.28934', '2.30956', '2.26961'] | Gamma1 Grad: ['0.00019', '-0.00069', '-0.00524', '0.00259', '-0.00008', '0.00269', '0.00493', '-0.00168']
Epoch 47: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00155 | Gamma1: ['2.27621', '2.27771', '2.26376', '2.27955', '2.29478', '2.29191', '2.31231', '2.27681'] | Gamma1 Grad: ['-0.00063', '-0.00115', '-0.00776', '0.00375', '0.01344', '-0.01002', '0.00842', '-0.00035']
Epoch 48: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00145 | Gamma1: ['2.27337', '2.28289', '2.26895', '2.25738', '2.29396', '2.29262', '2.31539', '2.27496'] | Gamma1 Grad: ['0.00032', '-0.00053', '0.00056', '-0.00366', '0.00025', '0.00098', '0.00456', '-0.00083']
Epoch 49: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00135 | Gamma1: ['2.27963', '2.28460', '2.28498', '2.28191', '2.29350', '2.29931', '2.29531', '2.27606'] | Gamma1 Grad: ['0.00561', '-0.01276', '0.00903', '0.01713', '-0.02264', '0.00033', '-0.00589', '-0.00585']
Epoch 50: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00126 | Gamma1: ['2.28230', '2.28461', '2.28522', '2.26867', '2.28993', '2.29589', '2.30533', '2.28876'] | Gamma1 Grad: ['-0.00082', '0.00070', '0.00036', '-0.00255', '0.00057', '-0.00256', '0.00199', '-0.00875']
Epoch 51: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00116 | Gamma1: ['2.26897', '2.28472', '2.26931', '2.28418', '2.29765', '2.29612', '2.31721', '2.28566'] | Gamma1 Grad: ['0.00220', '-0.00449', '-0.00884', '-0.02194', '-0.05179', '0.01390', '-0.00811', '-0.00064']
Epoch 52: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00106 | Gamma1: ['2.28207', '2.30334', '2.29675', '2.29893', '2.28946', '2.28874', '2.32363', '2.28410'] | Gamma1 Grad: ['-0.00234', '0.00102', '0.00874', '0.01214', '-0.02165', '0.01541', '-0.04396', '0.00924']
Epoch 53: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00096 | Gamma1: ['2.25416', '2.29591', '2.26731', '2.28904', '2.30224', '2.27516', '2.31913', '2.27626'] | Gamma1 Grad: ['0.00474', '-0.01040', '0.00675', '-0.00948', '0.00483', '-0.02711', '0.00628', '0.00170']
Epoch 54: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['2.25991', '2.28362', '2.28326', '2.28721', '2.28195', '2.28333', '2.31777', '2.27878'] | Gamma1 Grad: ['0.00463', '0.00433', '0.02251', '0.02441', '0.04273', '-0.00284', '-0.01069', '-0.00592']
Epoch 55: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.27767', '2.28273', '2.27729', '2.27999', '2.29474', '2.26947', '2.31423', '2.27740'] | Gamma1 Grad: ['-0.00570', '-0.00026', '0.02958', '-0.01661', '-0.00194', '0.00047', '0.00490', '-0.00702']
Epoch 56: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00069 | Gamma1: ['2.26125', '2.27273', '2.26214', '2.26735', '2.29837', '2.31537', '2.29291', '2.17980'] | Gamma1 Grad: ['-0.01831', '-0.00246', '-0.00944', '-0.03577', '0.00350', '0.03329', '0.00765', '0.02351']
Epoch 57: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00060 | Gamma1: ['2.26579', '2.27916', '2.28253', '2.26980', '2.29522', '2.26902', '2.29113', '2.21956'] | Gamma1 Grad: ['-0.00450', '-0.00368', '0.00839', '-0.01117', '-0.01096', '-0.01074', '-0.00213', '0.00401']
Epoch 58: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.27324', '2.28367', '2.28807', '2.26930', '2.29426', '2.28167', '2.28447', '2.23316'] | Gamma1 Grad: ['-0.00396', '0.00200', '0.02542', '0.00090', '0.00463', '-0.00452', '0.00058', '-0.00125']
Epoch 59: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.28856', '2.28531', '2.28517', '2.28439', '2.28117', '2.28679', '2.29404', '2.26170'] | Gamma1 Grad: ['0.00048', '-0.00008', '-0.00007', '0.00043', '0.00154', '-0.00131', '0.00050', '-0.00113']
Epoch 60: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00037 | Gamma1: ['2.27176', '2.28415', '2.28279', '2.28118', '2.26987', '2.29032', '2.31829', '2.25916'] | Gamma1 Grad: ['0.00117', '0.00236', '0.01166', '0.01625', '-0.08084', '0.00208', '-0.00776', '-0.00289']
Epoch 61: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00031 | Gamma1: ['2.27984', '2.29150', '2.28491', '2.27582', '2.29514', '2.29320', '2.31561', '2.26411'] | Gamma1 Grad: ['0.01096', '0.01102', '-0.01788', '-0.02572', '0.02802', '0.00980', '-0.02912', '0.00303']
Epoch 62: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00025 | Gamma1: ['2.27664', '2.27984', '2.28563', '2.28895', '2.28333', '2.27677', '2.32033', '2.26502'] | Gamma1 Grad: ['0.00276', '-0.00668', '-0.01223', '0.00592', '0.00761', '-0.01506', '0.00321', '-0.00167']
Epoch 63: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.26587', '2.28966', '2.29166', '2.29290', '2.29898', '2.28043', '2.32608', '2.28992'] | Gamma1 Grad: ['0.00270', '0.00034', '-0.00543', '0.00012', '0.00112', '0.00276', '0.00020', '-0.00026']
Epoch 64: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['2.26892', '2.28456', '2.26588', '2.29231', '2.29427', '2.28412', '2.32424', '2.28079'] | Gamma1 Grad: ['0.00062', '-0.00051', '-0.00076', '0.00101', '-0.00183', '0.00367', '0.00310', '-0.00001']
Epoch 65: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00010 | Gamma1: ['2.26893', '2.29961', '2.28272', '2.27687', '2.29374', '2.29100', '2.32982', '2.27485'] | Gamma1 Grad: ['0.00710', '0.00596', '-0.07098', '0.03017', '0.03175', '0.03252', '-0.00816', '0.00017']
Epoch 66: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.25952', '2.29599', '2.28229', '2.27307', '2.29416', '2.28924', '2.31944', '2.27695'] | Gamma1 Grad: ['0.00643', '0.00118', '0.00652', '0.01101', '0.00839', '0.00029', '-0.02165', '0.00285']
Epoch 67: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00004 | Gamma1: ['2.26537', '2.28960', '2.27794', '2.27316', '2.28131', '2.28697', '2.32145', '2.28388'] | Gamma1 Grad: ['0.00356', '-0.00053', '-0.00534', '-0.00491', '0.00503', '-0.00363', '0.00607', '-0.00133']
Epoch 68: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00003 | Gamma1: ['2.28093', '2.28341', '2.28351', '2.26207', '2.29254', '2.30019', '2.32443', '2.25405'] | Gamma1 Grad: ['0.00051', '-0.00212', '-0.00221', '0.00467', '0.00701', '-0.02114', '0.00606', '-0.00025']
Epoch 69: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00001 | Gamma1: ['2.26796', '2.27487', '2.27778', '2.28307', '2.28861', '2.28904', '2.34990', '2.28944'] | Gamma1 Grad: ['0.00532', '0.00234', '-0.00501', '-0.01560', '0.01707', '-0.00617', '0.00689', '-0.00014']
Epoch 70: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.26765', '2.27536', '2.27668', '2.28457', '2.30118', '2.28321', '2.34080', '2.28904'] | Gamma1 Grad: ['-0.00562', '0.00427', '-0.01725', '0.01043', '-0.00109', '0.00334', '-0.00106', '0.00523']
Epoch 71: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.28137', '2.27045', '2.30999', '2.29029', '2.29277', '2.28996', '2.31385', '2.31242'] | Gamma1 Grad: ['0.01731', '0.00141', '0.01091', '-0.00201', '0.02983', '-0.02638', '0.01664', '-0.00199']
Epoch 72: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.26745', '2.26755', '2.29824', '2.27880', '2.28569', '2.26799', '2.32076', '2.28365'] | Gamma1 Grad: ['0.00928', '0.00771', '0.01141', '0.02084', '0.02847', '-0.04310', '0.00688', '0.00325']
Epoch 73: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00249 | Gamma1: ['2.28988', '2.28698', '2.28553', '2.27921', '2.28594', '2.27111', '2.32404', '2.28510'] | Gamma1 Grad: ['0.00141', '-0.00089', '0.00555', '0.00756', '0.01271', '0.00149', '0.00443', '0.00029']
Epoch 74: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00248 | Gamma1: ['2.28400', '2.28072', '2.26427', '2.29446', '2.26269', '2.29057', '2.31064', '2.28012'] | Gamma1 Grad: ['-0.00026', '-0.00074', '0.00041', '-0.00046', '0.00153', '0.00065', '0.00030', '0.00005']
Epoch 75: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00248 | Gamma1: ['2.27491', '2.28395', '2.25509', '2.30007', '2.28176', '2.28275', '2.34454', '2.26962'] | Gamma1 Grad: ['-0.00122', '0.00258', '0.01151', '-0.01646', '-0.01743', '0.00268', '-0.00804', '-0.00193']
Epoch 76: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00247 | Gamma1: ['2.28417', '2.28424', '2.29260', '2.30608', '2.30951', '2.30116', '2.33143', '2.28236'] | Gamma1 Grad: ['0.00145', '0.00189', '0.00203', '0.01393', '0.01560', '-0.00015', '0.01153', '0.00430']
Epoch 77: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00245 | Gamma1: ['2.26013', '2.26668', '2.26351', '2.27065', '2.28829', '2.28506', '2.33026', '2.30960'] | Gamma1 Grad: ['-0.00627', '-0.00616', '-0.00968', '0.00450', '-0.01123', '0.00529', '0.00518', '0.00073']
Epoch 78: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00244 | Gamma1: ['2.27658', '2.28860', '2.29231', '2.29436', '2.28121', '2.29418', '2.33656', '2.29060'] | Gamma1 Grad: ['0.00285', '-0.00254', '0.04607', '-0.03355', '-0.02178', '-0.03454', '0.01016', '0.00311']
Epoch 79: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00242 | Gamma1: ['2.26221', '2.28662', '2.28931', '2.28701', '2.28579', '2.27491', '2.33320', '2.27940'] | Gamma1 Grad: ['0.00173', '0.00579', '0.00697', '-0.00020', '-0.02352', '0.00572', '-0.01108', '0.00028']
Epoch 80: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00241 | Gamma1: ['2.26118', '2.28004', '2.30241', '2.29388', '2.27122', '2.31155', '2.30867', '2.27703'] | Gamma1 Grad: ['-0.00113', '-0.00043', '0.00355', '0.00043', '-0.00051', '0.00195', '-0.00083', '0.00033']
Epoch 81: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00239 | Gamma1: ['2.26945', '2.28080', '2.29208', '2.28951', '2.26957', '2.29984', '2.29608', '2.28220'] | Gamma1 Grad: ['0.00010', '0.00010', '0.00021', '0.00014', '-0.00017', '0.00017', '0.00021', '0.00017']
Epoch 82: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00236 | Gamma1: ['2.27999', '2.29808', '2.27889', '2.27784', '2.28573', '2.27768', '2.26830', '2.28163'] | Gamma1 Grad: ['-0.00340', '0.00030', '-0.00329', '0.00000', '-0.00209', '-0.00197', '0.00199', '-0.00005']
Epoch 83: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00234 | Gamma1: ['2.27991', '2.28903', '2.28131', '2.26605', '2.27130', '2.29389', '2.29130', '2.28080'] | Gamma1 Grad: ['0.00312', '-0.00007', '0.00577', '0.00024', '0.00255', '0.00112', '0.00795', '-0.00292']
Epoch 84: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00232 | Gamma1: ['2.26650', '2.28467', '2.26004', '2.26329', '2.28279', '2.29687', '2.28591', '2.28657'] | Gamma1 Grad: ['-0.00033', '0.00053', '-0.00346', '-0.00039', '-0.00287', '0.00050', '-0.00043', '-0.00017']
Epoch 85: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00229 | Gamma1: ['2.28237', '2.27312', '2.26747', '2.27868', '2.28784', '2.28757', '2.28574', '2.28373'] | Gamma1 Grad: ['0.00001', '-0.00036', '-0.00050', '-0.00000', '-0.00044', '-0.00043', '0.00002', '-0.00016']
Epoch 86: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00226 | Gamma1: ['2.28006', '2.27766', '2.28343', '2.27682', '2.29580', '2.28938', '2.27760', '2.28168'] | Gamma1 Grad: ['-0.00009', '-0.00006', '0.00018', '-0.00015', '0.00035', '-0.00019', '-0.00012', '-0.00004']
Epoch 87: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00223 | Gamma1: ['2.27833', '2.26880', '2.26960', '2.28449', '2.30113', '2.29352', '2.27975', '2.28233'] | Gamma1 Grad: ['-0.00206', '0.00096', '-0.00002', '0.00023', '-0.00387', '0.00220', '-0.00724', '0.00001']
Epoch 88: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00220 | Gamma1: ['2.27540', '2.26618', '2.27393', '2.28527', '2.28463', '2.28203', '2.27070', '2.28207'] | Gamma1 Grad: ['-0.00034', '-0.00009', '-0.00063', '0.00014', '0.00009', '0.00116', '-0.00161', '0.00003']
Epoch 89: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00217 | Gamma1: ['2.27947', '2.26993', '2.29092', '2.28465', '2.26704', '2.27704', '2.27654', '2.28562'] | Gamma1 Grad: ['0.00010', '-0.00006', '0.00002', '0.00029', '-0.00038', '-0.00017', '-0.00006', '0.00009']
Epoch 90: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00214 | Gamma1: ['2.28231', '2.27335', '2.26243', '2.27436', '2.26331', '2.28927', '2.26267', '2.28346'] | Gamma1 Grad: ['0.00015', '-0.00003', '-0.00047', '-0.00015', '-0.00082', '0.00028', '-0.00021', '0.00014']
Epoch 91: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00210 | Gamma1: ['2.28032', '2.28594', '2.27068', '2.28373', '2.27873', '2.30477', '2.26120', '2.28500'] | Gamma1 Grad: ['0.00019', '0.00033', '-0.00100', '0.00055', '0.00028', '0.00057', '-0.00034', '0.00010']
Epoch 92: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00206 | Gamma1: ['2.27359', '2.27992', '2.28163', '2.29429', '2.28810', '2.28260', '2.27202', '2.28411'] | Gamma1 Grad: ['-0.00011', '-0.00011', '0.00025', '-0.00018', '-0.00006', '0.00009', '0.00044', '0.00005']
Epoch 93: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00203 | Gamma1: ['2.28337', '2.27567', '2.28205', '2.28470', '2.29185', '2.27171', '2.27948', '2.28412'] | Gamma1 Grad: ['0.00012', '-0.00021', '-0.00005', '0.00010', '0.00017', '0.00014', '-0.00027', '-0.00005']
Epoch 94: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00199 | Gamma1: ['2.28671', '2.26930', '2.28502', '2.28125', '2.28795', '2.27414', '2.27405', '2.27846'] | Gamma1 Grad: ['-0.00016', '-0.00003', '0.00021', '0.00086', '0.00155', '-0.00116', '-0.00125', '0.00000']
Epoch 95: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00195 | Gamma1: ['2.28552', '2.28448', '2.28211', '2.29226', '2.28950', '2.29893', '2.28085', '2.28327'] | Gamma1 Grad: ['0.00010', '0.00026', '0.00005', '0.00018', '0.00028', '0.00058', '0.00053', '0.00012']
Epoch 96: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00191 | Gamma1: ['2.27869', '2.27538', '2.28752', '2.27614', '2.29528', '2.27881', '2.27768', '2.27617'] | Gamma1 Grad: ['0.00041', '-0.00015', '-0.00063', '-0.00023', '0.00067', '0.00072', '-0.00044', '-0.00017']
Epoch 97: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00186 | Gamma1: ['2.29303', '2.27191', '2.28548', '2.28109', '2.27268', '2.28634', '2.27220', '2.28062'] | Gamma1 Grad: ['0.00031', '-0.00009', '0.00005', '0.00014', '-0.00040', '0.00012', '-0.00021', '0.00005']
Epoch 98: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00182 | Gamma1: ['2.28529', '2.28005', '2.26908', '2.28352', '2.28687', '2.26371', '2.27421', '2.28134'] | Gamma1 Grad: ['-0.00004', '0.00000', '-0.00023', '-0.00036', '0.00024', '-0.00058', '-0.00012', '-0.00007']
Epoch 99: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00178 | Gamma1: ['2.28648', '2.28288', '2.25542', '2.27533', '2.29050', '2.27850', '2.26377', '2.28312'] | Gamma1 Grad: ['0.00031', '0.00029', '-0.00047', '-0.00001', '0.00013', '0.00022', '-0.00023', '0.00018']
