{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarted pytorch_env (Python 3.10.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5d240-a4b2-437a-aab6-8646b576b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Current working directory: C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\n",
      "âœ… sys.path updated:\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\python310.zip\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\DLLs\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\n",
      "   ğŸ“‚ \n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\win32\\lib\n",
      "   ğŸ“‚ c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\Pythonwin\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\models\n",
      "   ğŸ“‚ C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\activation\n",
      "âœ… FFTGate imported successfully!\n",
      "âœ… FFTGate instance created successfully!\n",
      "âœ… FFTGate_SENet imported successfully!\n",
      "CIFAR10 Training Script Initialized...\n",
      "Using device: cuda\n",
      "Parsed learning rate: 0.001 (type: <class 'float'>)\n",
      "Formatted learning rate for filenames: 0_001\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Length of training dataset: 50000\n",
      "Length of testing dataset: 10000\n",
      "Number of classes in CIFAR-10: 10\n",
      "==> Building model..\n",
      "âœ… Found 8 FFTGate layers.\n",
      "âœ… Collected 8 trainable activation parameters.\n",
      "   ğŸ”¹ Layer 0: FFTGate()\n",
      "   ğŸ”¹ Layer 1: FFTGate()\n",
      "   ğŸ”¹ Layer 2: FFTGate()\n",
      "   ğŸ”¹ Layer 3: FFTGate()\n",
      "   ğŸ”¹ Layer 4: FFTGate()\n",
      "   ğŸ”¹ Layer 5: FFTGate()\n",
      "   ğŸ”¹ Layer 6: FFTGate()\n",
      "   ğŸ”¹ Layer 7: FFTGate()\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 1.A. IMPORTS LIBRARIES | XXX -----------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\"\"\"Train CIFAR10 with PyTorch.\"\"\"\n",
    "\n",
    "# Python 2/3 compatibility\n",
    "# from __future__ import print_function\n",
    "\n",
    "\n",
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch and related modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# torchvision for datasets and transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch_optimizer as torch_opt  # Use 'torch_opt' for torch_optimizer\n",
    "from timm.scheduler import CosineLRScheduler \n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Define currect working directory to ensure on right directory\n",
    "SENet18_PATH = r\"C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\"\n",
    "if os.getcwd() != SENet18_PATH:\n",
    "    os.chdir(SENet18_PATH)\n",
    "print(f\"âœ… Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# âœ… Define absolute paths\n",
    "PROJECT_PATH = SENet18_PATH\n",
    "MODELS_PATH = os.path.join(SENet18_PATH, \"models\")\n",
    "ACTIVATION_PATH = os.path.join(SENet18_PATH, \"activation\")\n",
    "# PAU_PATH = os.path.join(SENet18_PATH, \"pau\")\n",
    "\n",
    "# âœ… Ensure necessary paths are in sys.path\n",
    "for path in [PROJECT_PATH, MODELS_PATH, ACTIVATION_PATH]:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "# âœ… Print updated sys.path for debugging\n",
    "print(\"âœ… sys.path updated:\")\n",
    "for path in sys.path:\n",
    "    print(\"   ğŸ“‚\", path)\n",
    "\n",
    "# âœ… Import FFTGate (Check if the module exists)\n",
    "try:\n",
    "    from activation.FFTGate import FFTGate  # type: ignore\n",
    "    print(\"âœ… FFTGate imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ Import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'FFTGate.py' exists inside: {ACTIVATION_PATH}\")\n",
    "\n",
    "# âœ… Test if FFTGate is callable\n",
    "try:\n",
    "    activation_test = FFTGate()\n",
    "    print(\"âœ… FFTGate instance created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error while initializing FFTGate: {e}\")\n",
    "\n",
    "# âœ… Now import FFTGate_SENet (Ensure module exists inside models/)\n",
    "try:\n",
    "    from models.FFTGate_SENet import FFTGate_SENet  # type: ignore\n",
    "    print(\"âœ… FFTGate_SENet imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"âŒ FFTGate_SENet import failed: {e}\")\n",
    "    print(f\"ğŸ” Check that 'FFTGate_SENet.py' exists inside: {MODELS_PATH}\")\n",
    "\n",
    "# from models.FFTGate_SENet import Block  # Import the Block class explicitly!\n",
    "from models.FFTGate_SENet import PreActBlock  # Ensure the correct path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 1.B. SEEDING FOR REPRODUCIBILITY | XXX -------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def set_seed_torch(seed):\n",
    "    torch.manual_seed(seed)                          \n",
    "\n",
    "\n",
    "\n",
    "def set_seed_main(seed):\n",
    "    random.seed(seed)                                ## Python's random module\n",
    "    np.random.seed(seed)                             ## NumPy's random module\n",
    "    torch.cuda.manual_seed(seed)                     ## PyTorch's random module for CUDA\n",
    "    torch.cuda.manual_seed_all(seed)                 ## Seed for all CUDA devices\n",
    "    torch.backends.cudnn.deterministic = True        ## Ensure deterministic behavior for CuDNN\n",
    "    torch.backends.cudnn.benchmark = False           ## Disable CuDNN's autotuning for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)   \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(1)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (Optional) Import Optimizers - Uncomment as needed\n",
    "# from Opt import opt\n",
    "# from diffGrad import diffGrad\n",
    "# from diffRGrad import diffRGrad, SdiffRGrad, BetaDiffRGrad, Beta12DiffRGrad, BetaDFCDiffRGrad\n",
    "# from RADAM import Radam, BetaRadam\n",
    "# from BetaAdam import BetaAdam, BetaAdam1, BetaAdam2, BetaAdam3, BetaAdam4, BetaAdam5, BetaAdam6, BetaAdam7, BetaAdam4A\n",
    "# from AdamRM import AdamRM, AdamRM1, AdamRM2, AdamRM3, AdamRM4, AdamRM5\n",
    "# from sadam import sadam\n",
    "# from SdiffGrad import SdiffGrad\n",
    "# from SRADAM import SRADAM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 1.F. SEARCH FOR MYACTIVATION LAYERS| XXX -----------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "def find_activations(module, activation_layers, activation_params):\n",
    "    \"\"\"Recursively find FFTGate layers and collect them into activation_layers and activation_params.\"\"\"\n",
    "    for layer in module.children():\n",
    "        if isinstance(layer, FFTGate):\n",
    "            activation_layers.append(layer)  # âœ… Store the entire layer\n",
    "            activation_params.append(layer.gamma1)  # âœ… Store only gamma1 for optimization\n",
    "        elif isinstance(layer, nn.Sequential) or isinstance(layer, nn.Module):  \n",
    "            find_activations(layer, activation_layers, activation_params)  # âœ… Recursively search\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 2. DEFINE Lr | XXX ---------------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Main Execution (Placeholder)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"CIFAR10 Training Script Initialized...\")\n",
    "    # Add your training pipeline here\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Argument parser to get user inputs\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "\n",
    "args, unknown = parser.parse_known_args()  # Avoids Jupyter argument issues\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Ensure lr is correctly parsed\n",
    "lr = args.lr  # Get learning rate from argparse\n",
    "lr_str = str(lr).replace('.', '_')  # Convert to string and replace '.' for filenames\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Parsed learning rate: {lr} (type: {type(lr)})\")\n",
    "print(f\"Formatted learning rate for filenames: {lr_str}\")\n",
    "\n",
    "# Initialize training variables\n",
    "best_acc = 0  # Best test accuracy\n",
    "start_epoch = 0  # Start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 3. LOAD DATASET | XXX ------------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "bs = 128 #set batch size\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "# âœ… Length of train and test datasets\n",
    "len_train = len(trainset)\n",
    "len_test = len(testset)\n",
    "print(f\"Length of training dataset: {len_train}\")\n",
    "print(f\"Length of testing dataset: {len_test}\")\n",
    "\n",
    "\n",
    " \n",
    "# âœ… Print number of classes\n",
    "num_classes_Print = len(trainset.classes)\n",
    "print(f\"Number of classes in CIFAR-10: {num_classes_Print}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 4. DYNAMIC REGULARIZATION| XXX ---------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def apply_dynamic_regularization(inputs, feature_activations, epoch,\n",
    "                                  prev_params, layer_index_map, batch_idx):\n",
    "\n",
    "    global activation_layers  # âœ… MUST be declared first, before using activation_layers | # Uses collected layers\n",
    "\n",
    "\n",
    "    if batch_idx == 0 and epoch <= 4:\n",
    "        print(f\"\\nğŸš¨ ENTERED apply_dynamic_regularization | Epoch={epoch} | Batch={batch_idx}\", flush=True)\n",
    "\n",
    "\n",
    "        # ğŸ§  Print all gamma1 stats in one line (once per batch)\n",
    "        all_layer_info = []\n",
    "        for idx, layer in enumerate(activation_layers):\n",
    "            param = getattr(layer, \"gamma1\")\n",
    "            all_layer_info.append(f\"Layer {idx}: ID={id(param)} | Mean={param.mean().item():.5f}\")\n",
    "        print(\"ğŸ§  GAMMA1 INFO:\", \" | \".join(all_layer_info), flush=True)\n",
    "\n",
    "\n",
    "    # âœ… Initialize gamma1 regularization accumulator\n",
    "    gamma1_reg = 0.0\n",
    "\n",
    "    # âœ… Compute batch std and define regularization strength\n",
    "    batch_std = torch.std(inputs) + 1e-6\n",
    "    regularization_strength = 0.05 if epoch < 40 else (0.01 if epoch < 60 else 0.005)\n",
    "\n",
    "\n",
    "    # âœ… Track layers where noise is injected (informative)\n",
    "    noisy_layers = []\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        if idx not in layer_index_map:\n",
    "            continue\n",
    "\n",
    "        prev_layer_params = prev_params[layer_index_map[idx]]\n",
    "        param_name = \"gamma1\"  # âœ… Only gamma1 is trainable\n",
    "        param = getattr(layer, param_name)\n",
    "        prev_param = prev_layer_params[param_name]\n",
    "\n",
    "        # âœ… Target based on input stats\n",
    "        target = compute_target(param_name, batch_std)\n",
    "\n",
    "        # âœ… Adaptive Target Regularization\n",
    "        gamma1_reg += regularization_strength * (param - target).pow(2).mean() * 1.2\n",
    "\n",
    "        # âœ… Adaptive Cohesion Regularization\n",
    "        cohesion = (param - prev_param).pow(2)  \n",
    "        gamma1_reg += 0.005 * cohesion.mean() \n",
    "\n",
    "        # âœ… Adaptive Noise Regularization\n",
    "        epoch_AddNoise = 50\n",
    "        if epoch > epoch_AddNoise:\n",
    "            param_variation = torch.abs(param - prev_param).mean()\n",
    "            if param_variation < 0.015:  \n",
    "                noise = (0.001 + 0.0004 * batch_std.item()) * torch.randn_like(param)\n",
    "                penalty = (param - (prev_param + noise)).pow(2).sum()\n",
    "                gamma1_reg += 0.00015 * penalty  # Adjusted penalty\n",
    "                noisy_layers.append(f\"{idx} (Î”={param_variation.item():.5f})\") # Collect index and variation\n",
    "\n",
    "    # âœ… Print noise summary for first few epochs\n",
    "    if batch_idx == 0 and epoch <= (epoch_AddNoise+4) and noisy_layers:\n",
    "        print(f\"ğŸ”¥ Stable Noise Injected | Epoch {epoch} | Batch {batch_idx} | Layers: \" + \", \".join(noisy_layers), flush=True)\n",
    "    mags = feature_activations.abs().mean(dim=(0, 2, 3))\n",
    "    m = mags / mags.sum()\n",
    "    gamma1_reg += 0.005 * (-(m * torch.log(m + 1e-6)).sum())\n",
    "\n",
    "    return gamma1_reg\n",
    "\n",
    "\n",
    "def compute_target(param_name, batch_std):\n",
    "    if param_name == \"gamma1\":\n",
    "        return 2.0 + 0.2 * batch_std.item()     \n",
    "    \n",
    "    raise ValueError(f\"Unknown param {param_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 5. INITIALIZE MODEL | XXX --------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "#net = Elliott_VGG('VGG16'); net1 = 'Elliott_VGG16'\n",
    "#net = GELU_MobileNet(); net1 = 'GELU_MobileNet'\n",
    "#net = GELU_SENet18(); net1 = 'GELU_SENet18'\n",
    "#net = PDELU_ResNet50(); net1 = 'PDELU_ResNet50'\n",
    "# net = Sigmoid_GoogLeNet(); net1 = 'Sigmoid_GoogLeNet'\n",
    "#net = GELU_DenseNet121(); net1 = 'GELU_DenseNet121'\n",
    "# net = ReLU_VGG('VGG16'); net1 = 'ReLU_VGG16'\n",
    "# net = MY_VGG4('VGG16'); net1 = 'MY_VGG16'\n",
    "# net = MY_MobileNet4(num_classes=10); net1 = 'MY_MobileNet4'\n",
    "# net = MY_SENet4(num_classes=10); net1 = 'MY_SENet4'\n",
    "net = FFTGate_SENet(PreActBlock, [2, 2, 2, 2], num_classes=10); net1 = 'FFTGate_SENet'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9); optimizer1 = 'SGDM5'\n",
    "#optimizer = optim.Adagrad(net.parameters()); optimizer1 = 'AdaGrad'\n",
    "#optimizer = optim.Adadelta(net.parameters()); optimizer1 = 'AdaDelta'\n",
    "#optimizer = optim.RMSprop(net.parameters()); optimizer1 = 'RMSprop'\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr); optimizer1 = 'Adam'\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr, amsgrad=True); optimizer1 = 'amsgrad'\n",
    "#optimizer = diffGrad(net.parameters(), lr=args.lr); optimizer1 = 'diffGrad'\n",
    "#optimizer = Radam(net.parameters(), lr=args.lr); optimizer1 = 'Radam'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 6. INITIALIZE ACTIVATION PARAMETERS, OPTIMIZERS & SCHEDULERS | XXX ---------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "# âœ… Step 1: Collect Activation Parameters from ALL Layers (Ensure Compatibility with DataParallel)\n",
    "if isinstance(net, torch.nn.DataParallel):\n",
    "    model_layers = [\n",
    "        net.module.conv1, net.module.bn1, \n",
    "        *net.module.layer1, *net.module.layer2, \n",
    "        *net.module.layer3, *net.module.layer4\n",
    "    ]\n",
    "else:\n",
    "    model_layers = [\n",
    "        net.conv1, net.bn1, \n",
    "        *net.layer1, *net.layer2, \n",
    "        *net.layer3, *net.layer4\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 2: Recursively search for FFTGate layers\n",
    "activation_params = []\n",
    "activation_layers = []  # âœ… Define an empty list to store FFTGate layers\n",
    "\n",
    "\n",
    "# ğŸ” Correctly populate activation_layers and activation_params\n",
    "for layer in model_layers:\n",
    "    find_activations(layer, activation_layers, activation_params)\n",
    "\n",
    "\n",
    "# âœ… Step 3: Print collected activation layers and parameters\n",
    "if activation_layers and activation_params:\n",
    "    print(f\"âœ… Found {len(activation_layers)} FFTGate layers.\")\n",
    "    print(f\"âœ… Collected {len(activation_params)} trainable activation parameters.\")\n",
    "    \n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        print(f\"   ğŸ”¹ Layer {idx}: {layer}\")\n",
    "\n",
    "elif activation_layers and not activation_params:\n",
    "    print(f\"âš  Warning: Found {len(activation_layers)} FFTGate layers, but no trainable parameters were collected.\")\n",
    "\n",
    "elif activation_params and not activation_layers:\n",
    "    print(f\"âš  Warning: Collected {len(activation_params)} activation parameters, but no FFTGate layers were recorded.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš  Warning: No FFTGate layers or activation parameters found! Skipping activation optimizer.\")\n",
    "    activation_optimizers = None  # Prevents crash\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 4: Define Unfreeze Epoch\n",
    "unfreeze_activation_epoch = 1  # âœ… Change this value if needed\n",
    "# unfreeze_activation_epoch = 10  # âœ… Delay unfreezing until epoch 10\n",
    "\n",
    "\n",
    "# âœ…Step 5: Define the warm-up epoch value\n",
    "# WARMUP_ACTIVATION_EPOCHS = 5  # The number of epochs for warm-up\n",
    "WARMUP_ACTIVATION_EPOCHS = 0  # The number of epochs for warm-up\n",
    "\n",
    "\n",
    "# âœ… Step 6: Initially Freeze Activation Parameters\n",
    "for param in activation_params:\n",
    "    param.requires_grad = False  # ğŸš« Keep frozen before the unfreeze epoch\n",
    "\n",
    "\n",
    "# âœ… Step 7: Initialize Activation Optimizers (Using AdamW for Better Weight Decay)\n",
    "activation_optimizers = {\n",
    "    \"gamma1\": torch.optim.AdamW(activation_params, lr=0.0025, weight_decay=1e-6)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Step 8: Initialize Activation Schedulers with Warm Restarts (Per Parameter Type)\n",
    "activation_schedulers = {\n",
    "    \"gamma1\": CosineAnnealingWarmRestarts(\n",
    "        activation_optimizers[\"gamma1\"],\n",
    "        T_0=10,      # Slightly extended warm-up\n",
    "        T_mult=2,    # Increase cycle length gradually\n",
    "        eta_min=1e-5   # âœ… recommended safer modification\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 7. INITIALIZE MAIN OPTIMIZER SCHEDULER | XXX --------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# âœ… Step 6: Define MultiStepLR for Main Optimizer\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "main_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 8. MODEL CHECK POINT | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists('checkpoint'):\n",
    "    os.makedirs('checkpoint')\n",
    "\n",
    "if not os.path.exists('Results'):\n",
    "    os.makedirs('Results')\n",
    "\n",
    "# Construct checkpoint path\n",
    "checkpoint_path = f'./checkpoint/CIFAR10_B{bs}_LR{lr}_{net1}_{optimizer1}.t7'\n",
    "\n",
    "# Resume checkpoint only if file exists\n",
    "if args.resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 9. DEFINE TRAIN LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "# Training\n",
    "\n",
    "def train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler , WARMUP_ACTIVATION_EPOCHS):\n",
    "    global train_loss_history, best_train_acc, prev_params, recent_test_acc, gamma1_history, activation_layers, test_acc_history   # ğŸŸ¢ğŸŸ¢ğŸŸ¢\n",
    "\n",
    "    if epoch == 0:\n",
    "        train_loss_history = []\n",
    "        best_train_acc = 0.0\n",
    "        recent_test_acc = 0.0\n",
    "        gamma1_history = {}        # âœ… Initialize history\n",
    "        test_acc_history = []      # âœ… NEW: test accuracy history\n",
    "\n",
    "\n",
    "    prev_params = {}\n",
    "    layer_index_map = {idx: idx for idx in range(len(activation_layers))}\n",
    "\n",
    "    # âœ… Cache previous gamma1 values\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        prev_params[idx] = {\"gamma1\": layer.gamma1.clone().detach()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_accuracy = 0.0\n",
    "\n",
    "    # âœ… Initialize log history\n",
    "    log_history = []\n",
    "\n",
    "    # âœ… Define path to store Training log\n",
    "    save_paths = {\n",
    "       \n",
    "        \"log_history\": f\"C:\\\\Users\\\\emeka\\\\Research\\\\ModelCUDA\\\\Big_Data_Journal\\\\Comparison\\\\Code\\\\Paper\\\\github2\\\\CIFAR10\\\\SENet18\\\\Results\\\\FFTGate\\\\FFTGate_training_logs.txt\"  # âœ… Training log_history \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 1: Ensure Activation Parameters are Unfrozen at the Right Time\n",
    "    if epoch == unfreeze_activation_epoch:\n",
    "        print(\"\\nğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\")\n",
    "        for layer in activation_layers:  \n",
    "            layer.gamma1.requires_grad = True  \n",
    "        print(\"âœ… Activation Parameters Unfrozen! ğŸš€\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step 2: Gradual Warm-up for Activation Learning Rates (AFTER Unfreezing)\n",
    "    warmup_start = unfreeze_activation_epoch  # ğŸ”¹ Start warm-up when unfreezing happens\n",
    "    warmup_end = unfreeze_activation_epoch + WARMUP_ACTIVATION_EPOCHS  # ğŸ”¹ End warm-up period\n",
    "\n",
    "    # âœ… Adjust learning rates **only** during the warm-up phase\n",
    "    if warmup_start <= epoch < warmup_end:\n",
    "        warmup_factor = (epoch - warmup_start + 1) / WARMUP_ACTIVATION_EPOCHS \n",
    "\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            for param_group in act_scheduler.optimizer.param_groups:\n",
    "                if \"initial_lr\" not in param_group:\n",
    "                    param_group[\"initial_lr\"] = param_group[\"lr\"]  # ğŸ”¹ Store initial LR\n",
    "                param_group[\"lr\"] = param_group[\"initial_lr\"] * warmup_factor  # ğŸ”¹ Scale LR\n",
    "\n",
    "        # âœ… Debugging output to track warm-up process\n",
    "        print(f\"ğŸ”¥ Warm-up Epoch {epoch}: Scaling LR by {warmup_factor:.3f}\")\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            print(f\"  ğŸ”¹ {name} LR: {act_scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    activation_history = []  # ğŸ”´ Initialize empty history at start of epoch (outside batch loop)\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Training Loop\n",
    "    with tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch}\") as progress:\n",
    "        for batch_idx, (inputs, targets) in progress:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # zero_grad activation parameter\n",
    "            for opt in activation_optimizers.values():\n",
    "                opt.zero_grad()\n",
    "\n",
    "\n",
    "            # âœ… Forward Pass\n",
    "            # outputs = net(inputs, epoch=epoch, train_accuracy=train_accuracy, targets=targets)\n",
    "            outputs = net(inputs, epoch=epoch) \n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "\n",
    "            # âœ… Feature Extraction (Must Track Gradients) | Collect feature activations directly from the network\n",
    "            x = inputs.to(device)\n",
    "            x = net.module.conv1(x) if isinstance(net, torch.nn.DataParallel) else net.conv1(x)\n",
    "            x = net.module.bn1(x) if isinstance(net, torch.nn.DataParallel) else net.bn1(x)\n",
    "            x = net.module.activation(x, epoch=epoch) if isinstance(net, torch.nn.DataParallel) else net.activation(x, epoch=epoch)\n",
    "\n",
    "\n",
    "            features = (\n",
    "                [net.module.layer1, net.module.layer2, net.module.layer3, net.module.layer4]\n",
    "                if isinstance(net, torch.nn.DataParallel)\n",
    "                else [net.layer1, net.layer2, net.layer3, net.layer4]\n",
    "            )\n",
    "            for group in features:\n",
    "                for block in group:\n",
    "                    x = block(x, epoch=epoch) if isinstance(block, PreActBlock) else block(x)\n",
    "\n",
    "\n",
    "            feature_activations = x  # âœ… Ensures gradients flow properly\n",
    "\n",
    "            # âœ… Collect Activation History | Per-layer mean activations (scalar numbers only)\n",
    "            batch_means = [layer.saved_output.mean().item() for layer in activation_layers]\n",
    "            activation_history.extend(batch_means)  # âœ… Store scalar values efficiently\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Apply Decay strategy to history for each activation layer\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    if isinstance(layer, FFTGate):\n",
    "                        layer.decay_spectral_history(epoch, num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Compute Training Accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            train_accuracy = 100. * correct / total if total > 0 else 0.0  # âœ… Compute training accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Call Regularization Function for the Activation Parameter\n",
    "            if epoch > 0:\n",
    "                gamma1_reg = apply_dynamic_regularization(\n",
    "                    inputs, feature_activations, epoch,\n",
    "                    prev_params, layer_index_map, batch_idx\n",
    "                )\n",
    "                loss += gamma1_reg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "       \n",
    "\n",
    "            # âœ… ğŸ¯Adaptive Gradient Clipping of gamma1 \n",
    "            for layer in activation_layers:\n",
    "                torch.nn.utils.clip_grad_norm_([layer.gamma1], max_norm=0.7) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Apply Optimizer Step for Model Parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # âœ… Apply Optimizer Steps for Activation Parameters (Only if Unfrozen)\n",
    "            if epoch >= unfreeze_activation_epoch:\n",
    "                for opt in activation_optimizers.values():\n",
    "                    opt.step()\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ… Clamping of gamma1 (Applied AFTER Optimizer Step)\n",
    "            with torch.no_grad():\n",
    "                for layer in activation_layers:\n",
    "                    layer.gamma1.clamp_(0.1, 6.0)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # âœ…7ï¸âƒ£ Update progress bar\n",
    "            progress.set_postfix(Train_loss=round(train_loss / (batch_idx + 1), 3),\n",
    "                                 Train_acc=train_accuracy)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Step the main optimizer scheduler (ONLY for model parameters)\n",
    "    main_scheduler.step()\n",
    "\n",
    "    # âœ… Step the activation parameter schedulers (ONLY for activation parameters) | Epoch-wise stepping\n",
    "    if epoch >= unfreeze_activation_epoch:\n",
    "        for name, act_scheduler in activation_schedulers.items(): \n",
    "            act_scheduler.step()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Ensure Activation Learning Rate doesn't drop too low (Prevent vanishing updates)\n",
    "    for opt in activation_optimizers.values():\n",
    "        for group in opt.param_groups:\n",
    "            group['lr'] = max(group['lr'], 1e-5)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    # âœ… ONLY update prev_params here AFTER all updates | âœ… Update prev_params AFTER training epoch\n",
    "    for idx, layer in enumerate(features):\n",
    "        if isinstance(layer, FFTGate):  \n",
    "            prev_params[idx] = {\n",
    "                \"gamma1\": layer.gamma1.clone().detach()  \n",
    "            }\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Logging Parameters & Gradients\n",
    "    last_batch_grads = {\"Gamma1 Grad\": []}\n",
    "    current_params = {\"Gamma1\": []}\n",
    "\n",
    "    # âœ… Iterate over the previously collected activation layers\n",
    "    for idx, layer in enumerate(activation_layers):  # âœ… Use activation_layers instead of features\n",
    "        if layer.gamma1.grad is not None:\n",
    "            grad_value = f\"{layer.gamma1.grad.item():.5f}\"\n",
    "        else:\n",
    "            grad_value = \"None\"\n",
    "\n",
    "        param_value = f\"{layer.gamma1.item():.5f}\"\n",
    "\n",
    "        last_batch_grads[\"Gamma1 Grad\"].append(grad_value)\n",
    "        current_params[\"Gamma1\"].append(param_value)\n",
    "\n",
    "    # âœ… Build log message (showing params and gradients for ALL layers, cleaned and rounded)\n",
    "    log_msg = (\n",
    "        f\"Epoch {epoch}: M_Optimizer LR => {optimizer.param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1 LR => {activation_optimizers['gamma1'].param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1: {current_params['Gamma1']} | \"\n",
    "        f\"Gamma1 Grad: {last_batch_grads['Gamma1 Grad']}\"\n",
    "    )\n",
    "\n",
    "    log_history.append(log_msg)\n",
    "    print(log_msg)  # âœ… Prints only once per epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (Clear old logs)\n",
    "    if epoch == 0:  # âœ… Only clear at the start of training\n",
    "        with open(save_paths[\"log_history\"], \"w\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Save logs once per epoch (Append new logs)\n",
    "    if log_history:\n",
    "        with open(save_paths[\"log_history\"], \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(\"\\n\".join(log_history) + \"\\n\")  # âœ… Ensure each entry is on a new line\n",
    "        print(f\"ğŸ“œ Logs saved to {save_paths['log_history']}!\")  # âœ… Only prints once per epoch\n",
    "    else:\n",
    "        print(\"âš  No logs to save!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final training accuracy for the epoch\n",
    "    final_train_loss = train_loss / len(trainloader)\n",
    "    final_train_acc = 100. * correct / total\n",
    "\n",
    "    # Append to history\n",
    "    train_loss_history.append(final_train_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save training results (without affecting best accuracy tracking)\n",
    "    train_results_path = f'./Results/CIFAR10_Train_B{bs}_LR{lr}_{net1}_{optimizer1}.txt'\n",
    "\n",
    "    # âœ… Clear the log file at the start of training (Epoch 0)\n",
    "    if epoch == 0 and os.path.exists(train_results_path):\n",
    "        with open(train_results_path, 'w') as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs only once\n",
    "\n",
    "    # âœ… Append new training results for each epoch\n",
    "    with open(train_results_path, 'a') as f:\n",
    "        f.write(f\"Epoch {epoch} | Train Loss: {final_train_loss:.3f} | Train Acc: {final_train_acc:.3f}%\\n\")\n",
    "\n",
    "    if final_train_acc > best_train_acc:\n",
    "        best_train_acc = final_train_acc  # âœ… Update best training accuracy\n",
    "        print(f\"ğŸ† New Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "    # âœ… Append the best training accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):  # Only log once at the final epoch\n",
    "        with open(train_results_path, 'a') as f:\n",
    "            f.write(f\"\\nğŸ† Best Training Accuracy: {best_train_acc:.3f}%\\n\")  \n",
    "\n",
    "    # âœ… Print both Final and Best Training Accuracy\n",
    "    print(f\"ğŸ“Š Train Accuracy: {final_train_acc:.3f}% | ğŸ† Best Train Accuracy: {best_train_acc:.3f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"ğŸ“œ Training logs saved to {train_results_path}!\")\n",
    "    print(f\"ğŸ† Best Training Accuracy: {best_train_acc:.3f}% (Updated)\")\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"ğŸ“ Sizes â†’ ActivationHist: {len(activation_history)} | TestAccHist: {len(test_acc_history)} | TrainLossHist: {len(train_loss_history)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # return final_train_loss, final_train_acc, feature_activations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "####-------| NOTE 10. DEFINE TEST LOOP | XXX -------------------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(epoch, save_results=True):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and optionally saves the results.\n",
    "    \n",
    "    Args:\n",
    "    - epoch (int): The current epoch number.\n",
    "    - save_results (bool): Whether to save results to a file.\n",
    "\n",
    "    Returns:\n",
    "    - acc (float): Test accuracy percentage.\n",
    "    \"\"\"\n",
    "    global best_acc, val_accuracy \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # âœ… Ensure activation function parameters are clamped before evaluation\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(testloader), total=len(testloader), desc=f\"Testing Epoch {epoch}\") as progress:\n",
    "            for batch_idx, (inputs, targets) in progress:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                # outputs = net(inputs, epoch=0)  # Pass epoch=0 to fix TypeError!\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                # âœ… Pass validation accuracy to activation function\n",
    "                val_accuracy = 100. * correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "                # âœ… Update progress bar with loss & accuracy\n",
    "                progress.set_postfix(Test_loss=round(test_loss / (batch_idx + 1), 3),\n",
    "                                     Test_acc=round(val_accuracy, 3))\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Compute final test accuracy\n",
    "    final_test_loss = test_loss / len(testloader)\n",
    "    final_test_acc = 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Ensure \"Results\" folder exists (just like training logs)\n",
    "    results_dir = os.path.join(PROJECT_PATH, \"Results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # âœ… Define log file path for test results\n",
    "    test_results_path = os.path.join(results_dir, f'CIFAR10_Test_B{bs}_LR{lr}_{net1}_{optimizer1}.txt')\n",
    "\n",
    "    # âœ… Initialize log file at the beginning of training (clear old logs)\n",
    "    if epoch == 0:\n",
    "        with open(test_results_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")  # âœ… Clears previous logs\n",
    "\n",
    "    # âœ… Append new test results for each epoch (same style as training)\n",
    "    with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Epoch {epoch} | Test Loss: {final_test_loss:.3f} | Test Acc: {final_test_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Save checkpoint if accuracy improves (does NOT interfere with logging)\n",
    "    if final_test_acc > best_acc:\n",
    "        print('ğŸ† Saving best model...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': final_test_acc,  # âœ… Ensures the best test accuracy is saved in checkpoint\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Ensure checkpoint directory exists\n",
    "        checkpoint_dir = \"checkpoint\"\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "        # âœ… Format learning rate properly before saving filename\n",
    "        lr_str = str(lr).replace('.', '_')\n",
    "        checkpoint_path = f'./checkpoint/CIFAR10_B{bs}_LR{lr_str}_{net1}_{optimizer1}.t7'\n",
    "        torch.save(state, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "        best_acc = final_test_acc  # âœ… Update best accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Append the best test accuracy **only once at the end of training**\n",
    "    if epoch == (num_epochs - 1):\n",
    "        with open(test_results_path, 'a', encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\nğŸ† Best Test Accuracy: {best_acc:.3f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # âœ… Print both Final and Best Test Accuracy (always executed)\n",
    "    print(f\"ğŸ“Š Test Accuracy: {final_test_acc:.3f}% | ğŸ† Best Test Accuracy: {best_acc:.3f}%\")\n",
    "    print(f\"ğŸ“œ Test logs saved to {test_results_path}!\")\n",
    "\n",
    "\n",
    "    global recent_test_acc\n",
    "    recent_test_acc = final_test_acc  # Capture latest test accuracy for next train() call | Store latest test accuracy\n",
    "\n",
    "    return final_test_acc  # âœ… Return the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e9593-2885-48ca-b879-240ca904a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:29<00:00, 13.16it/s, Train_acc=41.2, Train_loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000', '1.50000'] | Gamma1 Grad: ['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 41.184% (Updated)\n",
      "ğŸ“Š Train Accuracy: 41.184% | ğŸ† Best Train Accuracy: 41.184%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 41.184% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 3128 | TestAccHist: 0 | TrainLossHist: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:02<00:00, 39.35it/s, Test_acc=56, Test_loss=1.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 55.990% | ğŸ† Best Test Accuracy: 55.990%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n",
      "\n",
      "ğŸ”“ Unfreezing Activation Function Parameters ğŸ”“\n",
      "âœ… Activation Parameters Unfrozen! ğŸš€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=1 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2575563766928 | Mean=1.50000 | Layer 1: ID=2575563542576 | Mean=1.50000 | Layer 2: ID=2575531526160 | Mean=1.50000 | Layer 3: ID=2575531524160 | Mean=1.50000 | Layer 4: ID=2576226911712 | Mean=1.50000 | Layer 5: ID=2576226913712 | Mean=1.50000 | Layer 6: ID=2576226915872 | Mean=1.50000 | Layer 7: ID=2576226917872 | Mean=1.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.22it/s, Train_acc=62.1, Train_loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00244 | Gamma1: ['2.19609', '2.19957', '2.20496', '2.19457', '2.19599', '2.19489', '2.18250', '2.18916'] | Gamma1 Grad: ['-0.00741', '-0.00281', '-0.00944', '-0.00480', '-0.00889', '-0.00556', '0.01464', '0.00021']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 62.102% (Updated)\n",
      "ğŸ“Š Train Accuracy: 62.102% | ğŸ† Best Train Accuracy: 62.102%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 62.102% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.85it/s, Test_acc=68.5, Test_loss=0.902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 68.500% | ğŸ† Best Test Accuracy: 68.500%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=2 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2575563766928 | Mean=2.19609 | Layer 1: ID=2575563542576 | Mean=2.19957 | Layer 2: ID=2575531526160 | Mean=2.20496 | Layer 3: ID=2575531524160 | Mean=2.19457 | Layer 4: ID=2576226911712 | Mean=2.19599 | Layer 5: ID=2576226913712 | Mean=2.19489 | Layer 6: ID=2576226915872 | Mean=2.18250 | Layer 7: ID=2576226917872 | Mean=2.18916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.32it/s, Train_acc=72.1, Train_loss=0.826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00226 | Gamma1: ['2.27312', '2.28002', '2.28873', '2.27233', '2.27663', '2.27561', '2.26207', '2.27252'] | Gamma1 Grad: ['-0.00027', '0.00302', '-0.01786', '-0.00605', '-0.00113', '0.00459', '-0.00012', '-0.00406']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 72.148% (Updated)\n",
      "ğŸ“Š Train Accuracy: 72.148% | ğŸ† Best Train Accuracy: 72.148%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 72.148% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.13it/s, Test_acc=74.4, Test_loss=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 74.380% | ğŸ† Best Test Accuracy: 74.380%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=3 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2575563766928 | Mean=2.27312 | Layer 1: ID=2575563542576 | Mean=2.28002 | Layer 2: ID=2575531526160 | Mean=2.28873 | Layer 3: ID=2575531524160 | Mean=2.27233 | Layer 4: ID=2576226911712 | Mean=2.27663 | Layer 5: ID=2576226913712 | Mean=2.27561 | Layer 6: ID=2576226915872 | Mean=2.26207 | Layer 7: ID=2576226917872 | Mean=2.27252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.33it/s, Train_acc=77.7, Train_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00199 | Gamma1: ['2.28008', '2.28817', '2.29141', '2.28098', '2.28192', '2.28244', '2.27652', '2.27946'] | Gamma1 Grad: ['0.00804', '0.00395', '0.00299', '-0.00312', '-0.00219', '0.00354', '0.01091', '0.00928']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 77.678% (Updated)\n",
      "ğŸ“Š Train Accuracy: 77.678% | ğŸ† Best Train Accuracy: 77.678%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 77.678% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.95it/s, Test_acc=79.1, Test_loss=0.618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 79.080% | ğŸ† Best Test Accuracy: 79.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš¨ ENTERED apply_dynamic_regularization | Epoch=4 | Batch=0\n",
      "ğŸ§  GAMMA1 INFO: Layer 0: ID=2575563766928 | Mean=2.28008 | Layer 1: ID=2575563542576 | Mean=2.28817 | Layer 2: ID=2575531526160 | Mean=2.29141 | Layer 3: ID=2575531524160 | Mean=2.28098 | Layer 4: ID=2576226911712 | Mean=2.28192 | Layer 5: ID=2576226913712 | Mean=2.28244 | Layer 6: ID=2576226915872 | Mean=2.27652 | Layer 7: ID=2576226917872 | Mean=2.27946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.47it/s, Train_acc=80.8, Train_loss=0.585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00164 | Gamma1: ['2.28074', '2.28604', '2.29322', '2.28037', '2.28040', '2.28595', '2.28205', '2.28513'] | Gamma1 Grad: ['-0.00733', '-0.01056', '-0.00721', '0.01363', '0.00784', '0.02062', '0.01390', '0.00696']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 80.842% (Updated)\n",
      "ğŸ“Š Train Accuracy: 80.842% | ğŸ† Best Train Accuracy: 80.842%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 80.842% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.60it/s, Test_acc=80.1, Test_loss=0.571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 80.110% | ğŸ† Best Test Accuracy: 80.110%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.47it/s, Train_acc=83.2, Train_loss=0.515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00126 | Gamma1: ['2.28334', '2.28527', '2.28913', '2.28144', '2.28266', '2.28366', '2.27867', '2.28933'] | Gamma1 Grad: ['0.00177', '-0.00519', '0.00671', '0.00553', '-0.00068', '-0.00337', '-0.00040', '-0.00648']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 83.216% (Updated)\n",
      "ğŸ“Š Train Accuracy: 83.216% | ğŸ† Best Train Accuracy: 83.216%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 83.216% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.57it/s, Test_acc=83.7, Test_loss=0.476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 83.660% | ğŸ† Best Test Accuracy: 83.660%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.42it/s, Train_acc=85.2, Train_loss=0.461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['2.27825', '2.28825', '2.28779', '2.28547', '2.27982', '2.28033', '2.27613', '2.28761'] | Gamma1 Grad: ['0.00515', '0.01551', '0.00307', '-0.00367', '-0.00096', '-0.00031', '-0.00853', '0.00850']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 85.194% (Updated)\n",
      "ğŸ“Š Train Accuracy: 85.194% | ğŸ† Best Train Accuracy: 85.194%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 85.194% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.27it/s, Test_acc=83.5, Test_loss=0.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 83.460% | ğŸ† Best Test Accuracy: 83.660%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.46it/s, Train_acc=86.6, Train_loss=0.421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.27936', '2.28374', '2.28654', '2.28120', '2.28462', '2.28579', '2.27868', '2.29001'] | Gamma1 Grad: ['0.00187', '-0.00263', '0.01164', '-0.00639', '-0.00050', '0.00509', '-0.00346', '-0.00269']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 86.574% (Updated)\n",
      "ğŸ“Š Train Accuracy: 86.574% | ğŸ† Best Train Accuracy: 86.574%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 86.574% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.28it/s, Test_acc=84.6, Test_loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 84.630% | ğŸ† Best Test Accuracy: 84.630%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.53it/s, Train_acc=88.1, Train_loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00025 | Gamma1: ['2.28196', '2.28303', '2.28299', '2.28612', '2.28155', '2.28463', '2.28338', '2.28710'] | Gamma1 Grad: ['-0.00154', '0.00523', '0.00692', '0.00286', '0.00573', '0.00461', '-0.01028', '0.00470']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.064% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.064% | ğŸ† Best Train Accuracy: 88.064%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.064% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.88it/s, Test_acc=85.4, Test_loss=0.426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 85.440% | ğŸ† Best Test Accuracy: 85.440%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.47it/s, Train_acc=88.8, Train_loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.28137', '2.28205', '2.28846', '2.28746', '2.28315', '2.28545', '2.28348', '2.28595'] | Gamma1 Grad: ['0.00312', '0.00161', '0.01041', '0.00188', '0.00334', '-0.00261', '0.00245', '-0.00756']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 88.760% (Updated)\n",
      "ğŸ“Š Train Accuracy: 88.760% | ğŸ† Best Train Accuracy: 88.760%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 88.760% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.23it/s, Test_acc=87.2, Test_loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 87.250% | ğŸ† Best Test Accuracy: 87.250%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.42it/s, Train_acc=89.9, Train_loss=0.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.27771', '2.28436', '2.28436', '2.28620', '2.28023', '2.28514', '2.28103', '2.28607'] | Gamma1 Grad: ['0.00480', '-0.00359', '-0.02357', '-0.00363', '-0.00348', '-0.00778', '0.00237', '-0.00295']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 89.924% (Updated)\n",
      "ğŸ“Š Train Accuracy: 89.924% | ğŸ† Best Train Accuracy: 89.924%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 89.924% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 3128 | TestAccHist: 0 | TrainLossHist: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.27it/s, Test_acc=88.3, Test_loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 88.310% | ğŸ† Best Test Accuracy: 88.310%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.38it/s, Train_acc=90.7, Train_loss=0.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00248 | Gamma1: ['2.28457', '2.27897', '2.28453', '2.28708', '2.28494', '2.28294', '2.27934', '2.28511'] | Gamma1 Grad: ['0.00180', '0.00214', '0.00157', '0.00852', '-0.00183', '-0.00017', '0.01457', '0.00003']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 90.734% (Updated)\n",
      "ğŸ“Š Train Accuracy: 90.734% | ğŸ† Best Train Accuracy: 90.734%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 90.734% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.56it/s, Test_acc=87.6, Test_loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 87.570% | ğŸ† Best Test Accuracy: 88.310%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.33it/s, Train_acc=91.3, Train_loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00244 | Gamma1: ['2.29209', '2.27762', '2.28290', '2.27979', '2.28070', '2.28605', '2.27970', '2.29053'] | Gamma1 Grad: ['0.00194', '0.00584', '0.01141', '0.00501', '-0.00091', '0.00351', '-0.00492', '0.00562']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 91.336% (Updated)\n",
      "ğŸ“Š Train Accuracy: 91.336% | ğŸ† Best Train Accuracy: 91.336%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 91.336% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.27it/s, Test_acc=88, Test_loss=0.375]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 88.020% | ğŸ† Best Test Accuracy: 88.310%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.42it/s, Train_acc=92, Train_loss=0.261]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00236 | Gamma1: ['2.27824', '2.28580', '2.27971', '2.29071', '2.28554', '2.29150', '2.27709', '2.29661'] | Gamma1 Grad: ['-0.00046', '0.01614', '0.00435', '-0.01313', '0.01261', '-0.00665', '-0.01336', '-0.00057']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.046% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.046% | ğŸ† Best Train Accuracy: 92.046%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.046% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:02<00:00, 38.00it/s, Test_acc=89.1, Test_loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 89.070% | ğŸ† Best Test Accuracy: 89.070%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.22it/s, Train_acc=92.7, Train_loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00226 | Gamma1: ['2.27521', '2.28582', '2.27547', '2.27263', '2.27694', '2.29963', '2.28744', '2.27519'] | Gamma1 Grad: ['-0.00061', '-0.00166', '-0.00774', '-0.00382', '0.01071', '-0.00903', '0.00744', '0.00608']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 92.670% (Updated)\n",
      "ğŸ“Š Train Accuracy: 92.670% | ğŸ† Best Train Accuracy: 92.670%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 92.670% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.76it/s, Test_acc=89.6, Test_loss=0.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 89.580% | ğŸ† Best Test Accuracy: 89.580%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.36it/s, Train_acc=93, Train_loss=0.227]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00214 | Gamma1: ['2.26945', '2.26893', '2.27901', '2.29139', '2.27904', '2.28695', '2.27519', '2.27903'] | Gamma1 Grad: ['0.00147', '0.00487', '-0.01087', '0.01925', '-0.00031', '0.00539', '0.02254', '-0.00490']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.032% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.032% | ğŸ† Best Train Accuracy: 93.032%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.032% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.85it/s, Test_acc=90, Test_loss=0.318]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 89.970% | ğŸ† Best Test Accuracy: 89.970%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.45it/s, Train_acc=93.5, Train_loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00199 | Gamma1: ['2.26911', '2.28694', '2.28658', '2.27196', '2.28088', '2.28862', '2.28194', '2.28021'] | Gamma1 Grad: ['-0.00294', '-0.00268', '0.00700', '-0.00731', '0.00599', '-0.00327', '0.01450', '0.00015']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 93.532% (Updated)\n",
      "ğŸ“Š Train Accuracy: 93.532% | ğŸ† Best Train Accuracy: 93.532%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 93.532% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.82it/s, Test_acc=90.4, Test_loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 90.410% | ğŸ† Best Test Accuracy: 90.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.39it/s, Train_acc=94.1, Train_loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00182 | Gamma1: ['2.27818', '2.28334', '2.27623', '2.28158', '2.27920', '2.27823', '2.29415', '2.28394'] | Gamma1 Grad: ['0.00147', '-0.00107', '-0.00853', '-0.00208', '-0.00270', '-0.01146', '0.00162', '0.00325']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.134% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.134% | ğŸ† Best Train Accuracy: 94.134%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.134% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.98it/s, Test_acc=90.2, Test_loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 90.170% | ğŸ† Best Test Accuracy: 90.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.38it/s, Train_acc=94.5, Train_loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00164 | Gamma1: ['2.28546', '2.28862', '2.27686', '2.28171', '2.27021', '2.28984', '2.27461', '2.29269'] | Gamma1 Grad: ['0.00171', '0.00130', '-0.00558', '-0.00262', '0.00057', '-0.00278', '0.01315', '0.00805']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.466% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.466% | ğŸ† Best Train Accuracy: 94.466%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.466% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.82it/s, Test_acc=90.4, Test_loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 90.410% | ğŸ† Best Test Accuracy: 90.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.41it/s, Train_acc=94.9, Train_loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00145 | Gamma1: ['2.27897', '2.29088', '2.29131', '2.28956', '2.28821', '2.28523', '2.28661', '2.29359'] | Gamma1 Grad: ['0.00750', '0.00008', '0.00458', '-0.00487', '-0.01697', '0.01593', '0.02820', '-0.00785']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.890% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.890% | ğŸ† Best Train Accuracy: 94.890%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.890% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.16it/s, Test_acc=89.9, Test_loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 89.940% | ğŸ† Best Test Accuracy: 90.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.39it/s, Train_acc=94.9, Train_loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00126 | Gamma1: ['2.27608', '2.28292', '2.29604', '2.28900', '2.28783', '2.28567', '2.28677', '2.28266'] | Gamma1 Grad: ['-0.00616', '-0.00325', '0.00514', '-0.00481', '0.00981', '0.02519', '-0.01494', '0.00281']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 94.922% (Updated)\n",
      "ğŸ“Š Train Accuracy: 94.922% | ğŸ† Best Train Accuracy: 94.922%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 94.922% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 3128 | TestAccHist: 0 | TrainLossHist: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.80it/s, Test_acc=90.2, Test_loss=0.324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 90.210% | ğŸ† Best Test Accuracy: 90.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.37it/s, Train_acc=95.6, Train_loss=0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00106 | Gamma1: ['2.27615', '2.28306', '2.28207', '2.29115', '2.28421', '2.28545', '2.28971', '2.27726'] | Gamma1 Grad: ['-0.00821', '-0.00566', '0.00688', '-0.00589', '-0.01389', '-0.00033', '0.00085', '-0.00192']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 95.614% (Updated)\n",
      "ğŸ“Š Train Accuracy: 95.614% | ğŸ† Best Train Accuracy: 95.614%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.614% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.18it/s, Test_acc=90.1, Test_loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 90.060% | ğŸ† Best Test Accuracy: 90.410%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.44it/s, Train_acc=95.7, Train_loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['2.28159', '2.27872', '2.28257', '2.28209', '2.28349', '2.28364', '2.28202', '2.28007'] | Gamma1 Grad: ['0.00031', '-0.00354', '0.00298', '-0.00798', '-0.00066', '0.00166', '0.00057', '-0.00612']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 95.660% (Updated)\n",
      "ğŸ“Š Train Accuracy: 95.660% | ğŸ† Best Train Accuracy: 95.660%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.660% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.31it/s, Test_acc=90.9, Test_loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 90.900% | ğŸ† Best Test Accuracy: 90.900%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.36it/s, Train_acc=95.9, Train_loss=0.145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00069 | Gamma1: ['2.28306', '2.28131', '2.28559', '2.27898', '2.28654', '2.28446', '2.28175', '2.28373'] | Gamma1 Grad: ['0.01074', '0.00621', '-0.01574', '0.00864', '-0.00840', '0.00459', '-0.00457', '0.00329']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 95.906% (Updated)\n",
      "ğŸ“Š Train Accuracy: 95.906% | ğŸ† Best Train Accuracy: 95.906%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 95.906% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:02<00:00, 37.35it/s, Test_acc=90.9, Test_loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 90.930% | ğŸ† Best Test Accuracy: 90.930%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.46it/s, Train_acc=96.2, Train_loss=0.137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.28160', '2.27961', '2.28263', '2.28548', '2.28271', '2.29673', '2.28226', '2.28153'] | Gamma1 Grad: ['-0.00248', '0.00431', '0.04474', '-0.03670', '-0.01473', '-0.02535', '0.01124', '-0.01103']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 96.244% (Updated)\n",
      "ğŸ“Š Train Accuracy: 96.244% | ğŸ† Best Train Accuracy: 96.244%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 96.244% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.09it/s, Test_acc=91.2, Test_loss=0.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 91.240% | ğŸ† Best Test Accuracy: 91.240%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.36it/s, Train_acc=96.3, Train_loss=0.132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00037 | Gamma1: ['2.28387', '2.27928', '2.27804', '2.28317', '2.28469', '2.28618', '2.28091', '2.29069'] | Gamma1 Grad: ['-0.00677', '-0.00690', '0.02268', '0.02219', '-0.00365', '0.00441', '-0.01214', '0.00237']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 96.304% (Updated)\n",
      "ğŸ“Š Train Accuracy: 96.304% | ğŸ† Best Train Accuracy: 96.304%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 96.304% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.47it/s, Test_acc=90.8, Test_loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 90.850% | ğŸ† Best Test Accuracy: 91.240%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.44it/s, Train_acc=96.5, Train_loss=0.127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00025 | Gamma1: ['2.27892', '2.27652', '2.27588', '2.27866', '2.28298', '2.28430', '2.28834', '2.28515'] | Gamma1 Grad: ['0.00356', '0.00742', '0.02103', '0.00452', '0.02247', '-0.02597', '0.00800', '0.00149']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 96.548% (Updated)\n",
      "ğŸ“Š Train Accuracy: 96.548% | ğŸ† Best Train Accuracy: 96.548%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 96.548% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.44it/s, Test_acc=90.8, Test_loss=0.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 90.790% | ğŸ† Best Test Accuracy: 91.240%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.45it/s, Train_acc=96.8, Train_loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['2.28724', '2.27641', '2.28516', '2.28699', '2.27742', '2.27915', '2.28344', '2.28257'] | Gamma1 Grad: ['-0.00790', '-0.01574', '-0.01035', '0.01048', '0.00599', '-0.00430', '0.00971', '-0.00064']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 96.790% (Updated)\n",
      "ğŸ“Š Train Accuracy: 96.790% | ğŸ† Best Train Accuracy: 96.790%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 96.790% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.87it/s, Test_acc=91.3, Test_loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 91.320% | ğŸ† Best Test Accuracy: 91.320%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.41it/s, Train_acc=97.1, Train_loss=0.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.28467', '2.28206', '2.28064', '2.27966', '2.28192', '2.28632', '2.28464', '2.28709'] | Gamma1 Grad: ['-0.00283', '-0.01668', '-0.04683', '-0.00255', '-0.02173', '0.01956', '0.01028', '-0.00398']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.106% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.106% | ğŸ† Best Train Accuracy: 97.106%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.106% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.16it/s, Test_acc=91.6, Test_loss=0.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 91.630% | ğŸ† Best Test Accuracy: 91.630%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.47it/s, Train_acc=97.1, Train_loss=0.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00003 | Gamma1: ['2.28380', '2.28892', '2.28288', '2.27508', '2.28295', '2.28632', '2.28987', '2.28229'] | Gamma1 Grad: ['-0.00052', '-0.00124', '-0.00262', '0.01534', '-0.01629', '-0.01079', '0.00998', '-0.00507']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 97.054% | ğŸ† Best Train Accuracy: 97.106%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.106% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.05it/s, Test_acc=91.4, Test_loss=0.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.380% | ğŸ† Best Test Accuracy: 91.630%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.50it/s, Train_acc=97.4, Train_loss=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.27217', '2.27609', '2.27885', '2.28536', '2.28559', '2.28430', '2.28388', '2.28056'] | Gamma1 Grad: ['-0.00293', '-0.00458', '-0.00023', '-0.00813', '0.01143', '0.00300', '0.00380', '0.00465']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.434% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.434% | ğŸ† Best Train Accuracy: 97.434%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.434% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 3128 | TestAccHist: 0 | TrainLossHist: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.17it/s, Test_acc=91.5, Test_loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.500% | ğŸ† Best Test Accuracy: 91.630%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.40it/s, Train_acc=97.3, Train_loss=0.103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.26936', '2.28426', '2.27744', '2.28326', '2.29729', '2.28617', '2.29708', '2.28914'] | Gamma1 Grad: ['-0.01726', '-0.00051', '-0.01302', '0.02857', '-0.00106', '-0.00395', '-0.02709', '-0.00941']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 97.304% | ğŸ† Best Train Accuracy: 97.434%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.434% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.21it/s, Test_acc=91.3, Test_loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.330% | ğŸ† Best Test Accuracy: 91.630%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.45it/s, Train_acc=97.5, Train_loss=0.097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00248 | Gamma1: ['2.27392', '2.27635', '2.26759', '2.28506', '2.30258', '2.26849', '2.29084', '2.28322'] | Gamma1 Grad: ['0.01028', '0.00091', '0.00888', '0.03617', '0.00289', '-0.03689', '0.00148', '-0.00965']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.520% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.520% | ğŸ† Best Train Accuracy: 97.520%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.520% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.17it/s, Test_acc=91.2, Test_loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.210% | ğŸ† Best Test Accuracy: 91.630%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.51it/s, Train_acc=97.6, Train_loss=0.092]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00247 | Gamma1: ['2.28208', '2.27918', '2.26963', '2.27774', '2.29028', '2.27433', '2.28136', '2.27174'] | Gamma1 Grad: ['0.00385', '0.00234', '0.00188', '-0.00603', '-0.03025', '0.00947', '-0.00298', '0.00203']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.598% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.598% | ğŸ† Best Train Accuracy: 97.598%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.598% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.59it/s, Test_acc=91.4, Test_loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.410% | ğŸ† Best Test Accuracy: 91.630%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.44it/s, Train_acc=97.5, Train_loss=0.098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00244 | Gamma1: ['2.28753', '2.28948', '2.28246', '2.28132', '2.25792', '2.27404', '2.27943', '2.27135'] | Gamma1 Grad: ['0.00273', '0.00258', '0.00547', '-0.00181', '-0.00464', '0.00205', '-0.00130', '-0.00050']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 97.532% | ğŸ† Best Train Accuracy: 97.598%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.598% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.12it/s, Test_acc=90.8, Test_loss=0.392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 90.830% | ğŸ† Best Test Accuracy: 91.630%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.49it/s, Train_acc=97.9, Train_loss=0.083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00241 | Gamma1: ['2.27851', '2.27595', '2.27604', '2.29238', '2.28160', '2.27694', '2.28180', '2.27755'] | Gamma1 Grad: ['0.00011', '-0.00554', '0.00456', '-0.00441', '0.03653', '-0.00327', '-0.00215', '0.00804']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.946% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.946% | ğŸ† Best Train Accuracy: 97.946%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.946% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.37it/s, Test_acc=91.2, Test_loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.190% | ğŸ† Best Test Accuracy: 91.630%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.44it/s, Train_acc=97.8, Train_loss=0.085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00236 | Gamma1: ['2.28297', '2.28216', '2.29484', '2.26531', '2.30974', '2.28330', '2.27903', '2.28132'] | Gamma1 Grad: ['0.00278', '0.01152', '0.01869', '0.02351', '-0.03471', '-0.01925', '0.02218', '-0.00148']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 97.848% | ğŸ† Best Train Accuracy: 97.946%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.946% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.27it/s, Test_acc=91.7, Test_loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 91.710% | ğŸ† Best Test Accuracy: 91.710%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.46it/s, Train_acc=97.9, Train_loss=0.081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00232 | Gamma1: ['2.27349', '2.28602', '2.28124', '2.28454', '2.29693', '2.29798', '2.29459', '2.28544'] | Gamma1 Grad: ['0.00780', '0.00679', '0.02755', '0.02800', '0.00717', '-0.00197', '0.00599', '-0.01156']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 97.938% | ğŸ† Best Train Accuracy: 97.946%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.946% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.97it/s, Test_acc=91.3, Test_loss=0.362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.280% | ğŸ† Best Test Accuracy: 91.710%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.43it/s, Train_acc=98, Train_loss=0.078]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00226 | Gamma1: ['2.28501', '2.28201', '2.28215', '2.28487', '2.27557', '2.28266', '2.28864', '2.29291'] | Gamma1 Grad: ['0.00154', '-0.00348', '-0.00492', '0.01710', '0.00731', '0.00914', '-0.00023', '0.00214']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 97.988% (Updated)\n",
      "ğŸ“Š Train Accuracy: 97.988% | ğŸ† Best Train Accuracy: 97.988%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 97.988% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.56it/s, Test_acc=91.4, Test_loss=0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.410% | ğŸ† Best Test Accuracy: 91.710%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.43it/s, Train_acc=98.1, Train_loss=0.073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00220 | Gamma1: ['2.27618', '2.28962', '2.27638', '2.27979', '2.30678', '2.27761', '2.29085', '2.26799'] | Gamma1 Grad: ['-0.00514', '-0.00610', '0.02867', '-0.00984', '-0.02613', '0.01753', '0.01331', '-0.00089']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.110% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.110% | ğŸ† Best Train Accuracy: 98.110%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.110% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.23it/s, Test_acc=91.4, Test_loss=0.413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.430% | ğŸ† Best Test Accuracy: 91.710%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.40it/s, Train_acc=98, Train_loss=0.077]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00214 | Gamma1: ['2.27217', '2.27911', '2.28713', '2.29071', '2.30012', '2.29142', '2.30250', '2.28823'] | Gamma1 Grad: ['-0.00094', '-0.00269', '0.00099', '0.00743', '-0.01120', '0.01205', '-0.00271', '0.00138']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 97.978% | ğŸ† Best Train Accuracy: 98.110%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.110% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 3128 | TestAccHist: 0 | TrainLossHist: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.20it/s, Test_acc=91.5, Test_loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.450% | ğŸ† Best Test Accuracy: 91.710%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.51it/s, Train_acc=98.3, Train_loss=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00206 | Gamma1: ['2.25465', '2.26918', '2.29424', '2.29572', '2.29925', '2.29382', '2.30833', '2.25259'] | Gamma1 Grad: ['-0.00065', '0.00205', '-0.00295', '0.01518', '0.01110', '-0.00043', '-0.00347', '0.00082']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.276% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.276% | ğŸ† Best Train Accuracy: 98.276%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.276% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.12it/s, Test_acc=91.4, Test_loss=0.399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.360% | ğŸ† Best Test Accuracy: 91.710%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.40it/s, Train_acc=98.4, Train_loss=0.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00199 | Gamma1: ['2.26897', '2.26985', '2.29653', '2.27228', '2.27420', '2.29337', '2.30099', '2.26747'] | Gamma1 Grad: ['0.00145', '0.00232', '-0.00149', '-0.00932', '0.00365', '0.00156', '-0.00749', '0.00088']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.376% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.376% | ğŸ† Best Train Accuracy: 98.376%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.376% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.41it/s, Test_acc=92.1, Test_loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 92.080% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.49it/s, Train_acc=98.4, Train_loss=0.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00191 | Gamma1: ['2.24837', '2.27053', '2.29257', '2.26392', '2.27337', '2.27959', '2.30932', '2.30129'] | Gamma1 Grad: ['-0.01574', '0.00180', '0.01123', '-0.01284', '-0.05683', '0.01131', '-0.00071', '0.00687']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.368% | ğŸ† Best Train Accuracy: 98.376%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.376% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.83it/s, Test_acc=91.7, Test_loss=0.395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.660% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.50it/s, Train_acc=98.3, Train_loss=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00182 | Gamma1: ['2.26048', '2.24302', '2.27007', '2.27741', '2.28222', '2.29031', '2.32352', '2.28448'] | Gamma1 Grad: ['0.00248', '-0.00141', '0.00179', '-0.00185', '-0.00638', '-0.00363', '0.00240', '0.00049']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.304% | ğŸ† Best Train Accuracy: 98.376%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.376% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.85it/s, Test_acc=91.3, Test_loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.340% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.42it/s, Train_acc=98.3, Train_loss=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00173 | Gamma1: ['2.29080', '2.25370', '2.28630', '2.28300', '2.28269', '2.30930', '2.30834', '2.28281'] | Gamma1 Grad: ['-0.00285', '-0.00215', '-0.01104', '0.00421', '0.00721', '0.00378', '0.00417', '-0.00076']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.316% | ğŸ† Best Train Accuracy: 98.376%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.376% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.20it/s, Test_acc=91.1, Test_loss=0.394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.100% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.45it/s, Train_acc=98.6, Train_loss=0.055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00164 | Gamma1: ['2.26747', '2.25711', '2.26291', '2.28728', '2.28187', '2.28934', '2.30956', '2.26961'] | Gamma1 Grad: ['0.00019', '-0.00069', '-0.00524', '0.00259', '-0.00008', '0.00269', '0.00493', '-0.00168']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.640% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.640% | ğŸ† Best Train Accuracy: 98.640%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.640% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.31it/s, Test_acc=91.8, Test_loss=0.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.770% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.47it/s, Train_acc=98.4, Train_loss=0.064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00155 | Gamma1: ['2.27621', '2.27771', '2.26376', '2.27955', '2.29478', '2.29191', '2.31231', '2.27681'] | Gamma1 Grad: ['-0.00063', '-0.00115', '-0.00776', '0.00375', '0.01344', '-0.01002', '0.00842', '-0.00035']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.400% | ğŸ† Best Train Accuracy: 98.640%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.640% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.74it/s, Test_acc=91.3, Test_loss=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.330% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.43it/s, Train_acc=98.5, Train_loss=0.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00145 | Gamma1: ['2.27337', '2.28289', '2.26895', '2.25738', '2.29396', '2.29262', '2.31539', '2.27496'] | Gamma1 Grad: ['0.00032', '-0.00053', '0.00056', '-0.00366', '0.00025', '0.00098', '0.00456', '-0.00083']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.512% | ğŸ† Best Train Accuracy: 98.640%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.640% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.25it/s, Test_acc=91.4, Test_loss=0.397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.440% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.50it/s, Train_acc=98.8, Train_loss=0.052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00135 | Gamma1: ['2.27963', '2.28460', '2.28498', '2.28191', '2.29350', '2.29931', '2.29531', '2.27606'] | Gamma1 Grad: ['0.00561', '-0.01276', '0.00903', '0.01713', '-0.02264', '0.00033', '-0.00589', '-0.00585']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.752% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.752% | ğŸ† Best Train Accuracy: 98.752%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.752% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.37it/s, Test_acc=91.9, Test_loss=0.39] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.910% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.46it/s, Train_acc=98.7, Train_loss=0.053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00126 | Gamma1: ['2.28230', '2.28461', '2.28522', '2.26867', '2.28993', '2.29589', '2.30533', '2.28876'] | Gamma1 Grad: ['-0.00082', '0.00070', '0.00036', '-0.00255', '0.00057', '-0.00256', '0.00199', '-0.00875']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.652% | ğŸ† Best Train Accuracy: 98.752%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.752% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 3128 | TestAccHist: 0 | TrainLossHist: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:02<00:00, 38.09it/s, Test_acc=91.5, Test_loss=0.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.480% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 51 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.29it/s, Train_acc=98.5, Train_loss=0.058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00116 | Gamma1: ['2.26897', '2.28472', '2.26931', '2.28418', '2.29765', '2.29612', '2.31721', '2.28566'] | Gamma1 Grad: ['0.00220', '-0.00449', '-0.00884', '-0.02194', '-0.05179', '0.01390', '-0.00811', '-0.00064']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.536% | ğŸ† Best Train Accuracy: 98.752%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.752% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.18it/s, Test_acc=91.7, Test_loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.650% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 52 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.36it/s, Train_acc=98.6, Train_loss=0.052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00106 | Gamma1: ['2.28207', '2.30334', '2.29675', '2.29893', '2.28946', '2.28874', '2.32363', '2.28410'] | Gamma1 Grad: ['-0.00234', '0.00102', '0.00874', '0.01214', '-0.02165', '0.01541', '-0.04396', '0.00924']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.636% | ğŸ† Best Train Accuracy: 98.752%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.752% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.33it/s, Test_acc=91.9, Test_loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.930% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 53 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.22it/s, Train_acc=98.8, Train_loss=0.048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00096 | Gamma1: ['2.25416', '2.29591', '2.26731', '2.28904', '2.30224', '2.27516', '2.31913', '2.27626'] | Gamma1 Grad: ['0.00474', '-0.01040', '0.00675', '-0.00948', '0.00483', '-0.02711', '0.00628', '0.00170']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.754% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.754% | ğŸ† Best Train Accuracy: 98.754%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.754% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.00it/s, Test_acc=91.8, Test_loss=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.760% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Stable Noise Injected | Epoch 54 | Batch 0 | Layers: 0 (Î”=0.00000), 1 (Î”=0.00000), 2 (Î”=0.00000), 3 (Î”=0.00000), 4 (Î”=0.00000), 5 (Î”=0.00000), 6 (Î”=0.00000), 7 (Î”=0.00000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.28it/s, Train_acc=98.8, Train_loss=0.049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['2.25991', '2.28362', '2.28326', '2.28721', '2.28195', '2.28333', '2.31777', '2.27878'] | Gamma1 Grad: ['0.00463', '0.00433', '0.02251', '0.02441', '0.04273', '-0.00284', '-0.01069', '-0.00592']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 98.778% (Updated)\n",
      "ğŸ“Š Train Accuracy: 98.778% | ğŸ† Best Train Accuracy: 98.778%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.778% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.72it/s, Test_acc=91.7, Test_loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.740% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.32it/s, Train_acc=98.7, Train_loss=0.051]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00078 | Gamma1: ['2.27767', '2.28273', '2.27729', '2.27999', '2.29474', '2.26947', '2.31423', '2.27740'] | Gamma1 Grad: ['-0.00570', '-0.00026', '0.02958', '-0.01661', '-0.00194', '0.00047', '0.00490', '-0.00702']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.718% | ğŸ† Best Train Accuracy: 98.778%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.778% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.11it/s, Test_acc=91.4, Test_loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.380% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.25it/s, Train_acc=97, Train_loss=0.13]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00069 | Gamma1: ['2.26125', '2.27273', '2.26214', '2.26735', '2.29837', '2.31537', '2.29291', '2.17980'] | Gamma1 Grad: ['-0.01831', '-0.00246', '-0.00944', '-0.03577', '0.00350', '0.03329', '0.00765', '0.02351']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 97.020% | ğŸ† Best Train Accuracy: 98.778%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.778% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.26it/s, Test_acc=91.1, Test_loss=0.418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.110% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.32it/s, Train_acc=98.2, Train_loss=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00060 | Gamma1: ['2.26579', '2.27916', '2.28253', '2.26980', '2.29522', '2.26902', '2.29113', '2.21956'] | Gamma1 Grad: ['-0.00450', '-0.00368', '0.00839', '-0.01117', '-0.01096', '-0.01074', '-0.00213', '0.00401']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.226% | ğŸ† Best Train Accuracy: 98.778%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.778% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.06it/s, Test_acc=91.3, Test_loss=0.444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.260% | ğŸ† Best Test Accuracy: 92.080%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.29it/s, Train_acc=98.7, Train_loss=0.055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['2.27324', '2.28367', '2.28807', '2.26930', '2.29426', '2.28167', '2.28447', '2.23316'] | Gamma1 Grad: ['-0.00396', '0.00200', '0.02542', '0.00090', '0.00463', '-0.00452', '0.00058', '-0.00125']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.682% | ğŸ† Best Train Accuracy: 98.778%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 98.778% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.06it/s, Test_acc=92.3, Test_loss=0.442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 92.280% | ğŸ† Best Test Accuracy: 92.280%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.28it/s, Train_acc=99.1, Train_loss=0.042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00045 | Gamma1: ['2.28856', '2.28531', '2.28517', '2.28439', '2.28117', '2.28679', '2.29404', '2.26170'] | Gamma1 Grad: ['0.00048', '-0.00008', '-0.00007', '0.00043', '0.00154', '-0.00131', '0.00050', '-0.00113']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.112% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.112% | ğŸ† Best Train Accuracy: 99.112%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.112% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.93it/s, Test_acc=92.7, Test_loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 92.650% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.33it/s, Train_acc=99, Train_loss=0.045]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00037 | Gamma1: ['2.27176', '2.28415', '2.28279', '2.28118', '2.26987', '2.29032', '2.31829', '2.25916'] | Gamma1 Grad: ['0.00117', '0.00236', '0.01166', '0.01625', '-0.08084', '0.00208', '-0.00776', '-0.00289']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.004% | ğŸ† Best Train Accuracy: 99.112%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.112% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 3128 | TestAccHist: 0 | TrainLossHist: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.26it/s, Test_acc=92.4, Test_loss=0.422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.400% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.26it/s, Train_acc=98.9, Train_loss=0.047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00031 | Gamma1: ['2.27984', '2.29150', '2.28491', '2.27582', '2.29514', '2.29320', '2.31561', '2.26411'] | Gamma1 Grad: ['0.01096', '0.01102', '-0.01788', '-0.02572', '0.02802', '0.00980', '-0.02912', '0.00303']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.926% | ğŸ† Best Train Accuracy: 99.112%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.112% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.75it/s, Test_acc=92.2, Test_loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.200% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.23it/s, Train_acc=99.2, Train_loss=0.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00025 | Gamma1: ['2.27664', '2.27984', '2.28563', '2.28895', '2.28333', '2.27677', '2.32033', '2.26502'] | Gamma1 Grad: ['0.00276', '-0.00668', '-0.01223', '0.00592', '0.00761', '-0.01506', '0.00321', '-0.00167']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.166% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.166% | ğŸ† Best Train Accuracy: 99.166%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.166% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.21it/s, Test_acc=92, Test_loss=0.434]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.960% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.33it/s, Train_acc=99, Train_loss=0.042]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00019 | Gamma1: ['2.26587', '2.28966', '2.29166', '2.29290', '2.29898', '2.28043', '2.32608', '2.28992'] | Gamma1 Grad: ['0.00270', '0.00034', '-0.00543', '0.00012', '0.00112', '0.00276', '0.00020', '-0.00026']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.050% | ğŸ† Best Train Accuracy: 99.166%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.166% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.19it/s, Test_acc=91.7, Test_loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.660% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.18it/s, Train_acc=98.9, Train_loss=0.047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['2.26892', '2.28456', '2.26588', '2.29231', '2.29427', '2.28412', '2.32424', '2.28079'] | Gamma1 Grad: ['0.00062', '-0.00051', '-0.00076', '0.00101', '-0.00183', '0.00367', '0.00310', '-0.00001']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.878% | ğŸ† Best Train Accuracy: 99.166%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.166% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.84it/s, Test_acc=91.8, Test_loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.800% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.29it/s, Train_acc=98.9, Train_loss=0.046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00010 | Gamma1: ['2.26893', '2.29961', '2.28272', '2.27687', '2.29374', '2.29100', '2.32982', '2.27485'] | Gamma1 Grad: ['0.00710', '0.00596', '-0.07098', '0.03017', '0.03175', '0.03252', '-0.00816', '0.00017']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.900% | ğŸ† Best Train Accuracy: 99.166%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.166% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.27it/s, Test_acc=91.7, Test_loss=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.710% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.32it/s, Train_acc=99.1, Train_loss=0.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00007 | Gamma1: ['2.25952', '2.29599', '2.28229', '2.27307', '2.29416', '2.28924', '2.31944', '2.27695'] | Gamma1 Grad: ['0.00643', '0.00118', '0.00652', '0.01101', '0.00839', '0.00029', '-0.02165', '0.00285']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.080% | ğŸ† Best Train Accuracy: 99.166%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.166% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.00it/s, Test_acc=92.1, Test_loss=0.466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.120% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.26it/s, Train_acc=99, Train_loss=0.042]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00004 | Gamma1: ['2.26537', '2.28960', '2.27794', '2.27316', '2.28131', '2.28697', '2.32145', '2.28388'] | Gamma1 Grad: ['0.00356', '-0.00053', '-0.00534', '-0.00491', '0.00503', '-0.00363', '0.00607', '-0.00133']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.962% | ğŸ† Best Train Accuracy: 99.166%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.166% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.35it/s, Test_acc=92, Test_loss=0.43]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.000% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.30it/s, Train_acc=99, Train_loss=0.044]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00003 | Gamma1: ['2.28093', '2.28341', '2.28351', '2.26207', '2.29254', '2.30019', '2.32443', '2.25405'] | Gamma1 Grad: ['0.00051', '-0.00212', '-0.00221', '0.00467', '0.00701', '-0.02114', '0.00606', '-0.00025']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.000% | ğŸ† Best Train Accuracy: 99.166%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.166% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.41it/s, Test_acc=92, Test_loss=0.405]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.980% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.25it/s, Train_acc=98.4, Train_loss=0.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00001 | Gamma1: ['2.26796', '2.27487', '2.27778', '2.28307', '2.28861', '2.28904', '2.34990', '2.28944'] | Gamma1 Grad: ['0.00532', '0.00234', '-0.00501', '-0.01560', '0.01707', '-0.00617', '0.00689', '-0.00014']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.434% | ğŸ† Best Train Accuracy: 99.166%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.166% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.73it/s, Test_acc=91.9, Test_loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.870% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.29it/s, Train_acc=99, Train_loss=0.043]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.26765', '2.27536', '2.27668', '2.28457', '2.30118', '2.28321', '2.34080', '2.28904'] | Gamma1 Grad: ['-0.00562', '0.00427', '-0.01725', '0.01043', '-0.00109', '0.00334', '-0.00106', '0.00523']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.962% | ğŸ† Best Train Accuracy: 99.166%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.166% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 3128 | TestAccHist: 0 | TrainLossHist: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.87it/s, Test_acc=92.2, Test_loss=0.424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.180% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.33it/s, Train_acc=99.2, Train_loss=0.034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.28137', '2.27045', '2.30999', '2.29029', '2.29277', '2.28996', '2.31385', '2.31242'] | Gamma1 Grad: ['0.01731', '0.00141', '0.01091', '-0.00201', '0.02983', '-0.02638', '0.01664', '-0.00199']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.234% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.234% | ğŸ† Best Train Accuracy: 99.234%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.234% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.98it/s, Test_acc=91.9, Test_loss=0.459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.890% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.32it/s, Train_acc=99.1, Train_loss=0.037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00250 | Gamma1: ['2.26745', '2.26755', '2.29824', '2.27880', '2.28569', '2.26799', '2.32076', '2.28365'] | Gamma1 Grad: ['0.00928', '0.00771', '0.01141', '0.02084', '0.02847', '-0.04310', '0.00688', '0.00325']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.112% | ğŸ† Best Train Accuracy: 99.234%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.234% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.96it/s, Test_acc=92.4, Test_loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.440% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.25it/s, Train_acc=99.1, Train_loss=0.037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00249 | Gamma1: ['2.28988', '2.28698', '2.28553', '2.27921', '2.28594', '2.27111', '2.32404', '2.28510'] | Gamma1 Grad: ['0.00141', '-0.00089', '0.00555', '0.00756', '0.01271', '0.00149', '0.00443', '0.00029']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.126% | ğŸ† Best Train Accuracy: 99.234%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.234% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.29it/s, Test_acc=91.8, Test_loss=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 91.760% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.35it/s, Train_acc=99.1, Train_loss=0.037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00248 | Gamma1: ['2.28400', '2.28072', '2.26427', '2.29446', '2.26269', '2.29057', '2.31064', '2.28012'] | Gamma1 Grad: ['-0.00026', '-0.00074', '0.00041', '-0.00046', '0.00153', '0.00065', '0.00030', '0.00005']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.136% | ğŸ† Best Train Accuracy: 99.234%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.234% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.89it/s, Test_acc=92.2, Test_loss=0.431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.220% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.25it/s, Train_acc=99.2, Train_loss=0.034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00248 | Gamma1: ['2.27491', '2.28395', '2.25509', '2.30007', '2.28176', '2.28275', '2.34454', '2.26962'] | Gamma1 Grad: ['-0.00122', '0.00258', '0.01151', '-0.01646', '-0.01743', '0.00268', '-0.00804', '-0.00193']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.196% | ğŸ† Best Train Accuracy: 99.234%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.234% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.14it/s, Test_acc=92.1, Test_loss=0.454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.100% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.35it/s, Train_acc=99.2, Train_loss=0.033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00247 | Gamma1: ['2.28417', '2.28424', '2.29260', '2.30608', '2.30951', '2.30116', '2.33143', '2.28236'] | Gamma1 Grad: ['0.00145', '0.00189', '0.00203', '0.01393', '0.01560', '-0.00015', '0.01153', '0.00430']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.208% | ğŸ† Best Train Accuracy: 99.234%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.234% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.13it/s, Test_acc=92.1, Test_loss=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.140% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.38it/s, Train_acc=98.9, Train_loss=0.044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00245 | Gamma1: ['2.26013', '2.26668', '2.26351', '2.27065', '2.28829', '2.28506', '2.33026', '2.30960'] | Gamma1 Grad: ['-0.00627', '-0.00616', '-0.00968', '0.00450', '-0.01123', '0.00529', '0.00518', '0.00073']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 98.874% | ğŸ† Best Train Accuracy: 99.234%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.234% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.16it/s, Test_acc=92, Test_loss=0.443]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.010% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.32it/s, Train_acc=99.2, Train_loss=0.033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00244 | Gamma1: ['2.27658', '2.28860', '2.29231', '2.29436', '2.28121', '2.29418', '2.33656', '2.29060'] | Gamma1 Grad: ['0.00285', '-0.00254', '0.04607', '-0.03355', '-0.02178', '-0.03454', '0.01016', '0.00311']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.242% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.242% | ğŸ† Best Train Accuracy: 99.242%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.242% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.21it/s, Test_acc=92.1, Test_loss=0.451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.110% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.28it/s, Train_acc=99.2, Train_loss=0.035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00242 | Gamma1: ['2.26221', '2.28662', '2.28931', '2.28701', '2.28579', '2.27491', '2.33320', '2.27940'] | Gamma1 Grad: ['0.00173', '0.00579', '0.00697', '-0.00020', '-0.02352', '0.00572', '-0.01108', '0.00028']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.204% | ğŸ† Best Train Accuracy: 99.242%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.242% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.98it/s, Test_acc=92.2, Test_loss=0.419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 92.150% | ğŸ† Best Test Accuracy: 92.650%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.29it/s, Train_acc=99.6, Train_loss=0.023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00241 | Gamma1: ['2.26118', '2.28004', '2.30241', '2.29388', '2.27122', '2.31155', '2.30867', '2.27703'] | Gamma1 Grad: ['-0.00113', '-0.00043', '0.00355', '0.00043', '-0.00051', '0.00195', '-0.00083', '0.00033']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.556% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.556% | ğŸ† Best Train Accuracy: 99.556%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.556% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 3128 | TestAccHist: 0 | TrainLossHist: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.99it/s, Test_acc=93.4, Test_loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 93.380% | ğŸ† Best Test Accuracy: 93.380%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.25it/s, Train_acc=99.8, Train_loss=0.017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00239 | Gamma1: ['2.26945', '2.28080', '2.29208', '2.28951', '2.26957', '2.29984', '2.29608', '2.28220'] | Gamma1 Grad: ['0.00010', '0.00010', '0.00021', '0.00014', '-0.00017', '0.00017', '0.00021', '0.00017']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.780% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.780% | ğŸ† Best Train Accuracy: 99.780%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.780% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.27it/s, Test_acc=93.2, Test_loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.230% | ğŸ† Best Test Accuracy: 93.380%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.36it/s, Train_acc=99.8, Train_loss=0.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00236 | Gamma1: ['2.27999', '2.29808', '2.27889', '2.27784', '2.28573', '2.27768', '2.26830', '2.28163'] | Gamma1 Grad: ['-0.00340', '0.00030', '-0.00329', '0.00000', '-0.00209', '-0.00197', '0.00199', '-0.00005']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.838% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.838% | ğŸ† Best Train Accuracy: 99.838%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.838% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.00it/s, Test_acc=93.4, Test_loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 93.390% | ğŸ† Best Test Accuracy: 93.390%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.26it/s, Train_acc=99.9, Train_loss=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00234 | Gamma1: ['2.27991', '2.28903', '2.28131', '2.26605', '2.27130', '2.29389', '2.29130', '2.28080'] | Gamma1 Grad: ['0.00312', '-0.00007', '0.00577', '0.00024', '0.00255', '0.00112', '0.00795', '-0.00292']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.898% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.898% | ğŸ† Best Train Accuracy: 99.898%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.898% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.29it/s, Test_acc=93.6, Test_loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 93.620% | ğŸ† Best Test Accuracy: 93.620%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.30it/s, Train_acc=99.9, Train_loss=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00232 | Gamma1: ['2.26650', '2.28467', '2.26004', '2.26329', '2.28279', '2.29687', '2.28591', '2.28657'] | Gamma1 Grad: ['-0.00033', '0.00053', '-0.00346', '-0.00039', '-0.00287', '0.00050', '-0.00043', '-0.00017']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.898% | ğŸ† Best Train Accuracy: 99.898%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.898% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.45it/s, Test_acc=93.6, Test_loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.560% | ğŸ† Best Test Accuracy: 93.620%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.33it/s, Train_acc=99.9, Train_loss=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00229 | Gamma1: ['2.28237', '2.27312', '2.26747', '2.27868', '2.28784', '2.28757', '2.28574', '2.28373'] | Gamma1 Grad: ['0.00001', '-0.00036', '-0.00050', '-0.00000', '-0.00044', '-0.00043', '0.00002', '-0.00016']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.922% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.922% | ğŸ† Best Train Accuracy: 99.922%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.922% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.96it/s, Test_acc=93.7, Test_loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 93.720% | ğŸ† Best Test Accuracy: 93.720%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.22it/s, Train_acc=99.9, Train_loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00226 | Gamma1: ['2.28006', '2.27766', '2.28343', '2.27682', '2.29580', '2.28938', '2.27760', '2.28168'] | Gamma1 Grad: ['-0.00009', '-0.00006', '0.00018', '-0.00015', '0.00035', '-0.00019', '-0.00012', '-0.00004']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.942% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.942% | ğŸ† Best Train Accuracy: 99.942%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.942% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.98it/s, Test_acc=93.7, Test_loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.690% | ğŸ† Best Test Accuracy: 93.720%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.28it/s, Train_acc=99.9, Train_loss=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00223 | Gamma1: ['2.27833', '2.26880', '2.26960', '2.28449', '2.30113', '2.29352', '2.27975', '2.28233'] | Gamma1 Grad: ['-0.00206', '0.00096', '-0.00002', '0.00023', '-0.00387', '0.00220', '-0.00724', '0.00001']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.912% | ğŸ† Best Train Accuracy: 99.942%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.942% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.20it/s, Test_acc=94, Test_loss=0.361]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† Saving best model...\n",
      "Checkpoint saved: ./checkpoint/CIFAR10_B128_LR0_001_FFTGate_SENet_Adam.t7\n",
      "ğŸ“Š Test Accuracy: 94.040% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.29it/s, Train_acc=99.9, Train_loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00220 | Gamma1: ['2.27540', '2.26618', '2.27393', '2.28527', '2.28463', '2.28203', '2.27070', '2.28207'] | Gamma1 Grad: ['-0.00034', '-0.00009', '-0.00063', '0.00014', '0.00009', '0.00116', '-0.00161', '0.00003']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.948% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.948% | ğŸ† Best Train Accuracy: 99.948%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.948% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.79it/s, Test_acc=93.7, Test_loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.680% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.22it/s, Train_acc=99.9, Train_loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00217 | Gamma1: ['2.27947', '2.26993', '2.29092', '2.28465', '2.26704', '2.27704', '2.27654', '2.28562'] | Gamma1 Grad: ['0.00010', '-0.00006', '0.00002', '0.00029', '-0.00038', '-0.00017', '-0.00006', '0.00009']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.948% | ğŸ† Best Train Accuracy: 99.948%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.948% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.24it/s, Test_acc=94, Test_loss=0.367]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.960% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.34it/s, Train_acc=100, Train_loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00214 | Gamma1: ['2.28231', '2.27335', '2.26243', '2.27436', '2.26331', '2.28927', '2.26267', '2.28346'] | Gamma1 Grad: ['0.00015', '-0.00003', '-0.00047', '-0.00015', '-0.00082', '0.00028', '-0.00021', '0.00014']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.958% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.958% | ğŸ† Best Train Accuracy: 99.958%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.958% (Updated)\n",
      "ğŸ“ Sizes â†’ ActivationHist: 3128 | TestAccHist: 0 | TrainLossHist: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.84it/s, Test_acc=93.8, Test_loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.810% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.23it/s, Train_acc=100, Train_loss=0.012] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00210 | Gamma1: ['2.28032', '2.28594', '2.27068', '2.28373', '2.27873', '2.30477', '2.26120', '2.28500'] | Gamma1 Grad: ['0.00019', '0.00033', '-0.00100', '0.00055', '0.00028', '0.00057', '-0.00034', '0.00010']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.950% | ğŸ† Best Train Accuracy: 99.958%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.958% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.29it/s, Test_acc=93.7, Test_loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.700% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.33it/s, Train_acc=99.9, Train_loss=0.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00206 | Gamma1: ['2.27359', '2.27992', '2.28163', '2.29429', '2.28810', '2.28260', '2.27202', '2.28411'] | Gamma1 Grad: ['-0.00011', '-0.00011', '0.00025', '-0.00018', '-0.00006', '0.00009', '0.00044', '0.00005']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.946% | ğŸ† Best Train Accuracy: 99.958%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.958% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.10it/s, Test_acc=93.7, Test_loss=0.386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.720% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.35it/s, Train_acc=99.9, Train_loss=0.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00203 | Gamma1: ['2.28337', '2.27567', '2.28205', '2.28470', '2.29185', '2.27171', '2.27948', '2.28412'] | Gamma1 Grad: ['0.00012', '-0.00021', '-0.00005', '0.00010', '0.00017', '0.00014', '-0.00027', '-0.00005']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.948% | ğŸ† Best Train Accuracy: 99.958%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.958% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.05it/s, Test_acc=93.8, Test_loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.820% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.24it/s, Train_acc=100, Train_loss=0.011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00199 | Gamma1: ['2.28671', '2.26930', '2.28502', '2.28125', '2.28795', '2.27414', '2.27405', '2.27846'] | Gamma1 Grad: ['-0.00016', '-0.00003', '0.00021', '0.00086', '0.00155', '-0.00116', '-0.00125', '0.00000']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.968% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.968% | ğŸ† Best Train Accuracy: 99.968%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.968% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.09it/s, Test_acc=93.7, Test_loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.660% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.21it/s, Train_acc=100, Train_loss=0.011] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00195 | Gamma1: ['2.28552', '2.28448', '2.28211', '2.29226', '2.28950', '2.29893', '2.28085', '2.28327'] | Gamma1 Grad: ['0.00010', '0.00026', '0.00005', '0.00018', '0.00028', '0.00058', '0.00053', '0.00012']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.954% | ğŸ† Best Train Accuracy: 99.968%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.968% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.28it/s, Test_acc=93.6, Test_loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.570% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.31it/s, Train_acc=100, Train_loss=0.011] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00191 | Gamma1: ['2.27869', '2.27538', '2.28752', '2.27614', '2.29528', '2.27881', '2.27768', '2.27617'] | Gamma1 Grad: ['0.00041', '-0.00015', '-0.00063', '-0.00023', '0.00067', '0.00072', '-0.00044', '-0.00017']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.950% | ğŸ† Best Train Accuracy: 99.968%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.968% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.82it/s, Test_acc=93.8, Test_loss=0.394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.790% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.19it/s, Train_acc=100, Train_loss=0.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00186 | Gamma1: ['2.29303', '2.27191', '2.28548', '2.28109', '2.27268', '2.28634', '2.27220', '2.28062'] | Gamma1 Grad: ['0.00031', '-0.00009', '0.00005', '0.00014', '-0.00040', '0.00012', '-0.00021', '0.00005']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ† New Best Training Accuracy: 99.974% (Updated)\n",
      "ğŸ“Š Train Accuracy: 99.974% | ğŸ† Best Train Accuracy: 99.974%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.974% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.05it/s, Test_acc=93.9, Test_loss=0.395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.920% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:41<00:00,  9.34it/s, Train_acc=100, Train_loss=0.011] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00182 | Gamma1: ['2.28529', '2.28005', '2.26908', '2.28352', '2.28687', '2.26371', '2.27421', '2.28134'] | Gamma1 Grad: ['-0.00004', '0.00000', '-0.00023', '-0.00036', '0.00024', '-0.00058', '-0.00012', '-0.00007']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.952% | ğŸ† Best Train Accuracy: 99.974%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.974% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 40.03it/s, Test_acc=93.6, Test_loss=0.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.610% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:42<00:00,  9.29it/s, Train_acc=100, Train_loss=0.01]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: M_Optimizer LR => 0.00010 | Gamma1 LR => 0.00178 | Gamma1: ['2.28648', '2.28288', '2.25542', '2.27533', '2.29050', '2.27850', '2.26377', '2.28312'] | Gamma1 Grad: ['0.00031', '0.00029', '-0.00047', '-0.00001', '0.00013', '0.00022', '-0.00023', '0.00018']\n",
      "ğŸ“œ Logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\FFTGate\\FFTGate_training_logs.txt!\n",
      "ğŸ“Š Train Accuracy: 99.960% | ğŸ† Best Train Accuracy: 99.974%\n",
      "ğŸ“œ Training logs saved to ./Results/CIFAR10_Train_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "ğŸ† Best Training Accuracy: 99.974% (Updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 39.98it/s, Test_acc=93.6, Test_loss=0.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Accuracy: 93.620% | ğŸ† Best Test Accuracy: 94.040%\n",
      "ğŸ“œ Test logs saved to C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\CIFAR10\\SENet18\\Results\\CIFAR10_Test_B128_LR0.001_FFTGate_SENet_Adam.txt!\n",
      "\n",
      "Best Test Accuracy:  94.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "####-------| NOTE 11. TRAIN MODEL WITH SHEDULAR | XXX ----------------------------------------------####################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Set Seed for Reproducibility BEFORE training starts\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)   \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(1)  \n",
    "\n",
    "\n",
    "\n",
    "# âœ… Training Loop\n",
    "num_epochs = 100 # Example: Set the total number of epochs\n",
    "for epoch in range(start_epoch, num_epochs):   # Runs training for 100 epochs\n",
    "\n",
    "    train(epoch, optimizer, activation_optimizers, activation_schedulers, unfreeze_activation_epoch, main_scheduler, WARMUP_ACTIVATION_EPOCHS) # âœ… Pass required arguments\n",
    "\n",
    "    test(epoch)  # âœ… Test the model\n",
    "    tqdm.write(\"\")  # âœ… Clear leftover progress bar from test()\n",
    "\n",
    "\n",
    "print(\"Best Test Accuracy: \", best_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
