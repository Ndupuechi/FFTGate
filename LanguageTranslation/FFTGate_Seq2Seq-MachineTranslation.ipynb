{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to pytorch_env (Python 3.10.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ed8f1-1752-4854-b08a-bb03642f92a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizers are ready.\n",
      "‚úÖ Training examples: 29000\n",
      "‚úÖ Validation examples: 1014\n",
      "‚úÖ Test examples: 1000\n",
      "\n",
      "‚úÖ Sample:\n",
      "{'src': ['zwei', 'junge', 'wei√üe', 'm√§nner', 'sind', 'im', 'freien', 'in', 'der', 'n√§he', 'vieler', 'b√ºsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emeka\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ === LOAD LIBRARIES ===\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ === SET SEED FOR REPRODUCIBILITY ===\n",
    "def set_seed_torch(seed):\n",
    "    torch.manual_seed(seed)                          \n",
    "\n",
    "\n",
    "def set_seed_main(seed):\n",
    "    random.seed(seed)                                ## Python's random module\n",
    "    np.random.seed(seed)                             ## NumPy's random module\n",
    "    torch.cuda.manual_seed(seed)                     ## PyTorch's random module for CUDA\n",
    "    torch.cuda.manual_seed_all(seed)                 ## Seed for all CUDA devices\n",
    "    torch.backends.cudnn.deterministic = True        ## Ensure deterministic behavior for CuDNN\n",
    "    torch.backends.cudnn.benchmark = False           ## Disable CuDNN's autotuning for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "# Variable seed for DataLoader shuffling\n",
    "set_seed_torch(1)   \n",
    "\n",
    "# Variable main seed (model, CUDA, etc.)\n",
    "set_seed_main(1)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ === DEFINE PATHS ===\n",
    "data_path = r\"C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\LanguageTranslation\\data\"\n",
    "multi30k_dir = os.path.join(data_path, \"multi30k\")\n",
    "os.makedirs(multi30k_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# MOVE FILES INTO multi30k if needed\n",
    "# train.de, train.en, val.de, val.en, test.de, test.en\n",
    "# Must be in: ...\\data\\multi30k\\\n",
    "\n",
    "\n",
    "# ‚úÖ === LOAD AND TOKENIZE ====\n",
    "os.system(\"python -m spacy download de_core_news_sm\")\n",
    "os.system(\"python -m spacy download en_core_web_sm\")\n",
    "spacy_ger = spacy.load('de_core_news_sm')\n",
    "spacy_eng = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ === TOKENIZERS ===\n",
    "# Tokenization of German Language\n",
    "def tokenize_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "\n",
    "# Tokenization of English Language\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "print(\"‚úÖ Tokenizers are ready.\")\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ === PREPROCESSING OF TEXT | DEFINE FIELDS ===\n",
    "# Applyling Tokenization , lowercase and special Tokens for preprocessing\n",
    "german = Field(tokenize = tokenize_ger,lower = True,init_token = '<sos>',eos_token = '<eos>')\n",
    "english = Field(tokenize = tokenize_eng,lower = True,init_token = '<sos>',eos_token = '<eos>')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ ===  LOAD DATASET ===\n",
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"),\n",
    "    fields=(german, english),\n",
    "    root=data_path,\n",
    "    test=\"test\"  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "## ‚úÖ === BUILD VOCAB | CREATING VOVABULARY IN EACH LANGUAGE ===  \n",
    "german.build_vocab(train_data,max_size = 10000,min_freq = 2)\n",
    "english.build_vocab(train_data,max_size = 10000,min_freq = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Print samples ===\n",
    "# === STEP 7: Confirm ===\n",
    "print(f\"‚úÖ Training examples: {len(train_data)}\")\n",
    "print(f\"‚úÖ Validation examples: {len(valid_data)}\")\n",
    "print(f\"‚úÖ Test examples: {len(test_data)}\\n\")\n",
    "print(\"‚úÖ Sample:\")\n",
    "print(vars(train_data[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ === FFTGate ACTIVATION FUNCTION ‚úÖ ===\n",
    "\n",
    "class FFTGate(nn.Module):\n",
    "    \"\"\"\n",
    "    FFTGate Activation Function\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    gamma1 : float\n",
    "        Trainable scaling factor applied to pre-activation input (controls gating sharpness).\n",
    "        Initialized as a learnable parameter.\n",
    "\n",
    "    phi : float\n",
    "        Frequency of the sinusoidal perturbation added to the gated activation output to prevent\n",
    "        neuron saturation and improve learning smoothness.\n",
    "\n",
    "    history_len : int\n",
    "        Temporal activation history window length (T), i.e., number of past activation states stored\n",
    "        for frequency-domain analysis.\n",
    "\n",
    "    enable_history : bool\n",
    "        Enables or disables tracking of temporal activation history. If False, FFT-based modulation is skipped.\n",
    "\n",
    "    use_skip : bool\n",
    "        Toggles an optional residual skip connection that blends the input and modulated output (0.95 * activation + 0.05 * input).\n",
    "\n",
    "    decay_mode : str\n",
    "        Temporal Activation History Decay Strategy:\n",
    "        - \"exp\"    ‚Üí Exponential decay strategy (cosine-based, monotonic)\n",
    "        - \"linear\" ‚Üí Non-monotonic decay strategy using sigmoid-blended multi-phase decay\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, gamma1=1.5, phi=0.1, history_len=12,\n",
    "                 enable_history=True, use_skip=False, decay_mode=\"linear\"):    \n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "        # üîß Trainable scaling factor\n",
    "        # self.gamma1 = nn.Parameter(torch.tensor(gamma1, device=self.device))\n",
    "        self.gamma1 = nn.Parameter(torch.full((1,), gamma1, device=self.device))\n",
    "\n",
    "\n",
    "        # üîí Fixed buffer phi (No gradient updates)\n",
    "        self.register_buffer('phi', torch.tensor(phi, device=self.device))\n",
    "\n",
    "        self.history_len = history_len\n",
    "        self.enable_history = enable_history\n",
    "        self.activation_history_initialized = False\n",
    "\n",
    "        # ‚öôÔ∏è Optional skip connection toggle\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        # ‚úÖ Save selected decay strategy\n",
    "        self.decay_mode = decay_mode\n",
    "\n",
    "\n",
    "    # üîç Computes sigmoid-blended multi-phase decay rate used for non-monotonic history scaling | non-monotonic decay stratey defination\n",
    "    def sigmoid_blended_decay(self, epoch, num_epochs,\n",
    "                            t1=50, t2=70,\n",
    "                            k1=0.25, k2=0.5,\n",
    "                            a=0.980, b=0.995, c=0.9801):\n",
    "        \"\"\"üìâ Sigmoid-based phase-decay transition\"\"\"\n",
    "\n",
    "        # Rescale current epoch to fit 0‚Äì100 reference scale\n",
    "        epoch_scaled = (epoch / num_epochs) * 100\n",
    "\n",
    "        s1 = 1 / (1 + math.exp(-k1 * (epoch_scaled - t1)))\n",
    "        s2 = 1 / (1 + math.exp(-k2 * (epoch_scaled - t2)))\n",
    "        return a * (1 - s1) + b * s1 * (1 - s2) + c * s2\n",
    "\n",
    "\n",
    "    # üîç Tracks moving average of pre-activation inputs over a history window (per channel)\n",
    "    def update_history(self, x):\n",
    "        if not self.enable_history:\n",
    "            return\n",
    "\n",
    "        # [seq_len, batch_size, embedding_size] ‚Üí average across seq_len and emb\n",
    "        if x.dim() == 3:\n",
    "            x = x.mean(dim=(0, 2))    # ‚Üí [batch_size]\n",
    "        elif x.dim() == 2:\n",
    "            x = x.mean(dim=0)\n",
    "        elif x.dim() == 4:\n",
    "            x = x.mean(dim=(0, 2, 3))  # for CNNs\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input shape {x.shape}\")\n",
    "\n",
    "        x = x.detach().to(self.device)\n",
    "\n",
    "        # üîÅ Reset buffer if shape changes | üî• Dynamically reset history buffer if shape changes\n",
    "        if not self.activation_history_initialized or self.activation_history.shape[1] != x.shape[0]:\n",
    "            self.register_buffer(\"activation_history\", torch.zeros(self.history_len, x.shape[0], device=self.device))\n",
    "            self.activation_history_initialized = True\n",
    "\n",
    "        self.activation_history = torch.cat([self.activation_history[1:], x.unsqueeze(0)])\n",
    "\n",
    "\n",
    "    # üîç Decays the stored activation history based on selected temporal decay strategy\n",
    "    def decay_spectral_history(self, epoch, num_epochs=50):\n",
    "        if not (self.enable_history and self.activation_history_initialized):\n",
    "            return\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.decay_mode == \"linear\":                                 # ‚úÖ Use Non-Monotonic decay Strategy   \n",
    "                # ‚úÖ Use sigmoid-based blended decay\n",
    "                decay_rate = self.sigmoid_blended_decay(epoch, num_epochs)  \n",
    "                self.activation_history *= decay_rate\n",
    "\n",
    "            elif self.decay_mode == \"exp\":                                  # ‚úÖ Use Exponential decay Strategy \n",
    "                decay_factor = 0.99 + 0.01 * math.cos(math.pi * epoch / num_epochs)\n",
    "                self.activation_history *= decay_factor\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown decay_mode: {self.decay_mode}. Use 'exp' or 'linear'.\")\n",
    "\n",
    "\n",
    "    def forward(self, x, epoch=0):\n",
    "        x = x.to(self.device)\n",
    "        self.saved_output = x.clone().detach()\n",
    "\n",
    "        if self.enable_history:\n",
    "            self.update_history(x)\n",
    "\n",
    "        # üîç Applies FFT to decayed activation history and computes average magnitude\n",
    "        if self.enable_history and self.activation_history_initialized:\n",
    "            freq_response = torch.fft.fft(self.activation_history, dim=0)\n",
    "            freq_magnitude = torch.abs(freq_response).mean(dim=0)\n",
    "            freq_magnitude = torch.clamp(freq_magnitude, min=0.05, max=1.5)\n",
    "\n",
    "            # üîß Smoothing across channels to prevent spikes              \n",
    "            smoothing_factor = max(0.1, 1 / (epoch + 10))\n",
    "            freq_magnitude = (1 - smoothing_factor) * freq_magnitude + smoothing_factor * freq_magnitude.mean()\n",
    "            # freq_magnitude = freq_magnitude.view(1, -1, 1, 1)\n",
    "            freq_magnitude = freq_magnitude.view(1, -1, 1) # match [seq_len, batch, emb_dim]\n",
    "        else:\n",
    "            freq_magnitude = torch.zeros_like(x)\n",
    "\n",
    "\n",
    "        # ‚úÖ Clamp gamma1 internally for stability\n",
    "        gamma1 = torch.clamp(self.gamma1, min=0.1, max=6.0)\n",
    "\n",
    "        # üîÑ Gate using FFT-derived frequency magnitude\n",
    "        freq_factor = min(0.3 + 0.007 * epoch, 0.8)\n",
    "        gate = torch.sigmoid(gamma1 * x - freq_factor * freq_magnitude)\n",
    "\n",
    "        # ‚úÖ Main activation logic: gated + sinusoidal regularization\n",
    "        activation = x * gate + 0.05 * torch.sin(self.phi * x)  \n",
    "\n",
    "\n",
    "        # üîÅ Optional: smart residual skip blend\n",
    "        if self.use_skip:\n",
    "            activation = 0.95 * activation + 0.05 * x\n",
    "\n",
    "\n",
    "        return activation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ === DEFINE REGULARIZATION FUNCTION ===\n",
    "def apply_dynamic_regularization(net, inputs, feature_activations, epoch, num_epochs,\n",
    "                                  prev_params, layer_index_map, gamma1_history,\n",
    "                                  batch_idx, total_batches, test_acc):\n",
    "\n",
    "\n",
    "    global activation_layers  # ‚úÖ Reference already-collected layers\n",
    "\n",
    "\n",
    "    if batch_idx == 0 and epoch <= 2:\n",
    "        print(f\"\\nüö® ENTERED apply_dynamic_regularization | Epoch={epoch} | Batch={batch_idx}\", flush=True)\n",
    "\n",
    "        # üß† Print all gamma1 stats in one line (once per batch)\n",
    "        all_layer_info = []\n",
    "        for idx, layer in enumerate(activation_layers):\n",
    "            param = getattr(layer, \"gamma1\")\n",
    "            all_layer_info.append(f\"Layer {idx}: ID={id(param)} | Mean={param.mean().item():.5f}\")\n",
    "        print(\"üß† GAMMA1 INFO:\", \" | \".join(all_layer_info), flush=True)\n",
    "\n",
    "    # ‚úÖ Initialize gamma1 regularization accumulator\n",
    "    gamma1_reg = 0.0\n",
    "\n",
    "    # ‚úÖ Compute batch std and define regularization strength\n",
    "    batch_std = torch.std(inputs.float()) + 1e-6\n",
    "    regularization_strength = 0.05 if epoch < 20 else (0.01 if epoch < 35 else 0.005)\n",
    "\n",
    "    # ‚úÖ Track layers where noise is injected (informative)\n",
    "    noisy_layers = []\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        if idx not in layer_index_map:\n",
    "            continue\n",
    "\n",
    "        prev_layer_params = prev_params[layer_index_map[idx]]\n",
    "        param_name = \"gamma1\"  # ‚úÖ Only gamma1 is trainable\n",
    "        param = getattr(layer, param_name)\n",
    "        prev_param = prev_layer_params[param_name]\n",
    "\n",
    "        # ‚úÖ Target based on input stats\n",
    "        target = compute_target(param_name, batch_std)\n",
    "\n",
    "        # ‚úÖ Adaptive Target Regularization\n",
    "        gamma1_reg += regularization_strength * (param - target).pow(2).mean() * 1.2\n",
    "\n",
    "        # ‚úÖ Adaptive Cohesion Regularization\n",
    "        cohesion = (param - prev_param).pow(2)  \n",
    "        gamma1_reg += 0.005 * cohesion.mean()  \n",
    "\n",
    "        # ‚úÖ Adaptive Noise Regularization\n",
    "        epoch_AddNoise = 25\n",
    "        if epoch > epoch_AddNoise:\n",
    "            param_variation = torch.abs(param - prev_param).mean()\n",
    "            if param_variation < 0.015:  \n",
    "                noise = (0.001 + 0.0004 * batch_std.item()) * torch.randn_like(param)\n",
    "                penalty = (param - (prev_param + noise)).pow(2).sum()\n",
    "                gamma1_reg += 0.00015 * penalty                  \n",
    "                noisy_layers.append(f\"{idx} (Œî={param_variation.item():.5f})\") # ‚úÖ Collect index and variation\n",
    "\n",
    "    # ‚úÖ Print noise injection summary\n",
    "    if batch_idx == 0 and epoch <= (epoch_AddNoise+2) and noisy_layers:\n",
    "        print(f\"üî• Stable Noise Injected | Epoch {epoch} | Batch {batch_idx} | Layers: \" + \", \".join(noisy_layers), flush=True)\n",
    "    mags = feature_activations.abs().mean(dim=(0, 1)) \n",
    "    m = mags / mags.sum()\n",
    "    gamma1_reg += 0.005 * (-(m * torch.log(m + 1e-6)).sum())\n",
    "\n",
    "    return gamma1_reg\n",
    "\n",
    "\n",
    "def compute_target(param_name, batch_std):\n",
    "    if param_name == \"gamma1\":\n",
    "        return 2.0 + 0.2 * batch_std.item()     \n",
    "    \n",
    "    raise ValueError(f\"Unknown param {param_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ === DEFINING THE ENCODER PART OF THE MODEL === \n",
    "# === Encoder with FFTGate ===\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        \n",
    "        # ‚úÖ Your activation with all parameters\n",
    "        self.activation = FFTGate(gamma1=1.5, phi=0.1, history_len=12, decay_mode=\"linear\")\n",
    "\n",
    "    def forward(self, x, epoch=0):\n",
    "        embedding = self.dropout(self.activation(self.embedding(x), epoch=epoch))\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ ===  DEFINING THE DECODER PART OF THE MODEL === \n",
    "# === Decoder with FFTGate ===\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # ‚úÖ Your activation with all parameters\n",
    "        self.activation = FFTGate(gamma1=1.5, phi=0.1, history_len=12, decay_mode=\"linear\")\n",
    "\n",
    "    def forward(self, x, hidden, cell, epoch=0):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.dropout(self.activation(self.embedding(x), epoch=epoch))\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        predictions = self.fc(outputs).squeeze(0)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ === DeEFINING THE COMPLETE MODEL === \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5, epoch=0):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source, epoch=epoch)  # <-- Pass epoch\n",
    "\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell, epoch=epoch)  # <-- Pass epoch\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "writer = SummaryWriter(f'runs/Loss_plot')\n",
    "step = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "     batch_size = batch_size, sort_within_batch = True, \n",
    "     sort_key = lambda x:len(x.src),\n",
    "     device = device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder, \n",
    "                      encoder_embedding_size,\n",
    "                      hidden_size,num_layers, \n",
    "                      enc_dropout).to(device)\n",
    "\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, \n",
    "                      decoder_embedding_size,\n",
    "                      hidden_size,output_size,num_layers, \n",
    "                      dec_dropout).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ====== üîß BUILD MODEL üîß ======\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "pad_idx = english.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ====== ‚úÖ FFTGate HANDLING ‚úÖ ======\n",
    "\n",
    "# ‚úÖ 1a. Collect activation parameters\n",
    "activation_params = (\n",
    "    list(encoder_net.activation.parameters()) +\n",
    "    list(decoder_net.activation.parameters())\n",
    ")\n",
    "\n",
    "\n",
    "# ‚úÖ 1b. Collect activation layers (used for decay/regularization logic)\n",
    "activation_layers = [\n",
    "    m for m in encoder_net.activation.modules() if isinstance(m, FFTGate)\n",
    "] + [\n",
    "    m for m in decoder_net.activation.modules() if isinstance(m, FFTGate)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ 1c. Define layer index map once\n",
    "layer_index_map = {idx: idx for idx in range(len(activation_layers))}\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ 2. Setup freezing, optimizer, scheduler\n",
    "unfreeze_activation_epoch = 1               # ‚è± When to unfreeze\n",
    "WARMUP_ACTIVATION_EPOCHS = 0                # üî• Optional warm-up delay\n",
    "\n",
    "# üîí Freeze activation parameter initially\n",
    "for param in activation_params:\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# üß† Define activation parameter optimizer\n",
    "activation_optimizers = {\n",
    "    \"gamma1\": optim.AdamW(activation_params, lr=0.0015, weight_decay=1e-6)\n",
    "}\n",
    "\n",
    "# üîÑ Cosine Annealing Scheduler for activation paramter\n",
    "activation_schedulers = {\n",
    "    \"gamma1\": torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        activation_optimizers[\"gamma1\"],\n",
    "        T_0=10,\n",
    "        T_mult=2,\n",
    "        eta_min=1e-5\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ === TRANSLATION FUNCTION FOR INFERENCE ===\n",
    "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
    "    spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    # Tokenization\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens.insert(0, german.init_token)\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(sentence_tensor, epoch=0)  # <-- Pass dummy epoch for inference\n",
    "\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(previous_word, hidden, cell, epoch=0)  # <-- Same here\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "\n",
    "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "    return translated_sentence[1:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "\n",
    "sentence = \"Cristiano Ronaldo ist ein gro√üartiger Fu√üballspieler mit erstaunlichen F√§higkeiten und Talenten.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc028a6-3ce7-480b-96b6-d00fd89e1b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 0: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['1.50000', '1.50000'] | Gamma1 Grad: ['None', 'None']\n",
      "[Epoch 1 / 50]\n",
      "üîì Unfreezing activation parameters at epoch 1\n",
      "=> Saving checkpoint\n",
      "\n",
      "üö® ENTERED apply_dynamic_regularization | Epoch=1 | Batch=0\n",
      "üß† GAMMA1 INFO: Layer 0: ID=1115257043712 | Mean=1.50000 | Layer 1: ID=1115257572304 | Mean=1.50000\n",
      "Epoch 1: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['1.78499', '1.78503'] | Gamma1 Grad: ['-0.70698', '-0.70717']\n",
      "[Epoch 2 / 50]\n",
      "=> Saving checkpoint\n",
      "\n",
      "üö® ENTERED apply_dynamic_regularization | Epoch=2 | Batch=0\n",
      "üß† GAMMA1 INFO: Layer 0: ID=1115257043712 | Mean=1.78499 | Layer 1: ID=1115257572304 | Mean=1.78503\n",
      "Epoch 2: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['2.06584', '2.06588'] | Gamma1 Grad: ['-0.70709', '-0.70706']\n",
      "[Epoch 3 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 3: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00119 | Gamma1: ['2.33461', '2.33467'] | Gamma1 Grad: ['-0.70710', '-0.70699']\n",
      "[Epoch 4 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 4: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00099 | Gamma1: ['2.58459', '2.58465'] | Gamma1 Grad: ['-0.70715', '-0.70699']\n",
      "[Epoch 5 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 5: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00076 | Gamma1: ['2.81089', '2.81095'] | Gamma1 Grad: ['-0.70714', '-0.70703']\n",
      "[Epoch 6 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 6: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['3.01096', '3.01102'] | Gamma1 Grad: ['-0.70711', '-0.70695']\n",
      "[Epoch 7 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 7: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00032 | Gamma1: ['3.18478', '3.18483'] | Gamma1 Grad: ['-0.70710', '-0.70704']\n",
      "[Epoch 8 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 8: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['3.33492', '3.33497'] | Gamma1 Grad: ['-0.70701', '-0.70702']\n",
      "[Epoch 9 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 9: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['3.46628', '3.46633'] | Gamma1 Grad: ['-0.70695', '-0.70706']\n",
      "[Epoch 10 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 10: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['3.58557', '3.58562'] | Gamma1 Grad: ['-0.70703', '-0.70703']\n",
      "[Epoch 11 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 11: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['3.87055', '3.87061'] | Gamma1 Grad: ['-0.70704', '-0.70702']\n",
      "[Epoch 12 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 12: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['4.15448', '4.15455'] | Gamma1 Grad: ['-0.70709', '-0.70706']\n",
      "[Epoch 13 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 13: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['4.43530', '4.43538'] | Gamma1 Grad: ['-0.70707', '-0.70706']\n",
      "[Epoch 14 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 14: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['4.71101', '4.71109'] | Gamma1 Grad: ['-0.70706', '-0.70706']\n",
      "[Epoch 15 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 15: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00128 | Gamma1: ['4.97976', '4.97984'] | Gamma1 Grad: ['-0.70706', '-0.70701']\n",
      "[Epoch 16 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 16: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00119 | Gamma1: ['5.23987', '5.23995'] | Gamma1 Grad: ['-0.70703', '-0.70704']\n",
      "[Epoch 17 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 17: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00109 | Gamma1: ['5.48987', '5.48995'] | Gamma1 Grad: ['-0.70702', '-0.70707']\n",
      "[Epoch 18 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 18: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00099 | Gamma1: ['5.72851', '5.72859'] | Gamma1 Grad: ['-0.70695', '-0.70689']\n",
      "[Epoch 19 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 19: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['5.95481', '5.95489'] | Gamma1 Grad: ['-0.70698', '-0.70697']\n",
      "[Epoch 20 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 20: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00076 | Gamma1: ['6.16769', '6.16777'] | Gamma1 Grad: ['-0.70518', '-0.70518']\n",
      "[Epoch 21 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 21: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00064 | Gamma1: ['6.36750', '6.36758'] | Gamma1 Grad: ['-0.70599', '-0.70599']\n",
      "[Epoch 22 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 22: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00052 | Gamma1: ['6.55411', '6.55420'] | Gamma1 Grad: ['-0.70592', '-0.70592']\n",
      "[Epoch 23 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 23: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00042 | Gamma1: ['6.72779', '6.72788'] | Gamma1 Grad: ['-0.70504', '-0.70504']\n",
      "[Epoch 24 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 24: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00032 | Gamma1: ['6.88920', '6.88929'] | Gamma1 Grad: ['-0.70625', '-0.70625']\n",
      "[Epoch 25 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 25: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00023 | Gamma1: ['7.03928', '7.03937'] | Gamma1 Grad: ['-0.70608', '-0.70608']\n",
      "[Epoch 26 / 50]\n",
      "=> Saving checkpoint\n",
      "üî• Stable Noise Injected | Epoch 26 | Batch 0 | Layers: 0 (Œî=0.00000), 1 (Œî=0.00000)\n",
      "Epoch 26: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00015 | Gamma1: ['7.17920', '7.17929'] | Gamma1 Grad: ['-0.70629', '-0.70629']\n",
      "[Epoch 27 / 50]\n",
      "=> Saving checkpoint\n",
      "üî• Stable Noise Injected | Epoch 27 | Batch 0 | Layers: 0 (Œî=0.00000), 1 (Œî=0.00000)\n",
      "Epoch 27: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00009 | Gamma1: ['7.31048', '7.31057'] | Gamma1 Grad: ['-0.70384', '-0.70384']\n",
      "[Epoch 28 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 28: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00005 | Gamma1: ['7.43481', '7.43490'] | Gamma1 Grad: ['-0.70607', '-0.70607']\n",
      "[Epoch 29 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 29: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00002 | Gamma1: ['7.55404', '7.55413'] | Gamma1 Grad: ['-0.70567', '-0.70567']\n",
      "[Epoch 30 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 30: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['7.67017', '7.67026'] | Gamma1 Grad: ['-0.70609', '-0.70609']\n",
      "[Epoch 31 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 31: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00150 | Gamma1: ['7.95509', '7.95518'] | Gamma1 Grad: ['-0.70615', '-0.70615']\n",
      "[Epoch 32 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 32: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00149 | Gamma1: ['8.23971', '8.23981'] | Gamma1 Grad: ['-0.70619', '-0.70618']\n",
      "[Epoch 33 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 33: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00148 | Gamma1: ['8.52358', '8.52368'] | Gamma1 Grad: ['-0.70602', '-0.70602']\n",
      "[Epoch 34 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 34: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00146 | Gamma1: ['8.80614', '8.80624'] | Gamma1 Grad: ['-0.70271', '-0.70271']\n",
      "[Epoch 35 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 35: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00144 | Gamma1: ['9.08566', '9.08576'] | Gamma1 Grad: ['-0.69449', '-0.69449']\n",
      "[Epoch 36 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 36: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00142 | Gamma1: ['9.36301', '9.36311'] | Gamma1 Grad: ['-0.70128', '-0.70128']\n",
      "[Epoch 37 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 37: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00139 | Gamma1: ['9.63776', '9.63785'] | Gamma1 Grad: ['-0.70231', '-0.70231']\n",
      "[Epoch 38 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 38: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00136 | Gamma1: ['9.90940', '9.90950'] | Gamma1 Grad: ['-0.70341', '-0.70341']\n",
      "[Epoch 39 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 39: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00132 | Gamma1: ['10.17746', '10.17756'] | Gamma1 Grad: ['-0.68915', '-0.68914']\n",
      "[Epoch 40 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 40: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00128 | Gamma1: ['10.44150', '10.44160'] | Gamma1 Grad: ['-0.69309', '-0.69309']\n",
      "[Epoch 41 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 41: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00124 | Gamma1: ['10.70102', '10.70112'] | Gamma1 Grad: ['-0.70277', '-0.70277']\n",
      "[Epoch 42 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 42: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00119 | Gamma1: ['10.95571', '10.95581'] | Gamma1 Grad: ['-0.70246', '-0.70246']\n",
      "[Epoch 43 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 43: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00114 | Gamma1: ['11.20528', '11.20538'] | Gamma1 Grad: ['-0.70286', '-0.70286']\n",
      "[Epoch 44 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 44: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00109 | Gamma1: ['11.44936', '11.44946'] | Gamma1 Grad: ['-0.68798', '-0.68798']\n",
      "[Epoch 45 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 45: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00104 | Gamma1: ['11.68768', '11.68778'] | Gamma1 Grad: ['-0.70332', '-0.70332']\n",
      "[Epoch 46 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 46: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00099 | Gamma1: ['11.92000', '11.92009'] | Gamma1 Grad: ['-0.70287', '-0.70287']\n",
      "[Epoch 47 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 47: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00093 | Gamma1: ['12.14601', '12.14610'] | Gamma1 Grad: ['-0.70281', '-0.70281']\n",
      "[Epoch 48 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 48: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00087 | Gamma1: ['12.36569', '12.36578'] | Gamma1 Grad: ['-0.70341', '-0.70341']\n",
      "[Epoch 49 / 50]\n",
      "=> Saving checkpoint\n",
      "Epoch 49: M_Optimizer LR => 0.00100 | Gamma1 LR => 0.00081 | Gamma1: ['12.57886', '12.57895'] | Gamma1 Grad: ['-0.69204', '-0.69204']\n",
      "‚úÖ BLEU Score: 21.79\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ === TRAINING ===\n",
    "\n",
    "\n",
    "name_Main = 'FFTGate' # ‚úÖ Used for naming files \n",
    "log_history = []      # ‚úÖ Log messages for all epochs\n",
    "log_path = fr\"C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\LanguageTranslation\\log_{name_Main}.txt\"\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "\n",
    "\n",
    "    # ‚úÖ Unfreeze activation parameters at specified epoch\n",
    "    if epoch == unfreeze_activation_epoch:\n",
    "        print(f\"üîì Unfreezing activation parameters at epoch {epoch}\")\n",
    "        for param in activation_params:\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "    # ‚úÖ Save model checkpoint\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    #translated_sentence = translate_sentence(\n",
    "    #    model, sentence, german, english, device, max_length=50\n",
    "    #)\n",
    "\n",
    "    #print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    # ‚úÖ Switch to training mode\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    # ‚úÖ Track per-layer activation means\n",
    "    activation_history = []   # üî¥ Initialize empty history at start of epoch (outside batch loop)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ‚úÖ Initialize prev_params for regularization | ‚úÖ Before epoch loop \n",
    "    prev_params = {}\n",
    "    \n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        prev_params[idx] = {\"gamma1\": layer.gamma1.clone().detach()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ‚úÖ Loop through training batches\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        # Get input and targets and get to cuda | === Move data to device\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        # ===  Forward prop\n",
    "        # output = model(inp_data, target)\n",
    "        output = model(inp_data, target, epoch=epoch)\n",
    "\n",
    "\n",
    "        # === Reshape output and target for loss\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin. While we're at it\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "        # ‚úÖ Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        if epoch >= unfreeze_activation_epoch:\n",
    "            for opt in activation_optimizers.values():\n",
    "                opt.zero_grad()\n",
    "\n",
    "\n",
    "        # ‚úÖ Loss calculation\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ‚úÖ Collect Activation History | ‚úÖ Per-layer mean activations\n",
    "        for layer in activation_layers:  # üîÑ Use activation_layers directly\n",
    "            if hasattr(layer, \"saved_output\"):\n",
    "                activation_history.append(layer.saved_output.mean().item())\n",
    "\n",
    "        # ‚úÖ Apply Decay strategy to history for each activation layer\n",
    "        with torch.no_grad():\n",
    "            for layer in activation_layers:\n",
    "                if isinstance(layer, FFTGate):\n",
    "                    layer.decay_spectral_history(epoch, num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ‚úÖ Call Regularization Function for the Activation Parameter\n",
    "        if epoch > 0:\n",
    "            gamma1_reg = apply_dynamic_regularization(\n",
    "                model, inp_data, output, epoch, num_epochs,\n",
    "                prev_params, layer_index_map, gamma1_history=None,\n",
    "                batch_idx=batch_idx, total_batches=None,\n",
    "                test_acc=None\n",
    "            )\n",
    "            loss += gamma1_reg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ‚úÖ Backprop\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # ‚úÖ Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "\n",
    "        # ‚úÖ Optimizer step\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "        if epoch >= unfreeze_activation_epoch:\n",
    "            for opt in activation_optimizers.values():\n",
    "                opt.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ‚úÖ Clamp gamma1 values after step\n",
    "        with torch.no_grad():\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, FFTGate):\n",
    "                    layer.gamma1.data.clamp_(0.3, 18.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ‚úÖ TensorBoard Logging | Plot to tensorboard\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ‚úÖ ONLY update prev_params here AFTER all updates | ‚úÖ Update prev_params AFTER training epoch\n",
    "    for idx, layer in enumerate(activation_layers):\n",
    "        prev_params[idx][\"gamma1\"] = layer.gamma1.clone().detach()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ‚úÖ Log activation mean (optional)\n",
    "    if activation_history:\n",
    "        avg_act = sum(activation_history) / len(activation_history)\n",
    "        writer.add_scalar(\"Activation/AvgGamma1Output\", avg_act, global_step=epoch)\n",
    "\n",
    "\n",
    "\n",
    "    # ‚úÖ Activation scheduler step (once per epoch)\n",
    "    if epoch >= unfreeze_activation_epoch:\n",
    "        for name, act_scheduler in activation_schedulers.items():\n",
    "            act_scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    # ‚úÖ Logging Parameters & Gradients after epoch\n",
    "    last_batch_grads = {\"Gamma1 Grad\": []}\n",
    "    current_params = {\"Gamma1\": []}\n",
    "\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, FFTGate):\n",
    "            last_batch_grads[\"Gamma1 Grad\"].append(\n",
    "                f\"{layer.gamma1.grad.item():.5f}\" if layer.gamma1.grad is not None else \"None\"\n",
    "            )\n",
    "            current_params[\"Gamma1\"].append(f\"{layer.gamma1.item():.5f}\")\n",
    "\n",
    "    log_msg = (\n",
    "        f\"Epoch {epoch}: M_Optimizer LR => {optimizer.param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1 LR => {activation_optimizers['gamma1'].param_groups[0]['lr']:.5f} | \"\n",
    "        f\"Gamma1: {current_params['Gamma1']} | \"\n",
    "        f\"Gamma1 Grad: {last_batch_grads['Gamma1 Grad']}\"\n",
    "    )\n",
    "    log_history.append(log_msg)\n",
    "    print(log_msg)\n",
    "\n",
    "# ‚úÖ Write log history to file\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in log_history:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ === BLEU SCORE ===\n",
    "\n",
    "def bleu(data, model, german, english, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in data:\n",
    "        src = vars(example)[\"src\"]\n",
    "        trg = vars(example)[\"trg\"]\n",
    "\n",
    "        prediction = translate_sentence(model, src, german, english, device)\n",
    "        prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "        targets.append([trg])\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "# print(f\"Bleu score {score*100:.2f}\")\n",
    "bleu_str = f\"‚úÖ BLEU Score: {score*100:.2f}\"\n",
    "\n",
    "print(bleu_str)\n",
    "output_path = fr\"C:\\Users\\emeka\\Research\\ModelCUDA\\Big_Data_Journal\\Comparison\\Code\\Paper\\github2\\LanguageTranslation\\BLEU_Score_{name_Main}.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(bleu_str + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
